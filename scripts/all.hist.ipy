 1/1: import pandas as pd
 1/2: df = pd.read_csv('people.csv')
 1/3: df
 1/4: df['first_name'] = df['# username'].str.split()[0]
 1/5: df['first_name'] = df['# username'].str.split().str[0]
 1/6: df
 1/7: df['last_name'] = df['# username'].str.split().str[1:]
 1/8: df['last_name'] = df['last_name'].str.join(' ')
 1/9: df
1/10: df.columns = ['username'] + list(df.columns[1:])
1/11: df['username'] = df['email'].str.strip(')').str[-3:]
1/12: df
1/13: df['username'].nunique()
1/14: df['username'] = df['email'].str.strip(')').str[-4:]
 3/1: hist -g
 3/2:
# create_runstone_roster.py
import pandas as pd
df = pd.read_csv('people.csv')
df['username'] = df['name']
df['first_name'] = df['username'].str.split().str[0]
df['last_name'] = df['username'].str.split().str[1:]
df['last_name'] = df['last_name'].str.join(' ')
df['username'] = df['url'].str.strip(')').str[-5:]
df['username'].nunique()
 3/3: df
 3/4: df['username'].unique()
 4/1: cd src
 6/1: ls
 6/2: cd scripts
 7/1: from dotenv import load_dotenv
 7/2: load_dotenv()
 7/3: import os
 7/4: env=dict(os.environ)
 7/5: env['CANVAS_API_TOKEN']
 7/6: cd scripts
 7/7: ls -hal
 7/8: more use_canvas_grab_and_runestone_export_to_join_roster.hist.py
 7/9: more use_canvas_grab_and_runestone_export_to_join_roster.hist.py.md
7/10: cd ../src
7/11: ls -hal
7/12: cd cisc179
7/13: ls -ahl
7/14: more checkcanvas.py
7/15: %run checkcanvas.py
7/16: ENV
7/17: nano ../../.env
7/18: !nano ../../.env
7/19: !nano checkcanvas.py
7/20: %run checkcanvas.py
7/21: who
7/22: canvas
7/23: canvas.get_accounts()
7/24: list(canvas.get_accounts())
7/25: course = canvas.get_course()
7/26: course = canvas.get_course?
7/27: course = canvas.get_course(CANVAS_COURSE_ID)
7/28: course
7/29: course.calendar
7/30: course.friendly_name
7/31: course.get_discussion_topics()
7/32: list(course.get_discussion_topics())
7/33: ls ../../scripts
7/34: dir(canvas)
7/35: [i for i in dir(canvas) if 'module' in i]
7/36: [i for i in dir(canvas) if 'course' in i]
7/37: [i for i in dir(course) if 'module' in i]
7/38: course.get_modules()
7/39: list(course.get_modules())
7/40: modules = _
7/41: len(modules)
7/42: modules[0]
7/43: dir(modules[0])
7/44: modules[0].id
7/45: modules[0].unlock_at
7/46: modules[0].published
7/47: modules[0].name
7/48: modules[1].name
7/49: modules[2].name
7/50: modules[3].name
7/51: modules[4].name
7/52: modules[5].name
7/53: pwd
7/54: hist -o -p -f test_module_upload.hist.ipy
7/55: hist -f test_module_upload.hist.py
 8/1: ls -hal
 8/2: %run search_engine.py
 8/3: ls data
 8/4: ls data/corpus
 8/5: ls data/corpus/*.csv
 8/6: ls data/corpus/*.joblib
 8/7: ls data/cache/*.joblib
 8/8: ls data/cache/
 8/9: ls -hal data/corpus/*.csv
8/10: ls -hal data/cache/*.csv
8/11: %run search_engine.py
8/12: %run search_engine.py
8/13: db = VectorDB()
8/14: %run search_engine.py
8/15: db = VectorDB()
8/16: %run search_engine.py
8/17: rm data/corpus/*.joblib
8/18: %run search_engine.py
8/19: db
8/20: db.embeddings
8/21: np.ma.average(db.embeddings, axis=1, weights=[.5, .5])
8/22: np.ma.average(db.embeddings, axis=2, weights=[.5, .5])
8/23: np.ma.average(db.embeddings, axis=0, weights=[.5, .5])
8/24: pd.DataFrame(db.embeddings)
8/25: e = pd.DataFrame(db.embeddings)
8/26: e.rolling?
8/27: e.rolling(2)
8/28: e.rolling(2).mean()
8/29: m2 = e.rolling(2).mean()
8/30: m2.shape
8/31: e.shape
8/32: e[0]
8/33: e.iloc[0]
8/34: m2.iloc[0]
8/35: m2.iloc[1]
8/36: e.iloc[0:2]
8/37: query = 'what is gum disease?' ; query_embedding = model.encode([query])
8/38:
        if not isinstance(db.embeddings, pd.DataFrame):
            self.embeddings = pd.DataFrame(db.embeddings)
        self.embeddings_2 = db.embeddings.rolling(2).mean()
8/39:
        if not isinstance(db.embeddings, pd.DataFrame):
            db.embeddings = pd.DataFrame(db.embeddings)
        db.embeddings_2 = db.embeddings.rolling(2).mean()
8/40: similarities = cosine_similarity(query_embedding, self.embeddings_2)[0]
8/41: similarities = cosine_similarity(query_embedding, db.embeddings_2)[0]
8/42: similarities = cosine_similarity(query_embedding, db.embeddings_2[1:])[0]
8/43: top_indices = np.argsort(similarities)[-limit:]
8/44: top_indices = np.argsort(similarities)[-10:]
8/45: top_indices
8/46: top_docs = self.df.iloc[top_indices:top_indices+1].copy()
8/47: top_docs = db.df.iloc[top_indices:top_indices+1].copy()
8/48: top_docs = db.df.iloc[top_indices].copy()
8/49: top_docs2 = db.df.iloc[top_indices+1].copy()
8/50: top_docs
8/51: top_docs['sentence'].str + ' ' + top_docs2['sentence'].str
8/52: top_docs['sentence'] + ' ' + top_docs2['sentence']
8/53: top_docs['sentence']
8/54: top_docs2['sentence']
8/55: [s1 + ' ' + s2 for (s1, s2) in zip(top_docs['sentence'], top_docs2['sentence'])]
8/56: %run search_engine.py
8/57: %run search_engine.py
 9/1: hash('0123')
10/1: from pathlib import Path
10/2: IMAGES_DIR = Path('.') / 'static' / 'images'
10/3:
for s in 'instructor student'.split():
    for p in IMAGES_DIR.glob(f'runestone-{s}-0*.png'):
        slide = f'---\n\n## {s.title()} workflow\n\n![{p.name}](/images/{p.name} "Runestone.Academy step {p.name[len('runestone-') + len(c) + 2:-4]}")\n\n'
10/4:
for s in 'instructor student'.split():
    for p in IMAGES_DIR.glob(f'runestone-{s}-0*.png'):
        slide = f'---\n\n## {s.title()} workflow\n\n'
        slide += f'![{p.name}](/images/{p.name} "Runestone.Academy step {p.name[len("runestone-") + len(c) + 2:-4]}")\n\n'
        print(slide)
10/5:
for s in 'instructor student'.split():
    for p in IMAGES_DIR.glob(f'runestone-{s}-0*.png'):
        slide = f'---\n\n## {s.title()} workflow\n\n'
        slide += f'![{p.name}](/images/{p.name} "Runestone.Academy step {p.name[len("runestone-") + len(s) + 2:-4]}")\n\n'
        print(slide)
10/6:
for s in 'instructor student'.split():
    for p in IMAGES_DIR.glob(f'runestone-{s}-0*.png'):
        title = f'Runestone.Academy {s} workflow'
        stepnum = f'p.name[len("runestone-") + len(s) + 2:-4]
        slide = f'---\n\n## {title}\n\n'
        slide += f'#### Step {stepnum}\n'
        slide += f'![{p.name}](/images/{p.name} "{title} - step {stepnum}")\n\n'
        print(slide)
10/7:
for s in 'instructor student'.split():
    for p in IMAGES_DIR.glob(f'runestone-{s}-0*.png'):
        title = f'Runestone.Academy {s} workflow'
        stepnum = p.name[len("runestone-") + len(s) + 2:-4]
        slide = f'---\n\n## {title}\n\n'
        slide += f'#### Step {stepnum}\n'
        slide += f'![{p.name}](/images/{p.name} "{title} - step {stepnum}")\n\n'
        print(slide)
10/8:
for s in 'instructor student'.split():
    for p in IMAGES_DIR.glob(f'runestone-{s}-0*.png'):
        title = f'Runestone.Academy {s} workflow'
        stepnum = p.name[len("runestone-") + len(s) + 2:-4]
        slide = f'---\n\n## {title}\n\n'
        slide += f'#### Step {stepnum}\n'
        slide += f'![{p.name}](/images/{p.name} "{title} - step {stepnum}")\n\n'
        print(slide)
10/9: ls
10/10: pwd
10/11: cp static/images/runestone-student-* ../2024/mesa_python.gitlab.io/static/images/
10/12: cp static/images/runestone-instructor-* ../2024/mesa_python.gitlab.io/static/images/
10/13: hist -f scripts/auto-generate-runestone-slides.py
10/14: cp -r scripts/ ../2024/mesa_python.gitlab.io/
11/1: import requests
11/2:
url = 'https://hackerpublicradio.org/eps/index.html'
resp = requests.get(url)
11/3: import bs4
11/4: bs4.BeautifulSoup?
11/5: bs = bs4(resp.content)
11/6: bs = bs4.BeautifulSoup(resp.content)
11/7: bs.attrs
11/8: bs.keys()
11/9: bs.keys
11/10: type(resp.content)
11/11: bs = bs4.BeautifulSoup(resp.text)
11/12: bs.attrs
11/13: bs.title
11/14: [link.attrs for link in bs.find_all('a')]
11/15: link.parent
11/16: [link for link in bs.find_all('a') if link.href.startswith('./eps/hpr/')]
11/17: links = [link for link in bs.find_all('a') if link.href and link.href.startswith('./eps/hpr/')]
11/18: len(links)
11/19: links = [link for link in bs.find_all('a')]
11/20: links[0]
11/21: links[0].href
11/22: links[0].get('href')
11/23: links = [link for link in bs.find_all('a') if link.get('href','').startswith('./eps/hpr/')]
11/24: links
11/25: links = [link.get('href') for link in bs.find_all('a') if link.get('href', '')]
11/26: links[0]
11/27: links[1]
11/28: links[2]
11/29: links[3]
11/30: links[4]
11/31: links[5]
11/32: [x for x in links if x.startswith('./eps')]
11/33: links = [link for link in bs.find_all('a') if link.get('href','').startswith('./eps/hpr')]
11/34: links
11/35: links[0].contents
11/36: links[0].parent
11/37: dict(links[0])
11/38: dir(links[0])
11/39: var(links[0])
11/40: vars(links[0])
11/41: ls -hal
11/42: ep = links[0]
11/43: ep.next_sibling()
11/44: ep.next_sibling
11/45: ep.next_sibling.next_sibling
11/46: ep.next_sibling.next_sibling['href']
11/47: host = ep.next_sibling.next_sibling
11/48: host.attrs
11/49: host.content
11/50: host.text
11/51: episodes = [ep['href'], ep.text, ep.next_sibling.next_sibling['href'],  for ep in bs.find_all('a') if ep.get('href','').startswith('./eps/hpr')]
11/52: episodes = [[ep['href'], ep.text, ep.next_sibling.next_sibling['href'], ep.next_sibline.next_sibling.text]  for ep in bs.find_all('a') if ep.get('href','').startswith('./eps/hpr')]
11/53: episodes = [[ep['href'], ep.text, ep.next_sibling.next_sibling['href'], ep.next_sibline.next_sibling.text]  for ep in bs.find_all('a') if ep.get('href','').startswith('./eps/hpr')]
11/54: episodes = [[ep['href'], ep.text, ep.next_sibling.next_sibling['href'], ep.next_sibling.next_sibling.text]  for ep in bs.find_all('a') if ep.get('href','').startswith('./eps/hpr')]
11/55: pd.DataFrame(episodes)
11/56: import pandas as pd ; pd.DataFrame(episodes, columns='episode_url episode_title host_url host_username'.split())
11/57: df = pd.DataFrame(episodes, columns='episode_url episode_title host_url host_username'.split())
11/58: df = df.sort_values('episode_url')
11/59: df = df.reset_index()
11/60: df['episode_number']=df.index.values + 1
11/61: df
11/62: df['episode_title2'] = df['episode_title'].str.split('::').str[-1]
11/63: df
11/64: df['short_title'] = df['episode_title'].str.split('::').str[-1]
11/65: df.drop('episode_title2')
11/66: df
11/67: df.drop(['episode_title2'])
11/68: df['episode_title2'].drop()
11/69: df['episode_title2'].drop('columns')
11/70: df['episode_title2']
11/71: del df['episode_title2']
11/72: df
11/73: df.columns
11/74: del df['index']
11/75: df
11/76: df.columns = 'url full_title host_url host_username episode_number title'.split()
11/77: df
11/78: pwd
11/79: df.to_csv('data/hpr_podcasts.csv', index=None)
11/80: df = df['seq_num title url host_name host_url full_title'.split()]
11/81: df = df['episode_number title url username host_url full_title'.split()]
11/82: df = df['episode_number title url host_username host_url full_title'.split()]
11/83: df.columns = 'seq_num title url host_name host_url full_title'.split()
11/84: df
11/85: df.to_csv('data/hpr_podcasts.csv', index=None)
11/86: more data/hpr_podcasts.csv
11/87: res = requests.get('https://hackerpublicradio.org/eps/hpr0030/')
11/88: s = bs4.BeautifulSoup(s)
11/89: s = bs4.BeautifulSoup(res.text)
11/90: s
11/91: s.findall('h1')
11/92: s
11/93: s.find_all('h1')
11/94: headings = s.find_all('h1')
11/95: headings[1].next_sibling()
11/96: headings[1].next_sibling
11/97: headings[1].next_sibling.next_sibling
11/98: s.find_all('h3')
11/99: subtitle = s.find_all('h3')[1]
11/100: subtitle
11/101: title = s.find_all('h1')[1]
11/102: title
11/103: subtitle, series = s.find_all('h3')[1:3]
11/104: series
11/105: title, comments = s.find_all('h1')[1:3]
11/106: comments
11/107: series.next_sibling
11/108: series.next_siblings
11/109: list(series.next_siblings)
11/110: list(series.children)
11/111: list(series.next_sibling.children)
11/112: list(series.next_siblings[1].children)
11/113: list(series.next_siblings)[1].children)
11/114: list(series.next_siblings)[1].children
11/115: list(list(series.next_siblings)[1].children)
11/116: series.next
11/117: series.next.next
11/118: series.next.next.next
11/119: series.next.next.next.next
11/120: series.next.next.next.next.next
11/121: series.next.next.next.next.next.next
11/122: series.next.next.next.next.next.next.next
11/123: .next
11/124: series.next.next.next.next.next.next.next.next
11/125: series.next.next.next.next.next.next.next.next.next
11/126: series.next.next.next.next.next.next.next.next.next.next
11/127: series.next.next.next.next.next.next.next.next.next.next.next
11/128: series.next.next.next.next.next.next.next.next.next.next.next.next
11/129: series.next.next.next.next.next.next.next.next.next.next.next.next.next
11/130: list(series.next_siblings)[-1].next.next
11/131: list(series.next_siblings)[-1].next
11/132: list(series.next_siblings)[-1].next.next_sibling
11/133: hist
11/134: pwd
11/135: hist -f scripts/scrape_hpr.py
11/136: ls -hal
11/137: hist -f scrape_hpr.py
11/138: cp scrape_hpr.py scrape_hpr.hist.py
11/139: df.reset_index?
11/140: list(series.next_siblings)[-1].next.next_sibling
11/141: list(series.next_siblings)[-1].next.next_sibling.text
11/142: list(series.next_siblings)[-1].find('div')
11/143: list(series.next_siblings)[-1].find_all('div')
11/144: list(series.next_siblings)[-1].div
11/145: vars(list(series.next_siblings)[-1])
11/146: vars(list(series.next_siblings)[-1]).keys()
11/147: series.parent
11/148: series.parent.get('href')
11/149: series.parent.find_all('href')
11/150: series.find_all('href')
11/151: series.find_all('a')
11/152: series.parent.find_all('a')
11/153: subtitle.text
11/154: title.text
11/155:
    links = series.parent.find_all('a')
    tags = [
        a.text for a in links
        if a.get('href', '').lstrip('.').lstrip('/').startswith('tags.html#')
    ]
    row = dict(full_title=title.text, subtitle=subtitle.text, show_notes=show_notes.text)
11/156:     show_notes = list(series.next_siblings)[-1].next.next_sibling
11/157:
    links = series.parent.find_all('a')
    tags = [
        a.text for a in links
        if a.get('href', '').lstrip('.').lstrip('/').startswith('tags.html#')
    ]
    row = dict(full_title=title.text, subtitle=subtitle.text, show_notes=show_notes.text)
11/158: row
11/159: tags
11/160: df
12/1: %run scrape_hpr.py
12/2: %run scrape_hpr.py
12/3: len(episodes)
12/4: %run scrape_hpr.py
12/5: series
12/6:
    if url.lstrip('.').lstrip('/').startswith('eps/hpr'):
        url = '/'.join(['https://hackerpublicradio.org', url.lstrip('.').lstrip('/')])
    resp = requests.get(url)
    s = bs4.BeautifulSoup(resp.text)
    title, comments = s.find_all('h1')[1:3]
    subtitle, series = s.find_all('h3')[1:3]
12/7: series
12/8: series.text
13/1: %run scrape_hpr.py
14/1: ls *.py
14/2: import spacy
14/3: nlp = spacy.load('en_core_web_sm')
14/4: nlp = spacy.load('en_core_web_md')
14/5: nlp = spacy.load('en_core_web_sm')
14/6: nlp('What is the capital of Kansas?')
14/7: query_doc = nlp('What is the capital of Kansas?')
14/8: query_doc[1].pos_
14/9: query_doc[2].pos_
14/10: query_doc = nlp('Who runs the capital of Kansas?')
14/11: query_doc[1].pos_
14/12: reversed(query_doc[:2])
14/13: list(reversed(query_doc[:2]))
13/2: who
13/3: df
13/4: import json
13/5: js = json.dumps(episodes)
13/6: import jsonlines
13/7:
with open('data/corpus/hpr_episodes.json', 'w') as fout:
    fout.write(js)
13/8: fout.close()
13/9: more data/hpr_episodes.json
13/10: more data/corpus/hpr_episodes.json
13/11: ls data/corpus/
13/12: mkdir corpus/hpr
13/13: mkdir data/corpus_hpr
13/14: ls corpus
13/15: ls data/corpus
13/16: mv data/corpus/hpr_episodes.json data/corpus_hpr/
13/17: !find . -name '*.csv'
13/18: !find ./data -name '*.csv'
13/19: mv ./data/hpr_podcasts.csv ./data/corpus_hpr/
13/20: !diff data/cache/nutrition_sentences.csv data/corpus/nutrition_sentences.csv
13/21: ls -hal data/cache/nutrition_sentences.csv data/corpus/nutrition_sentences.csv
13/22: rm data/cache/nutrition_sentences.csv
13/23:
import OpenAI from "openai"

const openai = new OpenAI({
  baseURL: "https://openrouter.ai/api/v1",
  apiKey: $OPENROUTER_API_KEY,
  defaultHeaders: {
    "HTTP-Referer": $YOUR_SITE_URL, // Optional, for including your app on openrouter.ai rankings.
    "X-Title": $YOUR_SITE_NAME, // Optional. Shows in rankings on openrouter.ai.
  },
  // dangerouslyAllowBrowser: true,
})
async function main() {
  const completion = await openai.chat.completions.create({
    model: "openai/gpt-3.5-turbo",
    messages: [
      { role: "user", content: "Say this is a test" }
    ],
  })

  console.log(completion.choices[0].message)
}
main()
16/1: import openai
16/2: import dotenv
16/3: dotenv.dotenv_values()
16/4: env = dotenv.dotenv_values()
16/5: globals().update(env)
16/6: who
16/7:
import OpenAI from "openai"

const openai = new OpenAI({
  baseURL: "https://openrouter.ai/api/v1",
  apiKey: $OPENROUTER_API_KEY,
  defaultHeaders: {
    "HTTP-Referer": $YOUR_SITE_URL, // Optional, for including your app on openrouter.ai rankings.
    "X-Title": $YOUR_SITE_NAME, // Optional. Shows in rankings on openrouter.ai.
  },
  // dangerouslyAllowBrowser: true,
})
async function main() {
  const completion = await openai.chat.completions.create({
    model: "openai/gpt-3.5-turbo",
    messages: [
      { role: "user", content: "Say this is a test" }
    ],
  })

  console.log(completion.choices[0].message)
}
main()
16/8:
import OpenAI from openai

const openai = new OpenAI({
  baseURL: "https://openrouter.ai/api/v1",
  apiKey: $OPENROUTER_API_KEY,
  defaultHeaders: {
    "HTTP-Referer": $YOUR_SITE_URL, // Optional, for including your app on openrouter.ai rankings.
    "X-Title": $YOUR_SITE_NAME, // Optional. Shows in rankings on openrouter.ai.
  },
  // dangerouslyAllowBrowser: true,
})
async function main() {
  const completion = await openai.chat.completions.create({
    model: "openai/gpt-3.5-turbo",
    messages: [
      { role: "user", content: "Say this is a test" }
    ],
  })

  console.log(completion.choices[0].message)
}
main()
16/9:
from openai import OpenAI
from os import getenv

# gets API Key from environment variable OPENAI_API_KEY
client = OpenAI(
  base_url="https://openrouter.ai/api/v1",
  api_key=getenv("OPENROUTER_API_KEY"),
)

completion = client.chat.completions.create(
  extra_headers={
    "HTTP-Referer": $YOUR_SITE_URL, # Optional, for including your app on openrouter.ai rankings.
    "X-Title": $YOUR_APP_NAME, # Optional. Shows in rankings on openrouter.ai.
  },
  model="openai/gpt-3.5-turbo",
  messages=[
    {
      "role": "user",
      "content": "Say this is a test",
    },
  ],
)
print(completion.choices[0].message.content)
16/10:
from openai import OpenAI
from os import getenv

# gets API Key from environment variable OPENAI_API_KEY
client = OpenAI(
  base_url="https://openrouter.ai/api/v1",
  api_key=OPEN_ROUTER_API_KEY,
)

completion = client.chat.completions.create(
  extra_headers={
    "HTTP-Referer": "https://qary.ai", # Optional, for including your app on openrouter.ai rankings.
    "X-Title": "https://qary.ai", # Optional. Shows in rankings on openrouter.ai.
  },
  model="openai/gpt-3.5-turbo",
  messages=[
    {
      "role": "user",
      "content": "Say this is a test",
    },
  ],
)
print(completion.choices[0].message.content)
16/11: hist -f llm.py
13/24: !pip install langchain
16/12: %run llm
16/13: rag()
16/14: %run llm
16/15: rag()
17/1: %run llm
17/2: rag()
17/3: %run llm
17/4: rag()
17/5: %run llm
17/6: %run llm
17/7: rag()
17/8: db
17/9: from search_engine import VectorDB
17/10: db = VectorDB()
18/1: %run scrape_hpr.py
18/2: %run scrape_hpr.py
19/1: %run scrape_hpr.py
17/11: hist -o -p
17/12: rag(model='auto')
17/13: MODELS
17/14: rag(model=MODELS[0])
17/15: rag(model=MODELS[1])
17/16: rag(model=MODELS[2])
17/17: rag(model=MODELS[3])
17/18: len(MODELS)
17/19: %run llm
17/20: MODELS[-1]
17/21: len(MODELS)
17/22:
for model in MODELS:
    rag(model=model, context="A Python is Fast.")
17/23:
answers = []
for model in MODELS:
    answers.append(rag(model=model, context="A Python is Fast."))
17/24: answers
17/25:
answers = []
for model in MODELS:
    answers.append(rag(model=model, context="Python is Fast."))
17/26: answers
17/27:
answers = []
for model in MODELS:
    answers.append(rag(model=model, context="Python is fast."))
17/28:

for model in MODELS:
    kwargs = dict(
        context='Python is fast.',
        question='What is Python?',
        model=model)
    kwargs.update(dict(answer=rag(**kwargs)[0]))
    answers.append(kwargs)
17/29: pd.DataFrame(answers)
17/30: import pandas as pd
17/31: pd.DataFrame(answers)
17/32:
answers = []
for model in MODELS:
    kwargs = dict(
        context='Python is fast.',
        question='What is Python?',
        model=model,
        )
    ans=rag(**kwargs)
    print(ans)
    kwargs.update(dict(answer=ans[0]))
    print(kwargs)
    answers.append(kwargs)
    print(pd.DataFrame(answers))
17/33:
for model in MODELS:
    kwargs = dict(
        context='Python is fast?',
        question='What is Python?',
        model=model,
        )
    ans=rag(**kwargs)
    kwargs.update(dict(answer=ans[0]))
    answers.append(kwargs)
    print(pd.DataFrame(answers))
17/34:
for model in MODELS:
    kwargs = dict(
        context='Python is fast?',
        question='What is Python?',
        model=model,
        )
    ans=rag(**kwargs)
    kwargs.update(dict(answer=ans[0]))
    answers.append(kwargs)
    df = pd.DataFrame(answers)
    print(df.iloc[-1])
17/35:
for model in MODELS:
    kwargs = dict(
        context='Python is fast?',
        question='What is Python?',
        model=model,
        )
    ans=rag(**kwargs)
    kwargs.update(dict(answer=ans[0]))
    answers.append(kwargs)
    df = pd.DataFrame(answers)
    print(df.iloc[-1])
    print()
17/36: hist -o -p -f test_llms_question_mark.hist.ipy
17/37: hist -f test_llms_question_mark.hist.py
17/38: df
17/39: df.to_csv('data/test_llms.csv')
17/40: db
17/41: db.search('What is the best source of antioxidants?')
17/42: df_nut = _
17/43: df_nut
17/44: df_nut['sentence']
17/45: df_nut['sentence'].iloc[0]
17/46: db.search('What is the best source of antioxidants?', preprocess=False)
17/47: db.search?
20/1: import pandas as pd
20/2: pd.read_csv('data/hpr_podcasts.csv')
20/3: df = _
20/4: df.columns
20/5: df.iloc[0]
20/6: df.iloc[0]['show_notes']
20/7: df.iloc[0]
20/8: df.iloc[0][7]
20/9: df.iloc[100][7]
20/10: df.iloc[100][10]
20/11: df.iloc[200][10]
20/12: df.iloc[300][10]
20/13: df.iloc[1000][10]
20/14: df.iloc[2000][10]
20/15: df.iloc[4000][10]
20/16: df.iloc[4100][10]
20/17: df.iloc[4020][10]
21/1: from search_engine import VectorDB
21/2: from search_engine import *
21/3: pd.read_csv('data/hpr_podcasts.csv')
21/4: df = _
21/5:
for i, row in df.iter_rows():
    print(row.as_dict())
21/6: columns = 'seq_num title url host_name host_url full_title'.split()
21/7: columns += 'full_title subtitle series audio show_notes tags'.split()
21/8: len(df.columns)
21/9: columns
21/10: len(columns)
21/11: df.columns = columns
21/12: df.head()
21/13: df.iloc[0]
21/14: columns[6]
21/15: columns[6] = 'full_title_4digit'
21/16: df.iloc[0]
21/17: df.columns = columns
21/18: df.iloc[0]
21/19:
for i, row in df.iter_rows():
    text = f"## {row['full_title_4digit']}\n\n {row['show_notes']}'
21/20:
for i, row in df.iter_rows():
    text = f"## {row['full_title_4digit']}\n\n {row['show_notes']}"
    with open('data/hpr_corpus/{url[6:13].strip(/)}.txt') as fout:
        fout.write(text)
21/21:
for i, row in df.iterrows():
    print(f'{row["url"][6:13].strip(/)}.txt')
    text = f"## {row['full_title_4digit']}\n\n {row['show_notes']}"
    with open(f'data/hpr_corpus/{row["url"][6:13].strip(/)}.txt') as fout:
        fout.write(text)
21/22:
for i, row in df.iterrows():
    print(f'{row["url"][6:13].strip("/")}.txt')
    text = f"## {row['full_title_4digit']}\n\n {row['show_notes']}"
    with open(f'data/hpr_corpus/{row["url"][6:13].strip("/")}.txt') as fout:
        fout.write(text)
21/23: ls data/hpr_corpus
21/24: ls data/
21/25:
for i, row in df.iterrows():
    print(f'{row["url"][6:13].strip("/")}.txt')
    text = f"## {row['full_title_4digit']}\n\n {row['show_notes']}"
    with open(f'data/corpus_hpr/{row["url"][6:13].strip("/")}.txt', 'w') as fout:
        fout.write(text)
21/26: db = VectorDB?
21/27: db = VectorDB(df='./data/corpus_hpr/hpr_sentences.csv')
21/28: db = VectorDB(df=Path('./data/corpus_hpr/hpr_sentences.csv'))
21/29: db = VectorDB()
21/30: db.cli()
21/31: %run llm
21/32: who
21/33: rag()
21/34: df.to_csv('data/test_llms.csv')
21/35: df
22/1: %run llm
22/2: who
22/3: MODELS
22/4: PROMPT
22/5: print(PROMPT)
22/6: rag('What is the healthiest fruit?', context)
22/7: from search_engine import *
22/8: db = VectorDB()
22/9: db.search('What is the healthiest fruit?')
22/10: contextdf = _
22/11: df.columns
22/12: df['sentence']
22/13: df['sentences']
22/14: df
22/15: who
22/16: contextdf.columns
22/17: contextdf.iloc[0]
22/18: '\n'.join([s for s in contextdf['sentence']])
22/19: context = '\n'.join([s for s in contextdf['sentence']])
22/20: rag('What is the healthiest fruit?', context=context)
22/21: rag('How much exercise is healthiest?', context)
22/22: q = 'How much exercise is healthiest?'
22/23: df2 = db.search(q)
22/24: df2.sentence.str.join('\n')
22/25: context = '\n'.join([s for s in contextdf['sentence']])
22/26: rag(q, context=context)
22/27: context
22/28: '\n'.join([s for s in df2['sentence']])
22/29: rag(q, context=context)
22/30: context = '\n'.join([s for s in df2['sentence']])
22/31: print(context)
22/32: q
22/33: rag(q, context=context)
22/34: !git remote -v
23/1: ls data
23/2: ls data/private
23/3: from pathlib import Path
23/4:
for p in Path('./data/private/').glob('*.xls'):
    print(p)
23/5: import pandas as pd
23/6:
dfs = {}
for p in Path('./data/private/').glob('*.xls'):
    print(p)
    df = pd.read_excel(p.open())
    dfs[p.with_suffix('').name] = df
23/7: pd.read_excel?
23/8:
dfs = {}
for p in Path('./data/private/').glob('*.xls'):
    print(p)
    df = pd.read_excel(p.open(), engine='xlrd')
    dfs[p.with_suffix('').name] = df
23/9: !pip install --upgrade xlrd
23/10:
dfs = {}
for p in Path('./data/private/').glob('*.xls'):
    print(p)
    df = pd.read_excel(p.open())
    dfs[p.with_suffix('').name] = df
23/11:
dfs = {}
for p in Path('./data/private/').glob('*.xls'):
    print(p)
    df = pd.read_excel(p.open(), engine='xlrd')
    dfs[p.with_suffix('').name] = df
24/1: import pandas as pd
24/2: from pathlib import Path
24/3:
dfs = {}
for p in Path('./data/private/').glob('*.xls'):
    print(p)
    df = pd.read_excel(p.open(), engine='xlrd')
    dfs[p.with_suffix('').name] = df
24/4: s = p.read()
24/5: s = p.open()
24/6: s = s.read()
24/7: s
24/8: pd.read_excel?
24/9:
dfs = {}
for p in Path('./data/private/').glob('*.xls'):
    print(p)
    df = pd.read_excel(p.open(), engine='pyopenxl')
    dfs[p.with_suffix('').name] = df
24/10: pd.read_excel?
24/11:
dfs = {}
for p in Path('./data/private/').glob('*.xls'):
    print(p)
    df = pd.read_excel(p.open(), engine='openpyxl')
    dfs[p.with_suffix('').name] = df
24/12:
dfs = {}
for p in Path('./data/private/').glob('*.xls'):
    print(p)
    df = pd.read_excel(p.open().read(), engine='openpyxl')
    dfs[p.with_suffix('').name] = df
24/13: dfs
24/14:
dfs = {}
for p in Path('./data/private/').glob('*.xls'):
    print(p)
    df = pd.read_excel(p, engine='openpyxl')
25/1: %run checkcanvas.py
25/2: ls
25/3: %run src/cisc179/checkcanvas.py
25/4: who
25/5: hist | grep canvas
25/6: [i for i in dir(canvas) if 'course' in i]
25/7: course = canvas.get_course(CANVAS_COURSE_ID)
25/8: list(course.get_modules())
25/9: modules = _
25/10: ls src/cisc179
25/11: cp src/cisc179/test_module_upload.hist.* scripts/
25/12: mv src/cisc179/checkcanvas.py scripts/
25/13: rm src/cisc179/test_module_upload.hist.*
25/14: git status
25/15: !git status
25/16: !git add scripts
25/17: ls data
25/18: git add data/public
25/19: !git add data/public
25/20: more data/name-url.csv
25/21: !mv data/name-url.csv data/private/
25/22: !git status
25/23: !git add notes
25/24: mv runestone-getting-started.* content/
25/25: !git commit -m public-data-and-scripts-dir
25/26: !git status
25/27: mv module-00-orientation/ content/
25/28: more content/module-00-orientation/*.json
25/29: ls -hal content/module-00-orientation/
25/30: git status
25/31: !git status
25/32: !git add content
25/33: !git commit -am 'manually download module00 orientation banner'
25/34: !git status
25/35:
dfs = {}
for p in Path('./data/private/').glob('*.html'):
    print(p)
    df = pd.read_html(p)
25/36: from pathlib import Path
25/37: import pandas as pd
25/38:
dfs = {}
for p in Path('./data/private/').glob('*.html'):
    print(p)
    df = pd.read_html(p)
25/39: dfs.keys()
25/40: dfs
25/41: df
25/42:
dfs = {}
for p in Path('./data/private/').glob('*.htm*'):
    print(p)
    df = pd.read_html(p)
    dfs[p.with_suffix('').name] = df
25/43: dfs.keys()
25/44: dfs['roster']
25/45: dfs['roster'][0]
25/46:
dfs = []
for p in Path('./data/private/').glob('*.htm*'):
    print(p)
    name = p.with_suffix('').name
    dfs.extend([(name+str(i or ''), df) for (i, df) in enumerate(pd.read_html(p))])
    dfs[p.with_suffix('').name] = df
25/47:
dfs = []
for p in Path('./data/private/').glob('*.htm*'):
    print(p)
    name = p.with_suffix('').name
    dfs.extend([(name+str(i or ''), df) for (i, df) in enumerate(pd.read_html(p))])
25/48: d = dict(dfs)
25/49: d
25/50: d['roster']
25/51: d['roster'].columns
25/52: d['roster'].iloc[0]
25/53: d['roster'].set_index('ID')
25/54: df = _
25/55: df.to_csv('data/private/roster_2024-01-27.csv')
25/56: df = pd.read_csv('data/private/roster_2024-01-27.csv')
25/57: df
25/58: df = pd.read_csv('data/private/roster_2024-01-27.csv', index_col=0)
25/59: df
25/60: hist -f scripts/process_roster.py
25/61: hist -f scripts/process_roster.hist.py
25/62: hist -o -p -f scripts/process_roster.hist.ipy
25/63: df = d['waitlist']
25/64: addcodes = d['addcodes']
25/65: df = pd.read_csv('data/private/roster_2024-01-27.csv', index_col=0)
25/66: dfadd = addcodes
25/67: dfwait = d['waitlist']
25/68: dfwait
25/69:
dfs = []
for p in Path('./data/private/').glob('*.htm*'):
    print(p)
    name = p.with_suffix('').name
    for i, df in enumerate(pd.read_html(p)):
        df.columns = [str(s).lower().strip().replace(' ', '_') for s in df.columns]
        df = df.set_index('id')
        dfs.append((name+str(i or ''), df))
25/70: df
25/71: df.reset_index()
25/72: d = dict(dfs)
25/73: d
25/74:
dfs = []
for p in Path('./data/private/').glob('*.htm*'):
    print(p)
    name = p.with_suffix('').name
    for i, df in enumerate(pd.read_html(p)):
        df.columns = [str(s).lower().strip().replace(' ', '_') for s in df.columns]
        df = df.set_index('id') if 'id' in df.columns else df.set_index('seq')
        dfs.append((name+str(i or ''), df))
25/75: d = dict(dfs)
25/76: d
25/77:
dfs = []
for p in Path('./data/private/').glob('*.htm*'):
    print(p)
    name = p.with_suffix('').name
    for i, df in enumerate(pd.read_html(p)):
        df.columns = [str(s).lower().strip().replace(' ', '_') for s in df.columns]
        df = df.set_index('id') if 'id' in df.columns else df.set_index('seq')
        dfs.append((name+str(i or ''), df))
25/78: dfadd = d['addcodes']
25/79: dfadd.index
25/80: 'id' in df.columns
25/81:
dfs = []
for p in Path('./data/private/').glob('*.htm*'):
    print(p)
    name = p.with_suffix('').name
    for i, df in enumerate(pd.read_html(p)):
        df.columns = [str(s).lower().strip().replace(' ', '_') for s in df.columns]
        if 'id' in df.columns:
            print('!!!!!! ID')
            df = df.set_index('id')
        else: 
            df = df.set_index('seq')
        dfs.append((name+str(i or ''), df))
25/82: dfadd.index
25/83: dfadd = d['addcodes']
25/84: dfadd
25/85:
dfs = []
for p in Path('./data/private/').glob('*.htm*'):
    print(p)
    name = p.with_suffix('').name
    for i, df in enumerate(pd.read_html(p)):
        
        if 'seq' in df.columns:
            print('!!!!!! ID')
            df = df.set_index('seq')
        else: 
            df = df.set_index('seq')
        dfs.append((name+str(i or ''), df))
25/86:
dfs = []
for p in Path('./data/private/').glob('*.htm*'):
    print(p)
    name = p.with_suffix('').name
    for i, df in enumerate(pd.read_html(p)):
        
        if 'seq' in df.columns:
            print('!!!!!! Seq')
            df = df.set_index('Seq')
        elif 'ID' in df.columns: 
            print('ID !!!!!!!!!')
            df = df.set_index('ID')
        df.columns = [str(s).lower().strip().replace(' ', '_') for s in df.columns]
        dfs.append((name+str(i or ''), df))
25/87: dfadd = d['addcodes']
25/88: d = dict(dfs)
25/89: d
25/90:
dfs = []
ymd = datetime.now()
ymd = ymd.year, ymd.month, ymd.day
for p in Path('./data/private/').glob('*.htm*'):
    print(p)
    name = p.with_suffix('').name
    path = p.parent / f
    for i, df in enumerate(pd.read_html(p)):
        
        if 'seq' in df.columns:
            print('!!!!!! ID')
            df = df.set_index('seq')
        else: 
            df = df.set_index('seq')
        dfs.append((name+str(i or ''), df))
25/91: hist
25/92:
# process_addcodes.py
from pathlib import Path
import pandas as pd
import datetime

ymd = datetime.now()
ymd = ymd.year, ymd.month, ymd.day
25/93: datetime.datetime.today()
25/94: dt = datetime.datetime.today()
25/95: dt.year
25/96: dt.month
25/97: dt.day
25/98: dt = datetime.date.today()
25/99: dt
25/100: dt.isoformat()
25/101: hist
25/102: dfwait
25/103:
dt = datetime.date.today()
date = dt.isoformat()

data_dir = Path('./data/private')
dfs = []
for p in data_dir.glob('*.htm*'):
    print(p)
    name = p.with_suffix('').name
    for i, df in enumerate(pd.read_html(p)):
        if 'Seq' in df.columns:
            print('!!!!!! Seq')
            df = df.set_index('Seq')
        elif 'ID' in df.columns:
            print('ID !!!!!!')
            df = df.set_index('ID')
        df.columns = [str(s).lower().strip().replace(' ', '_') for s in df.columns]
        dfs.append((name + str(i or ''), df))

d = dict(dfs)
for name, df in d.items():
    df.to_csv((data_dir / name).with_suffix('.{date}.csv.gz'))
25/104: d
25/105:

df = d['roster']
dfadd = d['addcodes']
dfwait = d['waitlist']

for (i, (seq, row)) in enumerate(dfadd[dfwait['comment'].isna()].iterrows()):
    waitcode = row['number']
    student = dfwait.iloc[i]
    print(i, student)
25/106: dfwait
25/107: dfadd
25/108:
df = d['roster']
dfadd = d['addcodes']
dfwait = d['waitlist']
not_yet_added = dfadd['comment'].isna()
for (i, (seq, row)) in enumerate(dfadd[not_yet_added].iterrows()):
    i += sum(not_yet_added)
    waitcode = row['number']
    student = dfwait.iloc[i]
    print(i, student)
25/109: dfadd
25/110: dfadd
25/111:
df = d['roster']
dfadd = d['addcodes']
dfwait = d['waitlist']
not_yet_added = dfadd['comments'].isna()
for (i, (seq, row)) in enumerate(dfadd[not_yet_added].iterrows()):
    i += sum(not_yet_added)
    waitcode = row['number']
    student = dfwait.iloc[i]
    print(i, student)
25/112:
df = d['roster']
dfadd = d['addcodes']
dfwait = d['waitlist']
not_yet_added = dfadd['comments'].isna()
for (i, (seq, row)) in enumerate(dfadd[not_yet_added].iterrows()):
    i += sum(not_yet_added)
    waitcode = row['number']
    if i > len(dfwait):
        break
    student = dfwait.iloc[i]
    print(i, student)
25/113: waitcode
25/114: student
25/115: len(dfwait)
25/116: not_yet_added
25/117:
df = d['roster']
dfadd = d['addcodes']
dfwait = d['waitlist']
not_yet_added = dfadd['comments'].isna()
for (i, (seq, row)) in enumerate(dfadd[not_yet_added].iterrows()):
    i += sum(~not_yet_added)
    waitcode = row['number']
    if i > len(dfwait):
        break
    student = dfwait.iloc[i]
    print(i, student)
25/118: len(dfwait)
25/119: i
25/120:
df = d['roster']
dfadd = d['addcodes']
dfwait = d['waitlist']
not_yet_added = dfadd['comments'].isna()
for (i, (seq, row)) in enumerate(dfadd[not_yet_added].iterrows()):
    i += sum(~not_yet_added)
    waitcode = row['number']
    if i >= len(dfwait):
        break
    student = dfwait.iloc[i]
    print(i, student)
25/121: student.to_dict()
25/122:
df = d['roster']
dfadd = d['addcodes']
dfwait = d['waitlist']
not_yet_added = dfadd['comments'].isna()
for (i, (seq, row)) in enumerate(dfadd[not_yet_added].iterrows()):
    i += sum(~not_yet_added)
    waitcode = row['number']
    if i >= len(dfwait):
        break
    student = dfwait.iloc[i].to_dict()
    student['ID'] = row.index
    print(i, student)
25/123:
df = d['roster']
dfadd = d['addcodes']
dfwait = d['waitlist']
not_yet_added = dfadd['comments'].isna()
for (i, (seq, row)) in enumerate(dfadd[not_yet_added].iterrows()):
    i += sum(~not_yet_added)
    waitcode = row['number']
    if i >= len(dfwait):
        break
    student = dfwait.iloc[i].to_dict()
    student['ID'] = dfwait.index.values[i]
    print(i, student)
25/124:
df = d['roster']
dfadd = d['addcodes']
dfwait = d['waitlist']
dfall = d['all']
not_yet_added = dfadd['comments'].isna()
emails = []
for (i, (seq, row)) in enumerate(dfadd[not_yet_added].iterrows()):
    i += sum(~not_yet_added)
    waitcode = row['number']
    if i >= len(dfwait):
        break
    student = dfwait.iloc[i].to_dict()
    student['ID'] = dfwait.index.values[i]
    student['email'] = df.loc[student['ID']]['email']
    print(student['email'], student)
    emails.append(student)
25/125: hist
25/126:
# process_addcodes.py
from pathlib import Path
import pandas as pd
import datetime

dt = datetime.date.today()
date = dt.isoformat()

data_dir = Path('./data/private')
dfs = []
for p in data_dir.glob('*.htm*'):
    print(p)
    name = p.with_suffix('').name
    for i, df in enumerate(pd.read_html(p)):
        if 'Seq' in df.columns:
            print('!!!!!! Seq')
            df = df.set_index('Seq')
        elif 'ID' in df.columns:
            print('ID !!!!!!')
            df = df.set_index('ID')
        df.columns = [str(s).lower().strip().replace(' ', '_') for s in df.columns]
        dfs.append((name + str(i or ''), df))

d = dict(dfs)
for name, df in d.items():
    df.to_csv((data_dir / name).with_suffix('.{date}.csv.gz'))
25/127:
df = d['roster']
dfadd = d['addcodes']
dfwait = d['waitlist']
dfall = d['all']
not_yet_added = dfadd['comments'].isna()
emails = []
for (i, (seq, row)) in enumerate(dfadd[not_yet_added].iterrows()):
    i += sum(~not_yet_added)
    waitcode = row['number']
    if i >= len(dfwait):
        break
    student = dfwait.iloc[i].to_dict()
    student['ID'] = dfwait.index.values[i]
    student['email'] = df.loc[student['ID']]['email']
    print(student['email'], student)
    emails.append(student)
25/128:
d = dict(dfs)
for name, df in d.items():
    p = (data_dir / name).with_suffix('.{date}.csv.gz')
    df.to_csv(p)
    d[name] = pd.read_csv(p, index_col=0)


df = d['roster']
dfadd = d['addcodes']
dfwait = d['waitlist']
dfall = d['all']
not_yet_added = dfadd['comments'].isna()
emails = []
for (i, (seq, row)) in enumerate(dfadd[not_yet_added].iterrows()):
    i += sum(~not_yet_added)
    waitcode = row['number']
    if i >= len(dfwait):
        break
    student = dfwait.iloc[i].to_dict()
    student['ID'] = dfwait.index.values[i]
    student['email'] = df.loc[student['ID']]['email']
    print(student['email'], student)
    emails.append(student)
25/129: student
25/130: dfall
25/131: dfall['ID']
25/132: dfall.index
25/133: len(dfall)
25/134:
df = d['roster']
dfadd = d['addcodes']
dfwait = d['waitlist']
dfall = d['all']
not_yet_added = dfadd['comments'].isna()
emails = []

for (i, (seq, row)) in enumerate(dfadd[not_yet_added].iterrows()):
    i += sum(~not_yet_added)
    waitcode = row['number']
    if i >= len(dfwait):
        break
    student = dfwait.iloc[i].to_dict()
    student['ID'] = dfwait.index.values[i]
    student.update(dfall.loc[student['ID']].to_dict())
    print(student['status'])
    emails.append(student)
25/135: student
25/136:
df = d['roster']
dfadd = d['addcodes']
dfwait = d['waitlist']
dfall = d['all']
not_yet_added = dfadd['comments'].isna()
emails = []

for (i, (seq, row)) in enumerate(dfadd[not_yet_added].iterrows()):
    i += sum(~not_yet_added)
    waitcode = row['number']
    if i >= len(dfwait):
        break
    student = row.to_dict()
    student.update(dfwait.iloc[i].to_dict())
    student['ID'] = dfwait.index.values[i]
    student.update(dfall.loc[student['ID']].to_dict())
    print(student['name'], student['ID'], student['number'], student['status'] == 'Waiting')
    emails.append(student)
25/137:
df = d['roster']
dfadd = d['addcodes']
dfwait = d['waitlist']
dfall = d['all']
not_yet_added = dfadd['comments'].isna()
emails = []

for (i, (seq, row)) in enumerate(dfadd[not_yet_added].iterrows()):
    i += sum(~not_yet_added)
    waitcode = row['number']
    if i >= len(dfwait):
        break
    student = row.to_dict()
    student.update(dfwait.iloc[i].to_dict())
    student['ID'] = dfwait.index.values[i]
    student.update(dfall.loc[student['ID']].to_dict())
    print(student[['name', 'ID', 'number']], student['status'] == 'Waiting')
    emails.append(student)
25/138:
df = d['roster']
dfadd = d['addcodes']
dfwait = d['waitlist']
dfall = d['all']
not_yet_added = dfadd['comments'].isna()
emails = []

for (i, (seq, row)) in enumerate(dfadd[not_yet_added].iterrows()):
    i += sum(~not_yet_added)
    waitcode = row['number']
    if i >= len(dfwait):
        break
    student = row.to_dict()
    student.update(dfwait.iloc[i].to_dict())
    student['ID'] = dfwait.index.values[i]
    student.update(dfall.loc[student['ID']].to_dict())
    print(student['name'], student['ID'], student['number'], student['status'] == 'Waiting')
    emails.append(student)
25/139: emails
25/140: pd.DataFrame(emails).to_csv(data_dir / 'emails.csv')
25/141: pd.read_html?
25/142: dfemail = pd.DataFrame(emails)
25/143: dfemail.columns
25/144: dfemail['number']
25/145: dfemail['FirstName'] = df['name'].split()[0]
25/146: dfemail['FirstName'] = df['name'].str.split()[0]
25/147: dfemail['FirstName'] = df['name'].str.split().str[0]
25/148: dfemail
25/149: dfemail['FirstName'] = dfemail['name'].str.split(',').str[-1].str.split().str[0]
25/150: dfemail['FirstName']
25/151: dfemail.to_csv(data_dir / 'emails.csv')
25/152: emailadds = 'mkaiser@student.sdccd.edu, atran024@student.sdccd.edu, fklecak@student.sdccd.edu, khudluman001@student.sdccd.edu, lpatapoutian@student.sdccd.edu, jpatrao@student.sdccd.edu, omorales003@student.sdccd.edu, dsanders001@student.sdccd.edu'
25/153: emailadds.split(',')
25/154: emailadds = [e.strip() for e in emailadds.split(',')]
25/155: emailadds
25/156: emails
25/157: dfemail['PrimaryEmail'] = emailadds
25/158: dfemail
25/159: dfemail['PrimaryEmail'] = emailadds[2:-1]
25/160: dfemail['PrimaryEmail'] = emailadds[2:]
25/161: dfemail
25/162: dfemail
25/163: dfemail[1:]
25/164: dfemail[1:].to csv(data_dir / 'waitlist_add_code_emails.csv')
25/165: dfemail[1:].to_csv(data_dir / 'waitlist_add_code_emails.csv')
25/166: emails
25/167: emails
25/168: emailadds
25/169: emails[-1]
25/170: hist
25/171: dfall[dfall['Status']=='Waiting']
25/172: dfall[dfall['status']=='Waiting']
25/173:
maillist = []
for (i, (ID, row)) in enumerate(dfall[dfall['status']=='Waiting'].iterrows()):
    dct = row.to_dict()
    dct['ID'] = ID
    dct.update(dfadd.iloc[i+2].to_dict())
    dct['id'] = ID
    emailadds[row['name']]
25/174: path = (data_dir / 'all.html')
25/175: path.is_file()
25/176:
def find_hrefs(path=(data_dir / 'all.html')):
    path = Path(path)
    with path.open() as f:
        soup = bs.BeautifulSoup(f, 'lxml')
        parsed_table = soup.find_all('table')[1]
    return parsed_table
25/177: find_hrefs()
25/178: import bs4 as bs
25/179: find_hrefs()
25/180:
def find_hrefs(path=(data_dir / 'all.html')):
    path = Path(path)
    with path.open() as f:
        soup = bs.BeautifulSoup(f, 'lxml')

        dfs = []
        for parsed_table in soup.find_all('table'):
            data = [[td.a['href'] if td.find('a') else
                     ''.join(td.stripped_strings)
                     for td in row.find_all('td')]
                    for row in parsed_table.find_all('tr')]
            df = pd.DataFrame(data[1:], columns=data[0])
            dfs.append(df)
    return dfs
25/181: find_hrefs()
25/182:

def find_hrefs(path=(data_dir / 'all.html')):
    path = Path(path)
    with path.open() as f:
        soup = bs.BeautifulSoup(f, 'lxml')

        dfs = []
        for parsed_table in soup.find_all('table'):
            data = [[td.a['href'] if td.find('a') else
                     ''.join(td.stripped_strings)
                     for td in row.find_all('td')]
                    for row in parsed_table.find_all('tr')]
            print(data)
            df = pd.DataFrame(data[1:], columns=data[0])
            dfs.append(df)
    return dfs
25/183: find_hrefs()
25/184: more data/private/all.csv
25/185: more data/private/all.html
25/186: more data/private/allraw.html
25/187:

def find_hrefs(path=(data_dir / 'allraw.html')):
    path = Path(path)
    with path.open() as f:
        soup = bs.BeautifulSoup(f, 'lxml')

        dfs = []
        for parsed_table in soup.find_all('table'):
            data = [[td.a['href'] if td.find('a') else
                     ''.join(td.stripped_strings)
                     for td in row.find_all('td')]
                    for row in parsed_table.find_all('tr')]
            print(data)
            df = pd.DataFrame(data[1:], columns=data[0])
            dfs.append(df)
    return dfs
25/188:

dfs = []
def find_hrefs(path=(data_dir / 'allraw.html')):
    path = Path(path)
    global dfs
    with path.open() as f:
        soup = bs.BeautifulSoup(f, 'lxml')
        for parsed_table in soup.find_all('table'):
            data = [[td.a['href'] if td.find('a') else
                     ''.join(td.stripped_strings)
                     for td in row.find_all('td')]
                    for row in parsed_table.find_all('tr')]
            print(data)
            df = pd.DataFrame(data[1:], columns=data[0])
            dfs.append(df)
    return dfs
25/189: find_hrefs()
25/190: more ~/Downloads/allraw.html
25/191: mv /home/hobs/Downloads/allraw.html data/private/
25/192: find_hrefs()
25/193: !grep 'Pisani,Andrew Jordan' ./data/private/allraw.html
25/194:

dfs = []
def find_hrefs(path=(data_dir / 'allraw.html')):
    path = Path(path)
    global dfs
    with path.open() as f:
        soup = bs.BeautifulSoup(f, 'lxml')
        for parsed_table in soup.find_all('table'):
            data = [[td.a['href'] if td.find('a') else
                     ''.join(td.get('name') or td.stripped_strings)
                     for td in row.find_all('td')]
                    for row in parsed_table.find_all('tr')]
            print(data)
            df = pd.DataFrame(data[1:], columns=data[0])
            dfs.append(df)
    return dfs
25/195: find_hrefs()
25/196: dfs
25/197: hist
25/198:
dfs = []


def find_hrefs(path=(data_dir / 'allraw.html')):
    path = Path(path)
    global dfs
    with path.open() as f:
        soup = bs.BeautifulSoup(f, 'lxml')
        for parsed_table in soup.find_all('table'):
            data = []
            for row in parsed_table.find_all('tr'):
                for td in row.find_all('td'):
                    if td.find('a'):
                        print(td.a)
                        data += [td.a]
                    else:
                        data += [td.text]
            print(data)
            # df = pd.DataFrame(data[1:], columns=data[0])
            # dfs.append(df)
    return dfs
25/199: find_hrefs()
25/200:
dfs = []


def find_hrefs(path=(data_dir / 'allraw.html')):
    path = Path(path)
    global dfs
    with path.open() as f:
        soup = bs.BeautifulSoup(f, 'lxml')
        for parsed_table in soup.find_all('table'):
            data = []
            for row in parsed_table.find_all('tr'):
                for td in row.find_all('td'):
                    if td.find('a'):
                        print(td.a)
                        data += [[td.a.get('name', ''), td.a.get('href', '')]]
                    else:
                        data += ['']
            print(data)
            # df = pd.DataFrame(data[1:], columns=data[0])
            # dfs.append(df)
    return dfs
25/201: find_hrefs()
25/202: data
25/203: data
25/204: find_hrefs()
25/205: dfs = find_hrefs()
25/206: dfs
25/207:
dfs = []


def find_hrefs(path=(data_dir / 'allraw.html')):
    path = Path(path)
    global dfs
    with path.open() as f:
        soup = bs.BeautifulSoup(f, 'lxml')
        for parsed_table in soup.find_all('table'):
            data = []
            for row in parsed_table.find_all('tr'):
                for td in row.find_all('td'):
                    if td.find('a'):
                        print(td.a)
                        data += [[td.a.get('name', ''), td.a.get('href', '')]]
                    else:
                        data += ['']
            print('\n'.join(data))
            # df = pd.DataFrame(data[1:], columns=data[0])
            dfs.append(data)
    return dfs
25/208: dfs = find_hrefs()
25/209:
dfs = []


def find_hrefs(path=(data_dir / 'allraw.html')):
    path = Path(path)
    global dfs
    with path.open() as f:
        soup = bs.BeautifulSoup(f, 'lxml')
        for parsed_table in soup.find_all('table'):
            data = []
            for row in parsed_table.find_all('tr'):
                for td in row.find_all('td'):
                    if td.find('a'):
                        print(td.a)
                        data += [[td.a.get('name', ''), td.a.get('href', '')]]
                    else:
                        data += ['']
            # print('\n'.join(data))
            # df = pd.DataFrame(data[1:], columns=data[0])
            dfs.append(data)
    return dfs
25/210: dfs = find_hrefs()
25/211: dfs[0]
25/212: dfs[1]
25/213: dfs[2]
25/214: dfs[3]
25/215: dfs[4]
25/216: dfs[5]
25/217: dfs[6]
25/218: dfs[7]
25/219: len(dfs)
25/220: dfs = []
25/221: dfs = find_hrefs()
25/222: len(dfs)
25/223:
dfs = []


def find_hrefs(path=(data_dir / 'allraw.html')):
    path = Path(path)
    global dfs
    with path.open() as f:
        soup = bs.BeautifulSoup(f, 'lxml')
        for parsed_table in soup.find_all('table'):
            data = []
            rows = list(parsed_table.find_all('tr'))
            print(len(rows))
            for row in rows:
                for td in row.find_all('td'):
                    if td.find('a'):
                        print(td.a)
                        data += [[td.a.get('name', ''), td.a.get('href', '')]]
                    else:
                        data += ['']
            # print('\n'.join(data))
            # df = pd.DataFrame(data[1:], columns=data[0])
            dfs.append(data)
    return dfs
25/224: dfs = find_hrefs()
25/225:
emailadds = 'mkaiser@student.sdccd.edu, atran024@student.sdccd.edu, fklecak@student.sdccd.edu, khudluman001@student.sdccd.edu, lpatapoutian@student.sdccd.edu, jpatrao@student.sdccd.edu, omorales003@student.sdccd.edu, dsanders001@student.sdccd.edu'
emailadds = [e.strip() for e in emailadds.split(',')]
25/226: emailadds
25/227: len(emailadds)
25/228:
# process_addcodes.py
import bs4 as bs
from pathlib import Path
import pandas as pd
import datetime

dt = datetime.date.today()
date = dt.isoformat()

data_dir = Path('./data/private')
dfs = []
for p in data_dir.glob('*.htm*'):
    print(p)
    name = p.with_suffix('').name
    for i, df in enumerate(pd.read_html(p)):
        if 'Seq' in df.columns:
            print('!!!!!! Seq')
            df = df.set_index('Seq')
        elif 'ID' in df.columns:
            print('ID !!!!!!')
            df = df.set_index('ID')
        df.columns = [str(s).lower().strip().replace(' ', '_') for s in df.columns]
        dfs.append((name + str(i or ''), df))

d = dict(dfs)
for name, df in d.items():
    p = (data_dir / name).with_suffix(f'.{date}.csv.gz')
    df.to_csv(p)
    d[name] = pd.read_csv(p, index_col=0)


df = d['roster']
dfadd = d['addcodes']
dfwait = d['waitlist']
dfall = d['all']
not_yet_added = dfadd['comments'].isna()
emails = []

for (i, (seq, row)) in enumerate(dfadd[not_yet_added].iterrows()):
    i += sum(~not_yet_added)
    waitcode = row['number']
    if i >= len(dfwait):
        break
    student = row.to_dict()
    student.update(dfwait.iloc[i].to_dict())
    student['ID'] = dfwait.index.values[i]
    student.update(dfall.loc[student['ID']].to_dict())
    print(student['name'], student['ID'], student['number'], student['status'] == 'Waiting')
    emails.append(student)
25/229: dfall.to_csv(data_dir / 'all.csv')
25/230: df = data_dir / 'allwithwaitlistemails.csv'
25/231: df = pd.read_csv(df)
25/232: df
25/233: waiting = df['status'] == 'Waiting'
25/234: df[waiting]
25/235: columns = list(df.columns)
25/236: df['FirstName'] = df['last_known_academic_activity']
25/237: df[waiting]
25/238: df[waiting].to_csv(data_dir / 'waiting.csv')
25/239: hist -o -p -f scripts/email_add_codes_failed.hist.ipy
25/240: hist -f scripts/email_add_codes_failed.hist.py
26/1:
# process_addcodes.py
import bs4 as bs
from pathlib import Path
import pandas as pd
import datetime

dt = datetime.date.today()
date = dt.isoformat()

data_dir = Path('./data/private')
dfs = []
for p in data_dir.glob('*.htm*'):
    print(p)
    name = p.with_suffix('').name
    for i, df in enumerate(pd.read_html(p)):
        if 'Seq' in df.columns:
            print('!!!!!! Seq')
            df = df.set_index('Seq')
        elif 'ID' in df.columns:
            print('ID !!!!!!')
            df = df.set_index('ID')
        df.columns = [str(s).lower().strip().replace(' ', '_') for s in df.columns]
        dfs.append((name + str(i or ''), df))

d = dict(dfs)
for name, df in d.items():
    p = (data_dir / name).with_suffix(f'.{date}.csv.gz')
    df.to_csv(p)
    d[name] = pd.read_csv(p, index_col=0)


df = d['roster']
dfadd = d['addcodes']
dfwait = d['waitlist']
dfall = d['all']
not_yet_added = dfadd['comments'].isna()
emails = []

for (i, (seq, row)) in enumerate(dfadd[not_yet_added].iterrows()):
    i += sum(~not_yet_added)
    waitcode = row['number']
    if i >= len(dfwait):
        break
    student = row.to_dict()
    student.update(dfwait.iloc[i].to_dict())
    student['ID'] = dfwait.index.values[i]
    student.update(dfall.loc[student['ID']].to_dict())
    print(student['name'], student['ID'], student['number'], student['status'] == 'Waiting')
    emails.append(student)
26/2: df
26/3: dfall
26/4: dfall.email
26/5: dfall.iloc[0]
26/6: dfall = data_dir / 'allwithwaitlistemails.csv'
26/7: dfall = read_csv(dfall)
26/8: dfall = pd.read_csv(dfall)
26/9: dfall.email
26/10: dfall[dfall['status']=='Waiting']
26/11: dfall[dfall['status']=='Enrolled']
26/12: dfall['runestone_username'] = dfall['name'].str.split(',').str[0]
26/13: dfall['runestone_username']
26/14: enrolled = dfall['status']=='Enrolled'
26/15: dfall['runestone_username'][enrolled]
26/16: len(dfall['runestone_username'][enrolled])
26/17: dfall['runestone_username'][enrolled].nunique()
26/18: dfall = data_dir / 'allwithrunestoninfo.csv'
26/19: dfall = pd.read_csv(dfall)
26/20: dfall = data_dir / 'allwithrunestoneinfo.csv'
26/21: dfall = pd.read_csv(dfall)
26/22: waiting = dfall['status']=='Waiting'
26/23: enrolled = dfall['status']=='Enrolled'
26/24: dfall['runestone_username'] = dfall['name'].str.split(',').str[0]
26/25: dfall['runestone_password'] = dfall['ID'].str[-6:]
26/26: dfall['runestone_password'] = dfall['ID'].astype('str').str[-6:]
26/27: dfall.iloc[-1]
26/28: columns = 'username,email,first_name,last_name,password,course'.split(',')
26/29: dfall['runestone_email'] = dfall['runestone_username'].apply(x: f'mesa2024_student_{x}@totalgood.com')
26/30: dfall['runestone_email'] = dfall['runestone_username'].apply(lambda x: f'mesa2024_student_{x}@totalgood.com')
26/31: dfall.iloc[-1]
26/32: dfall['runestone_email'] = dfall['runestone_username'].apply(lambda x: f'mesa2024_student_{x}@totalgood.com'.lower())
26/33: dfall.iloc[-1]
26/34: dfall['last_name'] = dfall['runestone_username']
26/35: dfall['runestone_username'] = dfall['name'].str.split(',').str[0] + '24'
26/36: dfall['runestone_username'] = (dfall['name'].str.split(',').str[0] + '24').str.lower().str.replace(' ', '_')
26/37: dfall['runestone_username']
26/38: dfall['runestone_username'].values
26/39: dfall['runestone_username'] = (dfall['name'].str.split(',').str[0] + '24').str.lower().str.replace(' ', '_').str.replace('.', '')
26/40: dfall['last_name']
26/41: dfall['first_name'] = (dfall['name'].str.split(',').str[1].str.split().str[0])
26/42: dfall['first_name'].unique().values
26/43: dfall['first_name'].unique()
26/44: len(dfall['first_name'].unique())
26/45: len(dfall['first_name'])
26/46: len(dfall['first_name'][enrolled])
26/47: len(dfall['first_name'][enrolled].unique())
26/48: len(dfall['last_name'][enrolled].unique())
26/49: columns
26/50: dfall.columns
26/51: dfall['course'] = 'CISC-179 Python Programming'
26/52: dfall
26/53: dfall[[columns]]
26/54: dfall[columns]
26/55: dfall['username'] = dfall['runestone_username']
26/56: dfall['password'] = dfall['runestone_password']
26/57: dfrune = pd.DataFrame([],columns=columns)
26/58: dfrune = dfall[columns]
26/59: dfrune
26/60: columns = 'runestone_username,runestone_email,first_name,last_name,runestone_password,course'.split(',')
26/61: dfrune = dfall[columns]
26/62: dfrune
26/63: dfrune.to_csv('runestone_roster.csv')
26/64: dfrune.to_csv('runestone_roster.csv',header=None)
26/65: more runestone_roster.csv
26/66: columns = 'runestone_username,runestone_email,first_name,last_name,runestone_password,runestone_course'.split(',')
26/67: dfall['course'] = 'mesa_python'
26/68: dfall['course'] = 'CISC-179 Python Programming'
26/69: dfall['runestone_course'] = 'mesa_python'
26/70: dfrune = dfall[columns]
26/71: dfrune.to_csv('runestone_roster.csv',header=None)
26/72: dfall.to_csv(data_dir / 'allwithrunestoneinfo.csv')
26/73: dfrune.to_csv(data_dir / 'runestone_roster_with_header.csv',header=None)
26/74: dfrune.to_csv(data_dir / 'runestone_roster_no_header.csv',header=None)
26/75: dfrune.to_csv(data_dir / 'runestone_roster_with_header.csv')
26/76: dfrune[enrolled].to_csv(data_dir / 'runestone_roster_with_header.csv')
26/77: dfrune[enrolled].to_csv(data_dir / 'runestone_roster_no_header.csv',header=None,index=None)
26/78: more data/private/runestone_roster_no_header.csv
27/1: hist ~2/
27/2: hist ~1/
28/1: url = 'https://runestone.academy/ns/books/published/fopp/index.html?mode=browsing'
28/2: import requests
28/3: r = requests.get(url)
28/4: import bs4 as bs
28/5: from bs4 import BeautifulSoup as Soup
28/6: s = Soup(r.text)
28/7: hrefs = list(s.find_all('href'))
28/8: hrefs
28/9: hrefs = list(s.find_all('a'))
28/10: hrefs
28/11: hrefs = [a.get('name'), a.get('href', '') for a in s.find_all('a')]
28/12: hrefs = [(a.get('name'), a.get('href', '')) for a in s.find_all('a')]
28/13: hrefs
28/14: dir(a)
28/15: anchors = list(s.find_all('a'))
28/16: dir(anchors[10])
28/17: dir(anchors[10].attrs)
28/18: list(anchors[10].attrs)
28/19: list(anchors[10].text)
28/20: anchors[10].text
28/21: hrefs = [(a.text, a.href) for a in s.find_all('a')]
28/22: anchors[10].attrs
28/23:
anchors = []
for a in s.find_all('a'):
    anchors.append(a.attrs)
    anchors[-1]['text'] = text
28/24:
anchors = []
for a in s.find_all('a'):
    anchors.append(a.attrs)
    anchors[-1]['text'] = a.text
28/25: import pandas as pd
28/26: df = pd.DataFrame(anchors)
28/27: df
28/28: df.head(30)
28/29: df[['text', 'href']]
28/30: df['is_book'] = df['text'].str.strip()..str.startswith(r'\d?\d\.')
28/31: df['is_book'] = df['text'].str.strip().str.startswith(r'\d?\d\.')
28/32: is_book = df['text'].str.strip().str.startswith(r'\d?\d\.')
28/33: df[is_book]
28/34: import re
28/35: is_book = df['text'].str.strip().str.startswith(re.compile(r'\d?\d\.'))
28/36: is_book = df['text'].str.strip().str.match(re.compile(r'\d?\d\.'))
28/37: sum(is_book)
28/38: df[is_book]
28/39: df[is_book]['text href'.split()]
28/40: from tqdm import tqdm
28/41:
for title, href in tqdm(zip(df[is_book]['text href'.split()].T)):
    print(title, href)
28/42:
for title, href in tqdm(zip(df[is_book]['text'], df[is_book]['href'])):
    print(title, href)
28/43: url
28/44:
from urllib.parse.urlparse

sections = urlparse(url)
28/45:
from urllib.parse import urlparse

sections = urlparse(url)
28/46: sections
28/47: sections.path.parent
28/48: str(sections)
28/49: sections.text
28/50: sections._asdict()
28/51: sections.asdict()
28/52: sections.encode()
28/53: sections.geturl()
28/54:
url = parseurl(url)
url.__str__ = url.get_url
28/55:
url = urlparse(url)
url.__str__ = url.get_url
28/56:
url = urlparse(url)
url.__str__ = url.geturl
28/57: url = urlparse(url)
28/58: url
28/59: url.__str__ = url.geturl
28/60: url
28/61: Path(url.path).parent
28/62: from pathlib import Path
28/63: Path(url.path).parent
28/64: baseurl = url
28/65: baseurl.path = Path(url.path).parent
28/66: baseurlstr = url.scheme + '///' + str(Path(url.path).parent)
28/67: url = 'https://runestone.academy/ns/books/published/fopp/index.html?mode=browsing'
28/68: urlstr = 'https://runestone.academy/ns/books/published/fopp/index.html?mode=browsing'
28/69: baseurlstr = url.scheme + '///' + str(Path(url.path).parent)
28/70: url = parseurl(url)
28/71: url = urlparse(url)
28/72: baseurlstr = url.scheme + '///' + str(Path(url.path).parent)
28/73:
pages = []
for title, hrefstr in tqdm(zip(df[is_book]['text'], df[is_book]['href'])):
    href = urlparse(href)
    print(title, href)
28/74: href.asdict
28/75: url.asdict()
28/76: dict(url)
28/77: url
28/78: vars(url)
28/79: dir(url)
28/80: url
28/81: url._asdict()
28/82:
for a in anchors:
    a.update(urlparse(a['href']))
28/83: anchors[0]
28/84: anchors[0]['href']
28/85: urlparse(anchors[0]['href'])
28/86:
for a in anchors:
    a.update(urlparse(a.get('href', '')))
28/87:
for a in anchors:
        a.update(urlparse(a.get('href', ''))._asdict())
28/88: df = pd.DataFrame(anchors)
28/89: anchors['path']
28/90: df['path']
28/91:
for a in tqdm(anchors):
    print(a['title'], a['netloc'] or baseurl.scheme, a['path'])
28/92:
for a in tqdm(anchors):
    print(a['text'], a['netloc'] or baseurl.scheme, a['path'])
28/93:
for a in tqdm(anchors):
    print(a['text'], a['netloc'] or baseurl.netloc, a['path'])
28/94:
for a in tqdm(anchors):
    print(a['text'], (a['scheme'] or baseurl.scheme) + ':///' + (a['netloc'] or baseurl.netloc) + '/' + a['path'])
28/95:
for a in tqdm(anchors):
    print(a['text'], (a['scheme'] or baseurl.scheme) + ':///' + (a['netloc'] or baseurl.netloc) + '/' + a['path'])
    sleep(.27)
28/96: imoport time
28/97: import time
28/98:
for a in tqdm(anchors):
    print(a['text'], (a['scheme'] or baseurl.scheme) + ':///' + (a['netloc'] or baseurl.netloc) + '/' + a['path'])
    time.sleep(.27)
28/99:
mkdir textbook

for a in tqdm(anchors):
    break
    a['url'] = (a['scheme'] or baseurl.scheme) + ':///' + (a['netloc'] or baseurl.netloc) + '/' + a['path']
    r = request.get(a['url'])
    path = Path(a['text'].replace('.', '_'))
    path.open
    time.sleep(.17)
28/100: ls
28/101: pwd
28/102: cd code/hobs/mesa/2024/cisc-179-private/
28/103: ls
28/104: cd content
28/105: ls
28/106: mkdir resources-student
28/107: cd resources-student
28/108: mkdir fopp_html
28/109: cd fopp_html
28/110:
mkdir textbook

for a in tqdm(anchors):
    a['url'] = (a['scheme'] or baseurl.scheme) + ':///' + (a['netloc'] or baseurl.netloc) + '/' + a['path']
    r = request.get(a['url'])
    a['page_text'] = r['text']
    path = Path(a['text'].replace('.', '_'))
    print(path)
    path.with_suffix('.html').open('w').write(r.text)
    time.sleep(.17)
28/111:
for a in tqdm(anchors):
    a['url'] = (a['scheme'] or baseurl.scheme) + ':///' + (a['netloc'] or baseurl.netloc) + '/' + a['path']
    r = request.get(a['url'])
    a['page_text'] = r['text']
    path = Path(a['text'].replace('.', '_'))
    print(path)
    path.with_suffix('.html').open('w').write(r.text)
    time.sleep(.17)
28/112:
for a in tqdm(anchors):
    a['url'] = (a['scheme'] or baseurl.scheme) + ':///' + (a['netloc'] or baseurl.netloc) + '/' + a['path']
    r = requests.get(a['url'])
    a['page_text'] = r['text']
    path = Path(a['text'].replace('.', '_'))
    print(path)
    path.with_suffix('.html').open('w').write(r.text)
    time.sleep(.17)
28/113:
for a in tqdm(anchors):
    a['url'] = (a['scheme'] or baseurl.scheme) + ':///' + '/'.join([(a['netloc'] or baseurl.netloc), a['path']])
    print(a['url'])
    r = requests.get(a['url'])
    a['page_text'] = r['text']
    path = Path(a['text'].replace('.', '_'))
    print(path)
    path.with_suffix('.html').open('w').write(r.text)
    time.sleep(.17)
28/114:
for a, i in tqdm(zip(anchors, is_book)):
    if not i:
        continue
    a['url'] = (a['scheme'] or baseurl.scheme) + ':///' + '/'.join([(a['netloc'] or baseurl.netloc), a['path']])
    print(a['url'])
    r = requests.get(a['url'])
    a['page_text'] = r['text']
    path = Path(a['text'].replace('.', '_'))
    print(path)
    path.with_suffix('.html').open('w').write(r.text)
    time.sleep(.17)
28/115: baseurl
28/116: baseurlstr
28/117: baseurlstr = url.scheme + '///' + url.netloc + str(Path(url.path).parent)
28/118: baseurlstr
28/119: baseurlstr + 'GeneralIntro/toctree.html'
28/120: baseurlstr + '/GeneralIntro/toctree.html'
28/121: newurl = _
28/122: newurl2 = 'https://runestone.academy/ns/books/published/fopp/index.html?mode=browsing'
28/123: newurl2
28/124: newurl
28/125: newurl3 = 'https://runestone.academy/ns/books/published/fopp/GeneralIntro/toctree.html?mode=browsing'
28/126: newurl3
28/127: newurltemplate = 'https://runestone.academy/ns/books/published/fopp/{path}?mode=browsing'
28/128:
for a, i in tqdm(zip(anchors, is_book)):
    if not i:
        continue
    path = a['path']
    print(path)
    a['url_old'] = (a['scheme'] or baseurl.scheme) + ':///' + '/'.join([(a['netloc'] or baseurl.netloc), a['path']])
    a['url'] = newurltemplate.format(path=path)
    print(a['url'])
    r = requests.get(a['url'])
    a['page_text'] = r['text']
    path2 = Path(a['text'].replace('.', '_'))
    print(path2)
    path = Path(path)
    filename = Path(path.with_suffix('.html').name)
    print(filename)
    filename.open('w').write(r.text)
    time.sleep(.17)
28/129:
for a, i in tqdm(zip(anchors, is_book)):
    if not i:
        continue
    path = a['path']
    print(path)
    a['url_old'] = (a['scheme'] or baseurl.scheme) + ':///' + '/'.join([(a['netloc'] or baseurl.netloc), a['path']])
    a['url'] = newurltemplate.format(path=path)
    print(a['url'])
    r = requests.get(a['url'])
    a['page_text'] = r.text
    path2 = Path(a['text'].replace('.', '_'))
    print(path2)
    path = Path(path)
    filename = Path(path.with_suffix('.html').name)
    print(filename)
    filename.open('w').write(r.text)
    time.sleep(.17)
28/130: hist -o -p -f scripts/scrape_textbook.hist.ipy
28/131: cd ..
28/132: cd ..
28/133: cd ..
28/134: hist -o -p -f scripts/scrape_textbook.hist.ipy
28/135: hist -f scripts/scrape_textbook.hist.py
30/1: hist
30/2: hist ~1
30/3: hist ~2
30/4: hist ~2/
30/5: newurltemplate = 'https://runestone.academy/ns/books/published/fopp/{path}?mode=browsing'
30/6: hist
30/7: url = 'https://runestone.academy/ns/books/published/fopp/index.html?mode=browsing'
30/8:
r = requests.get(url)
s = Soup(r.text)
anchors = []
for a in s.find_all('a'):
    anchors.append(a.attrs)
    anchors[-1]['text'] = a.text
30/9: import requests
30/10: from bs4 import BeautifulSoup as Soup
30/11: hist
30/12: df['path']
30/13: hist | grep path
30/14: hist ~2/ | grep path
30/15: hist ~2/
30/16:
# scrape_textbook.py
from tqdm import tqdm
from pathlib import Path
from bs4 import BeautifulSoup as Soup
from urllib import urlparse
import re
import pandas as pd
import requests
import time


def scrape_textbook(
        url='https://runestone.academy/ns/books/published/fopp/index.html?mode=browsing',
        url_template='https://runestone.academy/ns/books/published/fopp/{relpath}?mode=browsing'):
    r = requests.get(url)
    s = Soup(r.text)
    anchors = []
    for a in s.find_all('a'):
        anchors.append(a.attrs)
        anchors[-1]['heading'] = a.text
        anchors[-1].update(urlparse(a.get('href', ''))._asdict())
    df = pd.DataFrame(anchors)
    is_book = df['heading'].str.strip().str.match(re.compile(r'\d?\d\.'))

    baseurl = urlparse(url_template.split('{relpath}')[0])
    for a, i in tqdm(zip(anchors, is_book)):
        if not i:
            continue
        a['url_old'] = (a['scheme'] or baseurl.scheme) + ':///' + '/'.join([(a['netloc'] or baseurl.netloc), a['path']])
        a['url'] = url_template.format(path=a['path'])
        r = requests.get(a['url'])
        a['text'] = r.text
        path = Path(a['path'])
        filename = Path(path.with_suffix('.html').name)
        filename.open('w').write(r.text)
        time.sleep(.17)
    return pd.DataFrame(anchors)
30/17: %run scripts/scrape_textbook.py
30/18: df = scrape_textbook()
30/19:
        url='https://runestone.academy/ns/books/published/fopp/index.html?mode=browsing'
        url_template='https://runestone.academy/ns/books/published/fopp/{relpath}?mode=browsing'
30/20: url_template.split('{relpath}')
30/21: %run scripts/scrape_textbook.py
30/22: scrape_textbook()
30/23: scrape_textbook()
30/24: df = _
30/25: df['path']
30/26: df['path'].iloc[495]
30/27: Path(df['path'].iloc[495])
30/28: Path(df['path'].iloc[495]).parent.mkdir(exists_ok=True, parents=True)
30/29: Path(df['path'].iloc[495]).parent.mkdir(exist_ok=True, parents=True)
30/30: rm -r GeneralIntro/
30/31: mkdir data/fopp/
30/32: cd data
30/33: cd fopp
30/34: ls -hal
30/35: !pip install pickleshare
30/36: !sudo pip install pickleshare
31/1: hell
31/2: git status
32/1: %run scripts/scrape_textbook.py
32/2: who
32/3: DATA_DIR
32/4: scrape_textbook()
33/1: ls *.py
33/2: from search_engine import *
33/3: who
33/4: model
33/5: ls *.py
33/6: from llm import *
33/7: who
33/8: rag?
33/9: hist -g | grep rag
33/10: hist -g
33/11: hist -g -f scripts/hist_g_2024_02_01.hist.ipy
33/12: hist 21/
33/13: hist 22/
33/14: hist 22/ -f scripts/what_is_healthiest_fruit.hist.py
33/15: hist 21/ -f scripts/scrape_hpr.hist.py
33/16: hist 23/
33/17: hist 20/
33/18: hist 19/
33/19: hist 18/
33/20: hist 17/
33/21: hist 16/
33/22: hist -g
33/23: [].get(0)
33/24: [].__getitem__(0)
33/25: setattr([], x, 5)
33/26: setattr([], 'x', 5)
33/27: vars([])
33/28: vars(object())
33/29: vars(object)
34/1: from llm import *
34/2: rag = RAG()
34/3: rag.ask('What is the healthiest protein?')
34/4: %run llm
34/5: rag = RAG()
34/6: rag.ask('What is the healthiest protein?')
34/7: %run llm
34/8: rag.ask('What is the healthiest protein?')
34/9: rag = RAG()
34/10: rag.ask('What is the healthiest protein?')
34/11: rag.hist
34/12: %run llm
34/13: rag.ask('What is the healthiest protein?')
34/14: rag.hist
34/15: exot
35/1: %run llm
35/2: rag = RAG()
35/3: rag.ask('What is the healthiest protein?')
35/4: %run llm
35/5: rag = RAG()
35/6: rag.ask('What is the healthiest protein?')
35/7: c = _
35/8: c.content
35/9: c.keys()
35/10: c
35/11: c.message
35/12: c['message']
35/13: dir(c)
35/14: c.choices
35/15: c.choices[0]
35/16: c.choices[0].message
35/17: c
35/18: c.id
35/19: %run llm
35/20: rag = RAG()
35/21: rag.ask('What are some healthy proteins for a young man?')
35/22: %run llm
35/23: rag = RAG()
35/24: rag.ask('What are some healthy proteins for a young man?')
35/25: rag.context
36/1: from llm import *
36/2: rag = R
36/3: rag = RAG()
36/4: cd ..
36/5: pwd
37/1: from llm import *
37/2: rag = RAG()
37/3: from search_engine import *
37/4: load_data(globs='*.md')
37/5: load_data(globs='*.html.md')
37/6: ls data/corpus
37/7: rm data/corpus/nutrition_sentences.csv
37/8: ls data/
37/9: rm data/corpus/
37/10: rm -r data/corpus/
37/11: mv data/corpus_fopp/ data/corpus
38/1: from llm import *
38/2: rag = RAG()
38/3: from search_engine import *
38/4: df = load_data(globs='*.html.md')
38/5: df
39/1: from search_engine import *
39/2: df = load_data()
39/3: df = load_data(glob='*.html.md')
39/4: df = load_data(globs='*.html.md')
39/5: mv data/corpus_nutrition data/corpus
39/6: mv data/corpus data/corpus_nutrition
39/7: ls data/
39/8: ls ../../
39/9: ls ../../../
39/10: ls ../../../../
39/11: ls ../../../../../
39/12: ls ../../../../
39/13: ls ../../../../code/hobs/mesa/2024/cisc-179-private/data/
39/14: ls ../../../../code/hobs/mesa/2024/cisc-179-private/data/fopp_md
39/15: cp -r ../../../../code/hobs/mesa/2024/cisc-179-private/data/fopp_md/ data/corpus_fopp
39/16: cp -r data/corpus_fopp data/corpus
40/1: from search_engine import *
40/2: who
40/3: df = load_data(globs='*.html.md')
40/4: globs = ('*.md',)
40/5:
    data_dir = DATA_DIR / 'corpus'
    filepaths = []
    for g in globs:
        print(g)
        filepaths.extend(list(data_dir.glob(g)))
        print(len(filepaths))
40/6: data_dir.glob('**/*.md')
40/7: list(data_dir.glob('**/*.md'))
41/1: from search_engine import *
41/2: db = VectorDB()
41/3: db.search('What is a Python function?')
41/4: who
41/5: dir(db)
41/6: db.cli()
41/7: from llm import *
41/8: rag = RAG()
41/9: rag.ask('What is a list comprehension?')
41/10: rag.hist
41/11: rag.answer
41/12: rag.answer.content
41/13: rag.answer_id
41/14: rag.answer_logprob
41/15: %run llm
41/16: rag = RAG()
41/17: rag.ask('What is a function?')
41/18: rag.db
41/19: rag.hist
41/20:
print(rag.hist[-1]['prompt']
)
41/21: hist -o -p -f scripts/fopp_function_list_comprehensions.hist.ipy
41/22: hist -f scripts/fopp_function_list_comprehensions.hist.py
42/1: # dfall.to_csv(data_dir / 'allwithrunestoneinfo.csv')
43/1: from scripts.create_runestone_roster
43/2: from scripts.create_runestone_roster import *
43/3: %run scripts/create_runestone_roster
43/4: %run scripts/create_runstone_roster.py
43/5: mv scripts/create_runstone_roster.py scripts/create_runestone_roster.py
43/6: who
43/7: pd.read_csv('data/private/2024-02-04-all-myportal-contact-list-personal-email-student-sdccd-email.csv')
43/8: df = _
43/9: dfold = pd.read_csv('data/private/all-myportal-contact-list-personal-email-student-sdccd-email.csv')
43/10: ls -hal data/private/all-myportal*.csv
43/11: mv data/private/all-myportal-contact-list-personal-email-student-sdccd-email.csv data/private/2023-01-30-all-myportal-contact-list-personal-email-student-sdccd-email.csv
43/12: df['Student Email'].unique()
43/13: set(df['Student Email'])
43/14: set(df['Student ID']) - set(dfold['Student ID'])
43/15: df.set_index('Student ID')
43/16: df = df.set_index('Student ID', drop=True)
43/17: df
43/18: (set(df['Student ID']) - set(dfold['Student ID']))[0]
43/19: df.loc['5550208057']
43/20: df.loc[5550208057]
43/21: dfold.loc[5550208057]
43/22: ls data/private/*stone*.csv
43/23: dfold = pd.read_csv('data/private/runestone_roster_with_header.csv')
43/24: dfold
43/25: dfold.loc[5550208057]
43/26: dfruneold = dfold
43/27: dfold = pd.read_csv('data/private/runestone_roster_with_header.csv')
43/28: dfold.loc[5550208057]
43/29: dfold = pd.read_csv('data/private/runestone_roster_with_header.csv', index_col=0)
43/30: dfold.loc[5550208057]
43/31: dfold.loc['5550208057']
43/32: dfold
43/33: dfold = pd.read_csv('data/private/all-myportal-contact-list-personal-email-student-sdccd-email.csv')
43/34: dfold = pd.read_csv('data/private/2023-01-30-all-myportal-contact-list-personal-email-student-sdccd-email.csv')
43/35: ls -hal 'data/private/runestone_roster_with_header.csv'
43/36: mv 'data/private/runestone_roster_with_header.csv' data/private/2023-01-28-runestone_roster_with_header.csv
43/37: dfold
43/38: dfold = pd.read_csv('data/private/2023-01-30-all-myportal-contact-list-personal-email-student-sdccd-email.csv', index_col=0)
43/39: dfold
43/40: dfold.loc[5550208057]
43/41: dfold.loc['5550208057']
43/42: df.loc[5550208057]
43/43: g = pd.read_csv('data/private/grades/runestone_autograde_ch01.txt', delimiter=':')
43/44: g
43/45: g = pd.read_csv('data/private/grades/runestone_autograde_ch01.txt', delimiter=':', header=None)
43/46: g
43/47: g[0]
43/48: del g[0]
43/49: g
43/50: g[0]
43/51: g[1]
43/52: g['username'] = g[1].str.split().str[0]
43/53: g[1].str.split('autograded')[0]
43/54: g[1].str.split('autograded')
43/55: g[1].str.split('autograded').str[0]
43/56: g[1].str.split('autograded').split().str[1:]
43/57: g[1].str.split().str[1:]
43/58: g[1].str.split().str[1:-4]
43/59: g[1].str.split().str[1:-4].apply(lambda x: ' '.join(x))
43/60: g['full_name'] = g[1].str.split().str[1:-4].apply(lambda x: ' '.join(x))
43/61: del g[1]
43/62: g
43/63: g.columns = 'grade username full_name'.split()
43/64: g.to_csv('data/private/grades/runestone_autograde_ch01_2024-02-04.csv', index=False)
43/65: hist
43/66:
import pandas as pd
chnum = 2
g = pd.read_csv(f'data/private/grades/runestone_autograde_ch{chnum:02d}.txt', delimiter=':', header=None)
43/67:
import pandas as pd
chnum = 1
g = pd.read_csv(f'data/private/grades/runestone_autograde_ch{chnum:02d}.txt', delimiter=':', header=None)
44/1: import pandas as pd
44/2: pd.read_csv('data/private/2023-01-28-runestone_roster_with_header.csv')
44/3: pwd
45/1: df = pd.read_csv('data/private/2023-01-28-runestone_roster_with_header.csv')
45/2: import pandas as pd
45/3: df = pd.read_csv('data/private/2023-01-28-runestone_roster_with_header.csv')
45/4: df
45/5: df = pd.read_csv('data/private/2023-01-28-runestone_roster_with_header.csv', index_col=0)
45/6: df
45/7: hist -o -p
46/1: from cisc179 import constants
46/2: cd src
46/3: from cisc179 import constants
46/4: %run cisc179.constants
46/5: %run cisc179/constants
46/6: ls cisc179
46/7: %run cisc179/constants
46/8: from cisc179 import canvas
46/9: from cisc179 import canvas
46/10: from cisc179.canvas import *
46/11: who
46/12: api = CourseApi()
46/13: api = CourseAPI()
46/14: api = CourseAPI()
47/1: from cisc179.canvas import *
48/1: from cisc179.canvas import *
48/2: api = CourseAPI()
48/3: $run cisc179/canvas
48/4: %run cisc179/canvas
49/1: from cisc179.canvas import *
49/2: api = CourseAPI()
50/1: from cisc179.canvas import *
50/2: api = CourseAPI()
51/1: from cisc179.canvas import *
51/2: api = CourseAPI()
52/1: from cisc179.canvas import *
52/2: api = CourseAPI()
52/3: from cisc179.canvas import *
53/1: api = CourseAPI()
53/2: from cisc179.canvas import *
53/3: api = CourseAPI()
53/4: api.section_api
53/5: api
53/6: repr(api)
53/7: vars(api)
53/8: {k: v for (k, v) in vars(api).items() if not k.startswith('_')}
53/9: d = vars(api); str({k: d[k] for k in 'course_name course_id section_api'.split()})
53/10: d = vars(api); str({k: d[k] for k in 'course_name course_id section_api'.split()})
54/1: api = CourseAPI()
54/2: from cisc179.canvas import *
54/3: api = CourseAPI()
54/4: api
54/5: api.course_api
54/6: api.sections
54/7: len(api.sections)
54/8: api = CourseAPI('179')
54/9: api.sections
54/10: len(api.sections)
54/11:
    canvas = Canvas(CANVAS_API_URL, CANVAS_API_KEY)
    print(canvas)

    # interactive, noupdate, config = .get_options()
    # canvas = config.endpoint.login()
    courses = list(canvas.get_courses())
    print(courses)

    course_id = next(iter([c.id for c in courses if 'CISC 179' in c.name]))
    print(course_id)

    course = canvas.get_course(course_id)
    grade_changes = course.get_grade_change_events()
    print(grade_changes)
54/12: courses
54/13: len(courses)
54/14:
    course_id = next(iter([c.id for c in courses if '179' in c.name]))
    print(course_id)
54/15: import re
54/16:     [c.id for c in courses if re.match('CISC 179', c.name)]
54/17:     [c.id for c in courses if re.find_all('CISC 179', c.name)]
54/18:     [c.id for c in courses if re.findall('CISC 179', c.name)]
54/19: import re
55/1: from cisc179.canvas import *
55/2: api = CourseAPI('179')
55/3: api.course_id
55/4: api
55/5: api = CourseAPI('CISC 179')
55/6: api
55/7: api.courses[-1]
55/8: api.courses[-1].name
55/9: api.section_api.get_enrollments()
55/10: enrollments = list(api.section_api.get_enrollments())
55/11: len(enrollments)
55/12: enrollments[0]
55/13: vars(enrollments[0])
55/14: student_summaries = list(api.course_api.get_course_level_student_summary_data())
55/15: student_summaries
55/16: pd.DataFrame(student_summaries)
55/17: vars(student_summaries[0])
55/18: [enr.user for enr in enrollments]
55/19: enrollments[0]
55/20: dir(enrollments[0])
55/21: users = [enr.user for enr in enrollments]
55/22: users[0]
55/23:
    enrollments = list(api.section_api.get_enrollments())
    statuses = [enr.user for enr in enrollments]
    for s, e in zip(statuses, enrollments):
        s.update({getattr(e, k) for k in dir(e) if 'activity' in k})
55/24: s
55/25: e
55/26: dir(e)
55/27:
    for s, e in zip(statuses, enrollments):
        print(s)
        s.update({getattr(e, k) for k in dir(e) if 'activity' in k})
55/28:
    for s, e in zip(statuses, enrollments):
        print(s)
        s.update({k: getattr(e, k) for k in dir(e) if 'activity' in k})
55/29: student_summaries = list(api.course_api.get_course_level_student_summary_data())
55/30: student_summaries[0]
56/1: api = CourseStatus('CISC 179')
56/2: from cisc179.canvas import *
56/3: status = CourseStatus('CISC 179')
56/4: status.get_roster()
57/1: from cisc179.canvas import *
57/2: status = CourseStatus('CISC 179')
57/3: status.get_roster()
57/4: columns = list(status.df.columns)
57/5: columns[0] = 'user_id'
57/6: columns
57/7: df = status.df
57/8: df['name']
57/9: df['name'].label
57/10: df['name'].name
57/11: df['name'].name = 'full_name'
57/12: df
57/13: columns
57/14: df['name']
57/15: df['name'] == df['short_name']
57/16: all(df['name'] == df['short_name'])
57/17: columns
57/18: df['root_account']
57/19: df.drop(columns='root_account short_name'.split())
57/20: df.set_index('canvas_id')
57/21: df.set_index('id')
57/22: df.set_index('id', in_place=True)
57/23: df.set_index('id', inplace=True)
58/1: from cisc179.canvas import *
58/2: status = CourseStatus('CISC 179')
58/3: status.df.iloc[0]
58/4: status.get_roster()
58/5: df = _
58/6: df.iloc[0]
58/7: df.max_page_views
59/1: import mesa_python as mp
59/2: from mesa_python.canvas import CourseStatus
59/3: cs = CourseStatus()
59/4: dir(cs.api)
59/5: cs.course_api.create_page??
59/6: cs.course_api.create_page?
59/7: cs.course_api.get_page
59/8: cs.course_api.get_page?
59/9:
from canvasapi import Canvas
from .constants import CANVAS_API_URL, CANVAS_API_KEY
import logging
import pandas as pd
from pathlib import Path
from .schema import Page
import re
59/10:
from canvasapi import Canvas
from mesa_python.constants import CANVAS_API_URL, CANVAS_API_KEY
import logging
import pandas as pd
from pathlib import Path
from mesa_python.schema import Page
import re
59/11:
    page = Page(
        title="Hello World!",
        body='# Hello\nHow are you?',
    )
59/12: page.json()
59/13: page.json
59/14: page.json()
59/15: cs
59/16: cs.course_api.create_page?
59/17: cs.course_api.create_page(page)
59/18: cs.course_api.create_page(page.dict())
59/19: from markdown import markdown
59/20: markdown('#hi')
59/21: from markdown import *
59/22: who
59/23: import markdown
59/24: markdown.preprocessors('#Hi\nHello world!')
59/25: p, b = markdown.preprocessors.Preprocessor(), '#Hi\nHello world!'
59/26: p
59/27: b
59/28: p.md(b)
59/29: p, b = markdown.preprocessors.Preprocessor(md=True), '#Hi\nHello world!'
59/30: p.run(b)
59/31: p.md
59/32: p, b = markdown.preprocessors.Preprocessor(), '#Hi\nHello world!'
59/33: p.run(b)
59/34: p.md
59/35: dir(p)
59/36: p?
59/37: p.run(b.splitlines())
59/38: p.md
59/39: p.run(*b.splitlines())
59/40: markdown.markdown??
59/41: markdown.Markdown(b)
59/42: markdown.Markdown()
59/43: m = _
59/44: dir(m)
59/45: m.convert(b)
59/46: m.block_level_elements
59/47: m.references
59/48: m.set_output_format?
59/49: m.output_formats
59/50: m.style
59/51: m.treeprocessors
59/52: m.treeprocessors[0]
59/53: dir(m)
59/54: m.lines
59/55: m.lines[0]
59/56: m.is_block_level
59/57: m.is_block_level()
59/58: m.is_block_level(m.doc_tag)
59/59: m.doc_tag
59/60: m.convert?
59/61: m.convert??
60/1: from mesa_python.process_runestone_autogrades import *
60/2: from mesa_python.runestone import *
61/1: from mesa_python.runestone import *
61/2: who
61/3: runestone_autograde_df()
62/1: from mesa_python.runestone import *
62/2: df = runestone_autograde_df()
62/3: df = autograde_df()
62/4: df
62/5: df.full_name
62/6: [n for n in df.full_name if 'orm' in n.lower()]
62/7: [n for n in df.username if 'orm' in n.lower()]
62/8: who
62/9: cs
63/1: from mesa_python.canvas import *
64/1: from mesa_python.canvas import *
64/2: who
64/3: c = Course
64/4: c.create_page?
64/5: from mesa_python.constants imoprt *
64/6: from mesa_python.constants import *
64/7: who
64/8: DATA_DIR.parent.parent / 'mesa_python.gitlab.io'
64/9: p = _
64/10: p.is_dir()
64/11: p /= Path('src') / 'content' / 'blog' / 'runestone-walkthrough-assignment-1.md'
64/12: p
64/13: p.is_file()
64/14: p.parent.is_dir()
64/15: p = p.parent / 'runestone-walkthrough-first-assignment.md'
64/16: p
64/17: p.is_file()
64/18: c.create_page(filepath=p)
64/19: c
64/20: Page(p)
64/21: Page??
64/22: Page(filepath=p)
65/1: from mesa_python.constants import *
65/2: from mesa_python.canvas import *
65/3: p = DATA_DIR.parent.parent / 'mesa_python.gitlab.io' / Path('src') / 'content' / 'blog' / 'runestone-walkthrough-first-assignment.md'
65/4: p.is_file()
65/5: Page(filepath=p)
65/6: page = _
65/7: page.json()
66/1: from mesa_python.canvas import *
66/2: p = DATA_DIR.parent.parent / 'mesa_python.gitlab.io' / Path('src') / 'content' / 'blog' / 'runestone-walkthrough-first-assignment.md'
66/3: from mesa_python.constants import *
66/4: p = DATA_DIR.parent.parent / 'mesa_python.gitlab.io' / Path('src') / 'content' / 'blog' / 'runestone-walkthrough-first-assignment.md'
66/5: page = Page(filepath=p)
66/6: c = Course()
66/7: c.create_page(filepath=p)
66/8: page.title
66/9: page
66/10: page.p
66/11: page.p.title
67/1: from mesa_python.constants import *
67/2: from mesa_python.canvas import *
67/3: p = DATA_DIR.parent.parent / 'mesa_python.gitlab.io' / Path('src') / 'content' / 'blog' / 'runestone-walkthrough-first-assignment.md'
67/4: c = Course()
67/5: c.create_page(filepath=p)
68/1: from mesa_python.canvas import *
68/2: from mesa_python.constants import *
68/3: p = DATA_DIR.parent.parent / 'mesa_python.gitlab.io' / Path('src') / 'content' / 'blog' / 'runestone-walkthrough-first-assignment.md'
68/4: c = Course()
68/5: c.create_page(filepath=p)
68/6: p.suffix
68/7: p.with_suffix('')
68/8: exot
69/1: from mesa_python.constants import *
69/2: from mesa_python.canvas import *
69/3: p = DATA_DIR.parent.parent / 'mesa_python.gitlab.io' / Path('src') / 'content' / 'blog' / 'runestone-walkthrough-first-assignment.md'
69/4: c = Course()
69/5: c.create_page(filepath=p)
70/1: from mesa_python.constants import *
70/2: from mesa_python.canvas import *
70/3: p = DATA_DIR.parent.parent / 'mesa_python.gitlab.io' / Path('src') / 'content' / 'blog' / 'runestone-walkthrough-first-assignment.md'
70/4: c = Course()
70/5: c.create_page(filepath=p)
70/6: %run src/mesa_python/canvas.py
71/1: from mesa_python.constants import *
71/2: from mesa_python.canvas import *
71/3: p = DATA_DIR.parent.parent / 'mesa_python.gitlab.io' / Path('src') / 'content' / 'blog' / 'runestone-walkthrough-first-assignment.md'
71/4: c = Course()
71/5: c.create_page(filepath=p)
72/1: from mesa_python.constants import *
72/2: from mesa_python.canvas import *
72/3: p = DATA_DIR.parent.parent / 'mesa_python.gitlab.io' / Path('src') / 'content' / 'blog' / 'runestone-walkthrough-first-assignment.md'
72/4: c = Course()
72/5: c.create_page(filepath=p)
73/1: from mesa_python.constants import *
73/2: from mesa_python.canvas import *
73/3: p = DATA_DIR.parent.parent / 'mesa_python.gitlab.io' / Path('src') / 'content' / 'blog' / 'runestone-walkthrough-first-assignment.md'
73/4: c = Course()
73/5: c.create_page(filepath=p)
74/1: from mesa_python.constants import *
74/2: from mesa_python.canvas import *
74/3: p = DATA_DIR.parent.parent / 'mesa_python.gitlab.io' / Path('src') / 'content' / 'blog' / 'runestone-walkthrough-first-assignment.md'
74/4: c = Course()
74/5: c.create_page(filepath=p)
74/6: p
74/7: p = p.parent / 'runestone-assignment-chapter-1.md'
74/8: p.is_file()
74/9: p.is_file()
74/10: c.create_page(filepath=p)
74/11: p = p.parent / 'runestone-assignment-chapter-2-3.md'
74/12: p.is_file()
74/13: c.create_page(filepath=p)
75/1: 'abc'.split()
75/2: 'a b c'.split()
75/3: del 'a b c'.split()[1]
75/4: x = 'a b c'.split()
75/5: del x[1]
75/6: x
75/7: import jsonlines
75/8: eixt
76/1: import jsonlines as jl
76/2: jl.open('test.jsl')
76/3: f = jl.open('test.jsl', 'r+')
76/4: f = jl.open('test.jsl', 'a')
76/5: f.write({'z': 1, 'b': 2})
76/6: f.close
76/7: more test.jsl
76/8: ls -hal
76/9: jsonlines?
76/10: jl?
76/11: jl.jsonlines('test.jsl')
76/12: f = jl.open('test.jsl', 'a')
76/13: f.write({'z': 1, 'b': 2})
76/14: f.write({'z': 1, 'b': 2})
76/15: more test.jsl
76/16: f.close()
76/17: more test.jsl
76/18: who
76/19: from search_engine import *
76/20: from search_engine import *
76/21: db = VectorDB()
76/22: df = db.search('Explain to a new Python programmer what a list comprehension is.')
77/1: df = pd.read_csv('wikipedia-20220228-titles-all-in-ns0.csv.gz')
77/2: import pandas as pd
77/3: df = pd.read_csv('wikipedia-20220228-titles-all-in-ns0.csv.gz')
77/4: df.columns
77/5: df['0']
77/6: df = pd.read_csv('wikipedia-20220228-titles-all-in-ns0.csv.gz', header=0)
77/7: df.head()
77/8: import gzip
77/9: f = gzip.open('wikipedia-20220228-titles-all-in-ns0.csv.gz')
77/10: x = f.readline()
77/11: x
77/12: del df.iloc[0]
77/13: df.iloc[0]
77/14: df.iloc[0].drop()
77/15: df.drop(index=0, inplace=True)
77/16: df.head()
77/17: df.columns = ['title']
77/18: df = df.to_csv('wikipedia-20220228-titles-all-in-ns0.csv.gz', header=True, index=True)
77/19: df = df.to_csv('wikipedia-20220228-titles-all-in-ns0.csv.gz', header=False, index=False)
77/20: df = pd.read_csv('wikipedia-20220228-titles-all-in-ns0.csv.gz')
77/21: df.head()
77/22: df = pd.read_csv('wikipedia-20220228-titles-all-in-ns0.csv.gz', index_col=0)
77/23: df.to_csv('wikipedia-20220228-titles-all-in-ns0.csv.gz', header=False, index=False)
77/24: df.columns
77/25: df = pd.read_csv('wikipedia-20220228-titles-all-in-ns0.csv.gz')
77/26: df.columns
77/27: df = pd.read_csv('wikipedia-20220228-titles-all-in-ns0.csv.gz', header=False)
77/28: df = pd.read_csv('wikipedia-20220228-titles-all-in-ns0.csv.gz', header=None)
77/29: df['0'].str.lower().str.startswith('pan')
77/30: df[0].str.lower().str.startswith('pan')
77/31: ispan = _
77/32: df[ispan]
77/33: df[ispan.fillna(False)]
77/34: df[ispan.fillna(False)].values
77/35: df[ispan.fillna(False)].sample(100)
77/36: ispanc = df[0].str.lower().str.startswith('panc').fillna(False)
77/37: df[ispanc].sample(1000)
77/38: list(df[ispanc].sample(1000))
77/39: df[ispanc].sample(1000)
77/40: list(df[ispanc])
77/41: list(df[ispanc].values)
77/42: ispanc
77/43: df[ispanc].dtype
77/44: type(df[ispanc][0])
77/45: df.shape
77/46: df[0].shape
77/47: df[0].str[0].shape
77/48: df[0] = df[0].str[0]
77/49: df.head()
78/1: import pandas as pd
78/2: df = pd.read_csv('wikipedia-20220228-titles-all-in-ns0.csv.gz')
78/3: df
78/4: s = df['!']
78/5: s = pd.Series(['!'] + list(s))
78/6: s.name = 'title'
78/7: s.to_csv('wikipedia-20220228-titles-all-in-ns0.csv.gz')
78/8: s
78/9: s.name
78/10: df = pd.read_csv('wikipedia-20220228-titles-all-in-ns0.csv.gz')
78/11: df
78/12: df = pd.read_csv('wikipedia-20220228-titles-all-in-ns0.csv.gz', index_col=0)
78/13: ls -hal
78/14: s.to_csv('wikipedia-20220228-titles-all-in-ns0.csv.gz', index=None)
78/15: df = pd.read_csv('wikipedia-20220228-titles-all-in-ns0.csv.gz')
78/16: df
78/17: df.tail()
78/18: df.tail().to_list()
78/19: df.tail().to_dict()
78/20: '\u000ffffe'
78/21: '\U000ffffe'
78/22: s.tail()
78/23: isu = s.str.startswith('\U0')
78/24: isu = s.str.startswith('\U0000')
78/25: isu = s.str.startswith('\U0000ffff')
78/26: sum(isu)
78/27: sum(isu.fillna(False))
78/28: isu = s.str.contains('\U0000ffff')
78/29: sum(isu.fillna(False))
78/30: isu = s.str.contains('\U0000ffffe')
78/31: isu
78/32: isu = s.iloc[-1]
78/33: isu
78/34: u = list(s.iloc[-100:].values)
78/35: u
78/36: s2 = u
78/37: s2
78/38: s2 = pd.Series(u)
78/39: s2.isnull().sum()
78/40: s2.astype(bool).sum()
78/41: s = s.fillna('')
78/42: s = s.replace({o: '' for o in range(ord('\U000efffe'),ord('\U0010ffff')+1)})
78/43: s = s.replace({chr(o): '' for o in range(ord('\U000efffe'),ord('\U0010ffff')+1)})
78/44: s
78/45: s.tail()
78/46: s.tail(10)
78/47: s[-5:]
78/48: s[-6:]
78/49: s[-7:]
78/50: s[:-5]
78/51: s[:-6]
78/52: s = s[:-6].copy()
78/53: s.replace('', float('nan'))
78/54: s.replace('', None)
78/55: s = s.replace('', None)
78/56: s.dropna()
78/57: s.replace(None, float('nan'))
78/58: s.isna().sum()
78/59: s.dropna()
78/60: s = s.dropna()
78/61: s.isna().sum()
78/62: s[-5:]
78/63: s[-5:].values
78/64: s.replace('', float('nan'))
78/65: s = s.replace('', float('nan'))
78/66: s = s.dropna()
78/67: s.isna().sum()
78/68: s
78/69: (s == '').sum()
78/70: (s.astype(bool)).sum()
78/71: (~s.astype(bool)).sum()
78/72: s.tail(100)
78/73: s.tail(1000)
78/74: pd.options.display.max_rows = 1000
78/75: s.tail(1000)
78/76: s[:16424410]
78/77: s[:16424411]
78/78: s[:16424300]
78/79: s[:16424400]
78/80: s[:16424410]
78/81: s[:16424405]
78/82: s[:16424406]
78/83: s[:16424405]
78/84: s = s[:16424405].copy()
78/85: s
78/86: s.reset_index()
78/87: s.reset_index(drop=True)
78/88: s.reset_index(drop=True, inplace=True)
78/89: s.to_csv('wikipedia-20220228-titles-all-in-ns0.csv.gz', index=None)
78/90: ls -hal
78/91: s.to_hdf('wikipedia-20220228-titles-all-in-ns0.hdf')
78/92: s.to_hdf('titles', 'wikipedia-20220228-titles-all-in-ns0.hdf')
78/93: pip install pytables
78/94: s.to_hdf('titles', 'wikipedia-20220228-titles-all-in-ns0.hdf')
80/1: df = pd.read_csv('wikipedia-20220228-titles-all-in-ns0.csv.gz', index=None)
80/2: import pandas as pd
80/3: df = pd.read_csv('wikipedia-20220228-titles-all-in-ns0.csv.gz', index=None)
80/4: df = pd.read_csv('wikipedia-20220228-titles-all-in-ns0.csv.gz')
80/5: s = pd.read_csv(df.columns[0])
80/6: s = df[df.columns[0]]
80/7: s
80/8: s.to_hdf?
80/9: s.to_hdf?
80/10: s.to_hdf('wikipedia-20220228-titles-all-in-ns0.hdf', key='wikipedia_titles')
80/11: hdf = pd.read_hdf('wikipedia-20220228-titles-all-in-ns0.hdf')
80/12: hdf.memory()
80/13: hdf.usage()
80/14: hdf
80/15: ls -hal
80/16: rm *.hdf
80/17: rm *.hdf5
80/18: ls -hal
80/19: ls -hals
80/20: ls -halS
81/1: df = pd.read_csv('wikipedia-20220228-titles-all-in-ns0.csv.gz')
81/2: import pandas as pd
81/3: import vaex as vx
81/4: import parquet
81/5: df = pd.read_csv('wikipedia-20220228-titles-all-in-ns0.csv.gz')
81/6: s = df[df.columns[0]]
81/7: s.to_parquet('wikipedia-20220228-titles-all-in-ns0.parquet')
81/8: df.to_parquet('wikipedia-20220228-titles-all-in-ns0.parquet')
81/9: ls -halS
82/1: import pandas as pd
82/2: import vaex as vx
82/3: pd.read_parquet('wikipedia-20220228-titles-all-in-ns0.parquet')
82/4: df = _
82/5: memory()
82/6: mem
82/7: %memoryview
82/8: %memory
82/9: %usage
82/10: df.memory_usage()
82/11: df.memory_usage() / 1e6
83/1: from mesa_python import runestone as rs
83/2: rs.autograde_df
83/3: rs.autograde_df?
83/4: import vaex as vx
83/5:
from requests import session

payload = {
    'action': 'login',
    'username': lane24,
    'password': 'gratefulforstudents',
}

with session() as c:
    response = c.post('http://runestone.academy/', data=payload)
    # response = c.get('http://example.com/protected_page.php')
    print(response.headers)
    print(response.text)
83/6:
from requests import session

payload = {
    'action': 'login',
    'username': 'lane24',
    'password': 'gratefulforstudents',
}

with session() as c:
    response = c.post('http://runestone.academy/', data=payload)
    # response = c.get('http://example.com/protected_page.php')
    print(response.headers)
    print(response.text)
83/7:
# process_addcodes.py
import bs4 as bs
from pathlib import Path
import pandas as pd
import datetime

try:
    DATA_DIR = Path(__file__).parent.parent
except Exception:
    DATA_DIR = Path('./data/private')

TODAY = datetime.date.today()
83/8: TODAY.isoformat()
83/9: TODAY.date.isoformat()
83/10: TODAY.date().isoformat()
83/11: TODAY.date
83/12:
# process_addcodes.py
import bs4 as bs
from pathlib import Path
import pandas as pd
import datetime

try:
    DATA_DIR = Path(__file__).parent.parent
except Exception:
    DATA_DIR = Path('./data/private')

TODAY = datetime.datetime.now()
83/13: TODAY.date
83/14: TODAY.date()
84/1: %run src/mesa_python/runestone
84/2: from mesa_python import runestone as rs
84/3: float('')
85/1: from mesa_python import runestone as rs
85/2: rs.autograde_df()
85/3: ls data/private/grades/runestone_autograde_*.txt
85/4: ls /home/hobs/code/hobs/mesa/2024/cisc-179-private/data/private/grades/runestone_autograde_wk01-2024-02-08.txt
85/5: ls /home/hobs/code/hobs/mesa/2024/cisc-179-private/data/private/grades/
85/6: rs.autograde_df()
85/7: rs.autograde_df()
86/1: from mesa_python import runestone as rs
86/2: rs.autograde_df()
86/3: from mesa_python import runestone as rs
87/1: from mesa_python import runestone as rs
87/2: rs.autograde_df()
87/3: more /home/hobs/code/hobs/mesa/2024/cisc-179-private/data/private/grades/runestone_autograde-wk01-2024-02-08.txt
87/4: ls -hal /home/hobs/code/hobs/mesa/2024/cisc-179-private/data/private/grades/
87/5: ls -hal /home/hobs/code/hobs/mesa/2024/cisc-179-private/data/private/grades/runestone_autograde_wk01-2024-02-08.txt
87/6: ls -hal /home/hobs/code/hobs/mesa/2024/cisc-179-private/data/private/grades/runestone_autograde_wk01-2024-02-08.txt
87/7: more /home/hobs/code/hobs/mesa/2024/cisc-179-private/data/private/grades/runestone_autograde_wk01-2024-02-08.txt
88/1: from mesa_python import runestone as rs
88/2: rs.autograde_df()
88/3: df = _
88/4: !find data/private/ -iname '*roster*'
88/5: from mesa_python import canvas
88/6: course = canvas.Course()
88/7: course.export_runestone_roster()
88/8: dfcanvas = course.get_roster()
89/1: from mesa_python import canvas
89/2: course = canvas.Course()
89/3: dfcanvas = course.get_roster()
90/1: from mesa_python import canvas
90/2: course = canvas.Course()
90/3: dfcanvas = course.get_roster()
91/1: from mesa_python import canvas
91/2: course = canvas.Course()
91/3: dfcanvas = course.get_roster()
91/4: dfcanvas
91/5: dfrune = course.export_runestone_roster()
92/1: from mesa_python import canvas
92/2: course = canvas.Course()
92/3: dfcanvas = course.get_roster()
92/4: dfrune = course.export_runestone_roster()
93/1: from mesa_python import canvas
93/2: course = canvas.Course()
93/3: dfcanvas = course.get_roster()
93/4: dfrune = course.export_runestone_roster()
93/5: course.df.columns
93/6: !find data/private/ -iname '*email*'
94/1: from mesa_python import runestone as rs
96/1: from mesa_python import runestone as rs
96/2: rs.autograde_df(today='2024-02-08')
96/3: df = _
96/4: df.sort_values('username')
96/5: df = _
96/6: ls $DATA_DIR
97/1: rs.autograde_df(today='2024-02-10')
97/2: from mesa_python import runestone as rs
97/3: rs.autograde_df(today='2024-02-10')
97/4: df1 = rs.autograde_df(today='2024-02-10')
97/5: from mesa_python import canvas
97/6: dfcanvas = course.get_roster()
97/7: course = canvas.Course()
97/8: course = canvas.Course()
97/9: dfcanvas = course.get_roster()
97/10: dfcanvas.index
97/11: dfcanvas
97/12: dfcanvas.iloc[5]
97/13: dfold = pd.read_csv('data/private/2023-01-30-all-myportal-contact-list-personal-email-student-sdccd-email.csv', index_col=0)
97/14: import pandas as pd
97/15: dfold = pd.read_csv('data/private/2023-01-30-all-myportal-contact-list-personal-email-student-sdccd-email.csv', index_col=0)
98/1: import pandas as pd
99/1: dfold = pd.read_csv('data/private/2023-01-30-all-myportal-contact-list-personal-email-student-sdccd-email.csv', index_col=0)
99/2: import pandas as pd
99/3: dfold = pd.read_csv('data/private/2023-01-30-all-myportal-contact-list-personal-email-student-sdccd-email.csv', index_col=0)
99/4: dfold.iloc[0]
100/1: import pandas as pd
100/2: from mesa_python import canvas
101/1: import pandas as pd
101/2: from mesa_python import canvas
101/3: course = canvas.Course()
101/4: df = course.get_roster()
101/5: df
101/6: df.iloc[0]
101/7: course.index.values()
101/8: course.index.values
101/9: course.df.index.values
101/10: len(course.df.index)
101/11: course.df.index = [i.lower() for i in course.df.index]
101/12: course.df
101/13: course.df.iloc[0]
101/14: course.df.iloc[10]
101/15: dir(course)
101/16: course.join_emails()
101/17: course.private_dir
101/18: course.private_dir.is_dir()
101/19: list(course.private_dir.glob('**/*.csv'))
101/20: list(course.private_dir.glob('**/*grad*.csv'))
101/21: sorted(course.private_dir.glob('**/*grad*.csv'))
101/22: course.join_emails?
101/23: sorted(course.private_dir.glob('**/*mail*.csv'))
101/24: sorted((self.private_dir).glob(csvglob))
101/25: sorted((course.private_dir).glob(csvglob))
102/1: from mesa_python import canvas
102/2: course = canvas.Course()
102/3: df = course.get_roster()
102/4: sorted((course.private_dir).glob(csvglob))
102/5: csvglob='**/*grad*.csv')
102/6: csvglob='**/*grad*.csv'
102/7: sorted((course.private_dir).glob(csvglob))
102/8: sorted?
102/9: x = sorted((course.private_dir).glob(csvglob))
102/10: len(x)
102/11:
    def join_emails(self, csvglob='**/*mail*.csv'):
        # FIXME: need to load e-mails from other peoplesoft table
        # df['email'] = self.df['email']

        filepath = Path(csvglob)
        if not Path(csvglob).is_file():
            filepath = Path(sorted(self.private_dir.glob(csvglob))[-1])
        self.emails = pd.read_csv(filepath, index_col=0)
        self.emails['last_name'] = self.emails['full_name'].str.split().str[1:].str.join(' ')
        self.emails['username'] = self.emails['last_name'].apply(lambda x: str(x).lower().replace(' ', '_') + self.username_suffix)
        self.emails.setindex('username')
        return self.emails
102/12: join_emails(self=course)
102/13: from mesa_python.canvas import *
102/14: join_emails(self=course)
102/15: course.get_roster()
102/16: df = _
102/17: df.head().T
102/18: join_emails(self=course)
103/1: from mesa_python.canvas import *
103/2: from mesa_python.canvas import *
103/3: from mesa_python.canvas import *
103/4: from mesa_python.constants import *
103/5: from mesa_python.canvas import *
104/1: from mesa_python.canvas import *
105/1: from mesa_python.canvas import *
105/2: course = canvas.Course()
105/3: course = Course()
105/4: df = course.get_roster()
105/5: course.df
105/6: course.df.head().T
105/7: course.df.emails
106/1: from mesa_python.canvas import *
106/2: course = Course()
106/3: df = course.get_roster()
106/4: df.head().T
106/5: course.emails
106/6: course.df
106/7: csvglob='**/*mail*.csv'
106/8: filepath = Path(sorted(course.private_dir.glob(csvglob))[-1])
106/9: filepath
106/10: csvglob='**/*mail*.csv'
106/11: filepath = sorted(course.private_dir.glob(csvglob))
106/12: filepath
106/13: filter(filepath, lambda x: 'wait' in x.lower())
106/14: filter(lambda x: 'wait' in x.lower(), filepath)
106/15: sorted(filter(lambda x: 'wait' in x.lower(), filepath))
106/16: sorted(filter(lambda x: 'wait' in str(x).lower(), filepath))
106/17: sorted(filter(lambda x: 'wait' not in str(x).lower(), filepath))
106/18: ls -hal $PRIVATE_DIR
106/19: ls -hal $(PRIVATE_DIR.glob('*email*.csv'))
106/20: ls -hal $PRIVATE_DIR | grep email
107/1: from mesa_python.canvas import *
107/2: course = Course()
107/3: course.get_roster()
107/4: course.df
107/5: course.df.head().T
107/6: course.df.head()
107/7: course.df
108/1: from mesa_python.canvas import *
108/2: course = Course()
108/3: course.get_roster()
108/4: course.df.head()
108/5: course.df.head().T
109/1: from mesa_python.canvas import *
109/2: course = Course()
109/3: course.get_roster()
110/1: from mesa_python.canvas import *
110/2: course = Course()
110/3: course.get_roster()
111/1: from mesa_python.canvas import *
111/2: course = Course()
111/3: course.get_roster()
112/1: from mesa_python.canvas import *
112/2: course = Course()
112/3: course.get_roster()
112/4: df = _
112/5: em = course.emails
112/6: em
112/7: em.columns
112/8: df.head().T
113/1: from mesa_python.canvas import *
113/2: course = Course()
113/3: course.get_roster()
114/1: %run llm.py
114/2: from llm import *
114/3: who
114/4: rag = RAG()
114/5: ls data
114/6: ls data/corpus/
114/7: ls
115/1: from llm import *
115/2: rag = RAG()
116/1: from llm import *
117/1: from llm import *
117/2: rag = RAG()
117/3: rag.db.search('what is a generator?')
118/1: from llm import *
118/2: rag = RAG()
118/3: rag.db.search('what is a generator?')
118/4: rag.db.search('what is a generator?', limit=10)
118/5: rag.db.prettify_search_results(_)
118/6: rag.db.prettify_search_results?
118/7: rag.db.prettify_search_results()
120/1: from llm import *
120/2: rag.db.prettify_search_results()
120/3: rag = RAG()
120/4: rag.db.search('what is a generator?')
120/5: resultsdf = _
120/6: rag.db.prettify_search_results(resultsdf)
120/7: prettify_search_results(resultsdf)
120/8: from search_engine import *
120/9: prettify_search_results(resultsdf)
120/10: rag.db.search('what is a generator and what is an iterator?')
120/11: resultsdf = _
120/12: prettify_search_results(resultsdf)
120/13: rag.db.search('what is a generator and what is an iterator?', limit=3)
120/14: prettify_search_results(_)
120/15: print('\n'.join(prettify_search_results(_)))
120/16: print('\n'.join(prettify_search_results(__)))
121/1: from search_engine import *
121/2: from llm import *
121/3: rag = RAG()
121/4: rag.db.search('Who is the President of the United States?', limit=3)
121/5: results = _
121/6: prettify_search_results(results)
121/7: pretty_print(results)
121/8: !grep -R "John Rance" data/corpus/
121/9: rag.hist
121/10: rag.context
122/1: import pdb
122/2: pdb?
122/3: pdb.set_trace()
123/1: from llm import *
123/2: rag = RAG()
123/3: rag.db.search('Who is the President of the United States?', limit=3)
123/4: rag.ask('Who is the president?')
124/1: from llm import *
124/2: rag = RAG()
124/3: rag.ask('Who is the president?')
124/4: rag.ask('What is a generator?')
124/5: rag.ask('What is an iterator?')
125/1: from llm import *
125/2: rag = RAG()
125/3: rag.ask('What is an iterator?')
125/4: print(_)
125/5: rag.prompt
125/6: rag.prompt_template
125/7: rag.ask('What is a generator?')
125/8: print(_)
125/9: rag.hist
125/10: rag.hist[-1]
125/11: len(rag.hist)
125/12: rag.hist[0]
125/13: rag.hist[0]['answer']
125/14: rag.hist[0]['question']
125/15: rag.hist[1]['question']
125/16: rag.hist[1]['answer']
125/17: rag.prompt
125/18: print(rag.prompt)
126/1: from llm import *
126/2: rag = RAG()
126/3: rag.ask('What is a generator?')
126/4: rag.context
126/5: print(rag.context)
127/1: from llm import *
127/2: rag = RAG()
127/3: rag.ask('What is a generator?')
127/4: print(rag.ask('What is a generator?'))
127/5: print(rag.ask('What is a generator?'))
128/1: from llm import *
128/2: rag = RAG()
128/3: rag.ask('What are some special operators in python?')
128/4: rag.ask('Why are dunder methods used in python?')
128/5: rag.ask('What are some unique aspects of python?')
129/1: from mesa_python import runestone as rs
129/2: from mesa_python import canvas
129/3: course = canvas.Course()
129/4: course.get_roster()
129/5: df = _
129/6: df.head().T
129/7: df.head().T
129/8: df['last_activity_at total_activity_time page_views page_views_level'.split()]
129/9:
>>> x = [1, 2, 3]
>>> y = [4, 6, 8]
>>> z = x.extend(y)
>>> print(z)
129/10:
>>> x = [1, 2, 3]
>>> y = [4, 6, 8]
>>> x.extend(y)
>>> print(x)
129/11:
>>> x = [1, 2, 3]
>>> y = [4, 6, 8]
>>> for number in y:
...     x = x.append(number)
>>> print(x)
129/12:
>>> x = [1, 2, 3]
>>> y = [4, 6, 8]
>>> for number in y:
...     x.append(number)
>>> print(x)
130/1: ls -hal
130/2: import pandas as pd
130/3: pd.read_csv('data_for_hobsonlane.csv')
130/4: df = _
130/5: df.columns
130/6: df = df.set_index(0)
130/7: df = df.set_index(df.columns[0])
130/8: df
130/9: df = df.set_index(df.columns[0])
130/10: pd.read_csv('data_for_hobsonlane.csv', index_col='id')
130/11: df
130/12: df['act'].unique()
130/13: do = df['act'] == 'doAssignment'
130/14: df[do]
130/15: df['studentreport']
130/16: do = df['act'] == 'student_report'
130/17: do = df['act'] == 'doAssignment'
130/18: do = df['act'] == 'student_report'
130/19: df[do]
130/20: do = df['act'] == 'studentreport'
130/21: df[do]
130/22: watch -h
131/1: import vaex
132/1: import vaex
132/2: # vaex.from_csv('/home/hobs/.nlpia2-data/')
132/3: ls /home/hobs/.nlpia2-data/
132/4: ls /home/hobs/.nlpia2-data/wikipedia*csv*
132/5: import pandas as pd
132/6: pd.read_csv('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv')
132/7: df = vaex.from_csv('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv')
132/8: from search_engine import *
132/9: mkdir src
132/10: mkdir src/knowt
132/11: who
132/12: model
132/13: model(['hello world', 'Hello world'])
132/14: dir(model)
132/15: model.encode(['hello world', 'Hello world'])
132/16: _.diff()
132/17: np.linalg.norm(_[0] - _[1])
132/18: 100000*__.shape[1]
132/19: from tqdm import tqdm
132/20: a = ___
132/21: a
132/22: a.shape
132/23: np.concatenate?
132/24: a = []
132/25: df.concat(df)
132/26: df.concat(df,axis=1)
132/27: df.concat?
132/28: vaex.DataFrame
132/29: a
132/30:
for i in tqdm(range(0, len(df)+2, 1000)):
    print(df.iloc[1000*i:1000*i+1000].iloc[0])
132/31:
for i in tqdm(range(0, len(df)+2, 1000)):
    print(df.loc[1000*i:1000*i+1000].loc[0])
132/32: df
132/33: df[0]
132/34:
for i in tqdm(range(0, len(df)+2, 1000)):
    print(df[1000*i:1000*i+1000][0])
132/35:
for i in tqdm(range(0, len(df)+2, 1000)):
    print(df[1000*i:1000*i+1000])
132/36:
for i in tqdm(range(0, len(df)+2, 1000)):
    print(df[1000*i:(1000*i+1000)][0])
132/37:
for i in tqdm(range(0, len(df)+2, 1000)):
    print(df[1000*i:(1000*i+1000)])
132/38:
for i in tqdm(range(0, len(df)+2, 1000)):
    print(i,1000*i, 1000*i+1000)
    print(df[1000*i:(1000*i+1000)])
132/39:
for i in tqdm(range(0, len(df)+2, 1000)):
    print(i,i+1000)
    print(df[i:(i+1000)][0])
132/40:
for i in tqdm(range(0, len(df), 1000)):
    print(i,i+1000)
    print(df[i:(i+1000)][0])
132/41:
vecs = vaex.DataFrame()
for i in tqdm(range(0, len(df), 1000)):
    print(i, df[i:(i+1000)][0][0])
132/42: df.columns = ['title']
132/43: df
132/44: df['title'][0]
132/45: df['title']
132/46: df.str[0]
132/47: df.apply(lambda x: x[0])
132/48: df.apply(lambda x: x[0], arguments=list)
132/49: df.apply?
132/50: df.apply(lambda x: x[0], arguments=df.title)
132/51: df.title
132/52: df
132/53: df.columns = ['title']
132/54: df
132/55: df.columns
132/56: df = vaex.from_csv('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv', columns=['title'])
132/57: df = vaex.from_csv('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv')
132/58: df.columns
132/59: df.columns?
132/60: list(df.columns)
132/61: df['0']
132/62: df = vaex.from_csv?
132/63: df = vaex.from_csv('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv', convert='/home/hobs/.nlpia2-data/wikipedia-titles-100k.vaex.hdf5')
132/64: df.columns = ['title']
132/65: df['title']
132/66: df.columns = {'title': 0}
132/67: df['title']
132/68: df.columns = {'title': df['0']}
132/69: df['title']
132/70: df['0']
132/71: dir(vaex)
132/72: [x for x in dir(vaex) if x.startswith('from')]
132/73: df = vaex.from_dataset('/home/hobs/.nlpia2-data/wikipedia-titles-100k.vaex.hdf5')
132/74: df = vaex.from_dataset?
132/75: df
132/76: df.info
132/77: df.info()
132/78: df.describe()
132/79: vf = vaex.open('/home/hobs/.nlpia2-data/wikipedia-titles-100k.vaex.hdf5')
132/80: df.columns
132/81: df.rename?
132/82: df.rename('0', 'title')
132/83: df
132/84: df.rename('0', 'title')
133/1: pd.read_csv('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv')
133/2: import pandas as pd
134/1: df = vaex.from_csv('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv', convert='/home/hobs/.nlpia2-data/wikipedia-titles-100k.vaex.hdf5')
134/2: imoprt vaex
134/3: import vaex
134/4: df = vaex.from_csv('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv', convert='/home/hobs/.nlpia2-data/wikipedia-titles-100k.vaex.hdf5')
134/5: df
134/6: vdf = vaex.open('/home/hobs/.nlpia2-data/wikipedia-titles-100k.vaex.hdf5')
134/7: del df
134/8:
vecs = vaex.DataFrame()
for i in tqdm(range(0, len(vdf), 1000)):
    sentences = vdf[i:(i+1000)]
    batch = model.encode(sentences.values)
    print(batch.shape)
    vecs = vecs.concatenate(batch)
    print(vecs.shape)
134/9: from tqdm import tqdm
134/10: from search_engine import *
134/11:
vecs = vaex.DataFrame()
for i in tqdm(range(0, len(vdf), 1000)):
    sentences = vdf[i:(i+1000)]
    batch = model.encode(sentences.values)
    print(batch.shape)
    vecs = vecs.concatenate(batch)
    print(vecs.shape)
134/12: batch
134/13: sentences
134/14: sentences.values
134/15: sentences[0]
134/16: list(sentences)
134/17: type(sentences)
134/18: sentences
134/19:
for s in sentences:
    print(s)
134/20:
for i in range(10):
    print(sentences[i])
134/21:
for i in range(10):
    print(sentences[i][0])
134/22: sentences.index
134/23: dir(sentences)
134/24: sentences.to_arrays()
134/25: sentences.to_arrays()[0]
134/26:
vecs = vaex.DataFrame()
for i in tqdm(range(0, len(vdf), 1000)):
    sentences = vdf[i:(i+1000)].to_arrays()[0]
    batch = model.encode(sentences)
    print(batch.shape)
    vecs = vecs.concatenate(batch)
    print(vecs.shape)
134/27:
vecs = vaex.DataFrame()
for i in tqdm(range(0, len(vdf), 1000)):
    sentences = list(vdf[i:(i+1000)].to_arrays()[0])
    batch = model.encode(sentences)
    print(batch.shape)
    vecs = vecs.concatenate(batch)
    print(vecs.shape)
134/28:
vecs = vaex.DataFrame()
for i in tqdm(range(0, len(vdf), 1000)):
    sentences = vdf[i:(i+1000)].to_arrays()
    batch = model.encode(sentences)
    print(batch.shape)
    vecs = vecs.concatenate(batch)
    print(vecs.shape)
134/29: sentences
134/30:     sentences = vdf[i:(i+1000)].to_arrays()
134/31: sentences
134/32:     sentences = vdf[i:(i+1000)].to_arrays()[0]
134/33: i
134/34: sentences.shape
134/35: sentences.size
134/36:     sentences = vdf[i:(i+1000)].to_numpy()
134/37:     sentences = vdf[i:(i+1000)].to_numpy
134/38:     sentences = vdf[i:(i+1000)].to_pandas_series()
134/39:
vecs = vaex.DataFrame()
for i in tqdm(range(0, len(vdf), 1000)):
    sentences = vdf[i:(i+1000)].to_arrays()
    batch = model.encode(sentences)
    print(batch.shape)
    vecs = vecs.concatenate(batch)
    print(vecs.shape)
134/40: sentences['0']
134/41: sentences[0]
134/42: sentences[0][0]
134/43: sentences[0][0].value
134/44: sentences[0].values
134/45: sentences[0].to_numpy
134/46: sentences[0].to_numpy()
134/47: sentences[0].to_numpy(zero_copy_only=False)
134/48:
vecs = vaex.DataFrame()
for i in tqdm(range(0, len(vdf), 1000)):
    sentences = vdf[i:(i+1000)][0].to_numpy(zero_copy_only=False)

    batch = model.encode(sentences)
    print(batch.shape)
    vecs = vecs.concatenate(batch)
    print(vecs.shape)
134/49:
vecs = vaex.DataFrame()
for i in tqdm(range(0, len(vdf), 1000)):
    sentences = vdf[i:(i+1000)][0]
    batch = model.encode(sentences)
    print(batch.shape)
    vecs = vecs.concatenate(batch)
    print(vecs.shape)
134/50: sentences
134/51:
vecs = vaex.DataFrame()
for i in tqdm(range(0, len(vdf), 1000)):
    sentences = vdf[i:(i+1000)]
    batch = model.encode(sentences)
    print(batch.shape)
    vecs = vecs.concatenate(batch)
    print(vecs.shape)
134/52: sentences
134/53: sentences.to_arrays()[0]
134/54: sentences.to_arrays()[0].to_numpy(zero_copy_only=False)
134/55:
vecs = vaex.DataFrame()
for i in tqdm(range(0, len(vdf), 1000)):
    sentences = vdf[i:(i+1000)].to_arrays()[0].to_numpy(zero_copy_only=False)
    batch = model.encode(sentences)
    print(batch.shape)
    vecs = vecs.concatenate(batch)
    print(vecs.shape)
134/56: vaex.concat?
134/57:
vecs = vaex.DataFrame()
for i in tqdm(range(0, len(vdf), 1000)):
    sentences = vdf[i:(i+1000)].to_arrays()[0].to_numpy(zero_copy_only=False)
    batch = model.encode(sentences)
    print(batch.shape)
    vecs = vaex.concat(vecs, batch)
    print(vecs.shape)
134/58:
vecs = vaex.DataFrame()
for i in tqdm(range(0, len(vdf), 1000)):
    sentences = vdf[i:(i+1000)].to_arrays()[0].to_numpy(zero_copy_only=False)
    batch = model.encode(sentences)
    print(batch.shape)
    vecs = vaex.concat([vecs, batch])
    print(vecs.shape)
134/59: batch
134/60: vecs
134/61: vecs
135/1: from dask import dataframe as dd
135/2: ddf = dd.read_csv('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv')
135/3: ddf.shape
135/4: ddf.shape.compute()
135/5: ddf.shape[0].compute()
135/6: ddf.shape[1]
135/7: ddf.shape[0]
135/8: ddf.shape[1]
135/9: ddf.columns
135/10: ddf.to_parquet?
135/11: ddf = ddf.to_parquet('/home/hobs/.nlpia2-data/wikipedia-titles-100k.parquet')
135/12: ddf = dd.open('/home/hobs/.nlpia2-data/wikipedia-titles-100k.parquet')
135/13: ddf = dd.from_dask_array('/home/hobs/.nlpia2-data/wikipedia-titles-100k.parquet')
135/14: ddf = dd.read_parquet('/home/hobs/.nlpia2-data/wikipedia-titles-100k.parquet')
135/15: ddf.iloc[0]
135/16: ddf.loc[0]
135/17: ddf.loc[1]
135/18: ddf.loc[1].compute()
135/19: ddf.loc[0].compute()
135/20: ddf.head()
135/21: ddf[0:1000].compute()
135/22: ddf['title'][0:1000].compute()
135/23:
batches = [] 
for i in tqdm(range(0, len(vdf), 1000)):
    sentences = vdf[i:(i+1000)].to_arrays()[0].to_numpy(zero_copy_only=False)
    batch = model.encode(sentences)
    print(batch.shape)
    batches.append(vaex.DataFrame(np.array(batch)))
135/24: import vaex
135/25: from tqdm import tqdm
135/26:
batches = [] 
for i in tqdm(range(0, len(vdf), 1000)):
    sentences = vdf[i:(i+1000)].to_arrays()[0].to_numpy(zero_copy_only=False)
    batch = model.encode(sentences)
    print(batch.shape)
    batches.append(vaex.DataFrame(np.array(batch)))
135/27: df = pd.read_csv('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv')
135/28: import pandas as pd
135/29: from search_engine import *
135/30: df = pd.read_csv('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv')
135/31:
batches = [] 
for i in tqdm(range(0, len(vdf), 1000)):
    sentences = df['title'].values
    batch = model.encode(sentences)
    print(batch.shape)
    batches.append(vaex.DataFrame(np.array(batch)))
135/32:
batches = [] 
for i in tqdm(range(0, len(df), 1000)):
    sentences = df['title'].values
    batch = model.encode(sentences)
    print(batch.shape)
    batches.append(vaex.DataFrame(np.array(batch)))
135/33:
batches = [] 
df2 = df.sample(1000)
for i in tqdm(range(0, len(df2), 10)):
    sentences = df2['title'].values
    batch = model.encode(sentences)
    print(batch.shape)
    batches.append(vaex.DataFrame(np.array(batch)))
135/34: vx = vaex.concat(batches)
135/35: vaex.concat(*batches)
135/36: vaex.concat?
135/37: len(batches)
135/38: len(batches[0])
135/39: len(batches[0][0])
135/40: batches[0]
135/41: batches
135/42: type(batches)
135/43:
batches = [] 
df2 = df.sample(100)
for i in tqdm(range(0, len(df2), 10)):
    sentences = df2['title'].values
    batch = model.encode(sentences)
    print(batch.shape)
    batches.extend(list(batch))
135/44: batches[0]
135/45:
batches = [] 
df2 = df.sample(1000)
for i in tqdm(range(0, len(df2), 10)):
    sentences = df2['title'].values
    batch = model.encode(sentences)
    print(batch.shape)
    batches.extend(list(batch))
135/46: batches
135/47: len(batches)
135/48: np.memmap('batches.np.memmap', mode='w+')
135/49: np.memmap('batches.np.memmap', mode='w+', shape=(len(batches), len(batches[0])))
135/50: mm = _
135/51: mm = np.array(batches)
135/52: mm
135/53: ls -hal
135/54: ls -hal batches*
135/55: mm2 = np.memmap('batches.np.memmap', mode='w+', shape=(len(batches), len(batches[0])))
135/56: mm2
135/57: mm2 = np.memmap('batches.np.memmap', mode='w+', shape=(len(batches), len(batches[0])), order='R')
135/58: mm2 = np.memmap('batches.np.memmap', mode='w+', shape=(len(batches), len(batches[0])), order='C')  # C=C-style == row-major order!
135/59: del mm
135/60: mm2 = np.memmap('batches.np.memmap', mode='w+', shape=(len(batches), len(batches[0])), order='C')  # C=C-style == row-major order!
135/61: mm2[:] = batches[:]
135/62: del mm2
135/63: mm = np.memmap('batches.np.memmap', mode='w+', shape=(len(batches), len(batches[0])), order='C')  # C=C-style == row-major order, which is the default!
135/64: mm
135/65: mm[:] = batches[:]
135/66: mm.flush()
135/67: del mm
135/68: mm = np.memmap('batches.np.memmap', mode='w+', shape=(len(batches), len(batches[0])), offset=0, dtype=np.float32, order='C')  # C=C-style == row-major order, which is the default!
135/69: mm
135/70: mm = np.memmap('batches.np.memmap', mode='w+', shape=(len(batches), len(batches[0])), offset=0, dtype=np.float32, order='C')  # C=C-style == row-major order, which is the default!
135/71: mm[:] = batches[:]
135/72: mm.flush()
135/73: mm = np.memmap('batches.np.memmap', mode='r+', shape=(len(batches), len(batches[0])), offset=0, dtype=np.float32, order='C')  # C=C-style == row-major order, which is the default!
135/74: mm
135/75: del mm
135/76: mm = np.memmap('batches.np.memmap', mode='r+', shape=(len(batches), len(batches[0])), offset=0, dtype=np.float32, order='C')  # C=C-style == row-major order, which is the default!
135/77: mm
135/78: mm = np.memmap('batches.np.memmap', mode='w+', shape=(len(batches), len(batches[0])), offset=0, dtype=np.float32, order='C')  # C=C-style == row-major order, which is the default!
135/79: mm
135/80: mm.flush()
135/81: del mm
135/82: mm = np.memmap('batches.np.memmap', mode='r+', shape=(len(batches), len(batches[0])), offset=0, dtype=np.float32, order='C')  # C=C-style == row-major order, which is the default!
135/83: mm
135/84: mm = np.memmap('batches.np.memmap', mode='w+', shape=(len(batches), len(batches[0])), offset=0, dtype=np.float32, order='C')  # C=C-style == row-major order, which is the default!
135/85: mm[:] = batches[:]
135/86: mm.flush()
135/87: del mm
135/88: mm = np.memmap('batches.np.memmap', mode='r+', shape=(len(batches), len(batches[0])), offset=0, dtype=np.float32, order='C')  # C=C-style == row-major order, which is the default!
135/89: mm
135/90: ans = model.encode(['hello world']).dot(mm)
135/91: ans = model.encode(['hello world']).dot(mm.T)
135/92: ans.argmax()
135/93: df2
135/94: df2.iloc[118]
135/95: ind = np.argpartition(ans, -4)[-4:]
135/96: ind
135/97: df2.iloc[ind]
135/98: [df2.iloc[i] for i in ind]
135/99: [df2.loc[i] for i in ind]
135/100: ind = np.argpartition(ans, -4)[-4:]
135/101: ind.shape
135/102: ind = np.argpartition(ans, -4)
135/103: ind.shape
135/104: ind = np.argpartition(ans, -4)[-4:][:4]
135/105: ind
135/106: ans.shape
135/107: ans = ans.flatten()
135/108: ans.argmax()
135/109: ind = np.argpartition(ans, -4)[-4:]
135/110: ind
135/111: df2[ind]
135/112: df2.values[ind]
135/113: df2.loc[ind]
135/114: [df2.loc[i] for i in ind]
135/115: df2
135/116: len(ans)
135/117: argmax(ans)
135/118: ans.argmax()
135/119: ind = np.argpartition(ans, -4)[-4:]
135/120: ans
135/121: ind
135/122: [df2.iloc[i] for i in ind]
135/123: i = ind[0]
135/124: i
135/125: df2.iloc[31118]
135/126: df2.loc[31118]
135/127: df2.shape
135/128: len(df2)
135/129: mm.shape
135/130:
batches = [] 
for i in tqdm(range(0, len(df), 100)):
    sentences = df2['title'].values[i:i+100]
    batch = model.encode(sentences)
    print(batch.shape)
    batches.extend(list(batch))
135/131:
batches = [] 
for i in tqdm(range(0, len(df), 500)):
    sentences = df['title'].values[i:i+500]
    batch = model.encode(sentences)
    print(batch.shape)
    batches.extend(list(batch))
135/132: mm = np.memmap('batches.np.memmap', mode='r+', shape=(len(batches), len(batches[0])), offset=0, dtype=np.float32, order='C')  # C=C-style == row-major order, which is the default!
135/133: mm[:] = batches[:]
135/134: mm.shape
135/135: %free
135/136: %mem
135/137: %memfree
135/138: %usage
135/139: dir(mm)
135/140: mm.flush()
135/141: ans = model.encode(['turing complete'])[0].dot(mm.T)
135/142: ans.argmax()
135/143: df.iloc[ans.argmax()]
135/144: ind = np.argpartition(ans, -10)[-10:]
135/145: ind.shape
135/146: df.iloc[ind]
135/147: hist /140-150
135/148: hist ~0/140-150
135/149: hist ~0/120-150
135/150: more '/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv'
135/151: docs = np.loadtxt('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv')
135/152: docs = np.loadtxt('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv', dtype=str)
135/153: docs[0]
135/154: mm.name
135/155: mm.filename
135/156: import io
135/157: io.StringIO('hello\nworld\n')
135/158: f = _
135/159: f.readline()
135/160: f.readline()
135/161: f.name
135/162: f.fileno
135/163: f.fileno()
135/164: range(100)[0]
135/165: range(100)[5]
135/166: len(mm)
135/167: mm.shape
135/168: encoder=model.encode
135/169:
def find_similar(memmap, limit=1, queries=None, labels=None, encoder=model.encode):
    if isinstance(memmap, (str, Path)):
        memmap = np.memmap(memmap, mode='r')
    if isinstance(queries, str):
        queries = [queries]
    num_docs, num_dims = memmap.shape
    if labels is None:
        labels = np.arange(num_docs)
    similarity = encoder(queries)
    similarity /= np.linalg.norm(similarity, axis=1)
    similarity = similarity.dot(memmap.T)
    ind = np.array([
        np.argpartition(s, -limit)[-limit:]
        for s in similarity
    ])
    return labels[ind]
    ]
135/170:
def find_similar(memmap, limit=1, queries=None, labels=None, encoder=model.encode):
    if isinstance(memmap, (str, Path)):
        memmap = np.memmap(memmap, mode='r')
    if isinstance(queries, str):
        queries = [queries]
    num_docs, num_dims = memmap.shape
    if labels is None:
        labels = np.arange(num_docs)
    similarity = encoder(queries)
    similarity /= np.linalg.norm(similarity, axis=1)
    similarity = similarity.dot(memmap.T)
    ind = np.array([
        np.argpartition(s, -limit)[-limit:]
        for s in similarity
    ])
    return labels[ind]
135/171: find_similar(mm, queries='Allan Turing')
135/172: find_similar(mm, queries='Allan Turing', labels=df['title'].values)
135/173: find_similar(mm, queries='Turing', labels=df['title'].values)
135/174: [t for t in df['title'].values if t.lower().strip().endswith('turing')]
135/175: [t for t in df['title'].values if t.lower().strip().startswith('turing')]
135/176: [t for t in df['title'].values if t.lower().strip().startswith('putin')]
135/177: [t for t in df['title'].values if t.lower().strip().endswith('putin')]
135/178: [t for t in df['title'].values if t.lower().strip().endswith('_putin')]
135/179: find_similar(mm, queries='Putin', labels=df['title'].values)
135/180: find_similar(mm, queries="Putin's palace", labels=df['title'].values)
135/181: find_similar(mm, queries="Putin graft", labels=df['title'].values)
135/182: find_similar(mm, queries="Putin corruption", labels=df['title'].values)
135/183: find_similar(mm, queries="Putin on the take", labels=df['title'].values)
135/184: find_similar(mm, queries="Putin on the take", labels=df['title'].values, limit=5)
135/185: find_similar(mm, queries="Putin example $ corruption", labels=df['title'].values, limit=5)
135/186: find_similar(mm, queries=["Putin example $ corruption", "most corrupt people in history"], labels=df['title'].values, limit=5)
135/187: similarity = encoder(queries)
135/188: queries=["Putin example $ corruption", "most corrupt people in history"]
135/189: queries
135/190: similarity = encoder(queries)
135/191: np.linalg.norm(similarity)
135/192: np.linalg.norm(similarity, axis=-)
135/193: np.linalg.norm(similarity, axis=0)
135/194: np.linalg.norm(similarity, axis=1)
135/195: similarity.shape
135/196: np.linalg.norm(similarity, axis=1).shape
135/197: similarity = similarity / np.linalg.norm(similarity, axis=1)
135/198: similarity = similarity / np.linalg.norm(similarity, axis=1).T
135/199: similarity = similarity.T / np.linalg.norm(similarity, axis=1)
135/200: similarity
135/201: similarity.shape
135/202: mm.shape
135/203: similarity.dot(mm)
135/204: similarity.T.dot(mm)
135/205: similarity.T.dot(mm).T
135/206: similarity.dot(mm).T
135/207: similarity.T.dot(mm)
135/208: similarity.T.dot(mm).T
135/209: similarity.dot(mm)
135/210: similarity.dot(mm.T)
135/211: similarity.T.dot(mm.T)
135/212:
    similarity = encoder(queries).T
    similarity /= np.linalg.norm(similarity, axis=1)
    similarity = similarity.T.dot(memmap.T)
135/213: similarity.shape
135/214:
    similarity = encoder(queries)
    similarity = similarity / np.linalg.norm(similarity, axis=1)
135/215:
    similarity = encoder(queries)
    similarity = similarity.T / np.linalg.norm(similarity, axis=1)
135/216:
    similarity = encoder(queries).T
    similarity = similarity / np.linalg.norm(similarity, axis=1)
135/217:
    similarity = encoder(queries)
    similarity = similarity.T / np.linalg.norm(similarity, axis=1)
    similarity = similarity.T.dot(memmap.T)
135/218:
    similarity = encoder(queries)
    similarity = similarity.T / np.linalg.norm(similarity, axis=1)
    similarity = similarity.T.dot(mm.T)
135/219: similarity
135/220: limit=10
135/221:
    ind = np.array([
        np.argpartition(s, -limit)[-limit:]
        for s in similarity
    ])
135/222: ind
135/223: labels=df['title'].values; limit=20
135/224: labels
135/225: labels[ind]
135/226: queries=["Kyiv", "Lviv"]
135/227:
    similarity = encoder(queries)
    similarity = similarity.T / np.linalg.norm(similarity, axis=1)
    similarity = similarity.T.dot(mm.T)
135/228:
    ind = np.array([
        np.argpartition(s, -limit)[-limit:]
        for s in similarity
    ])
135/229: labels[ind]
135/230: queries=["Kyiv", "Kyev"]
135/231:
    similarity = encoder(queries)
    similarity = similarity.T / np.linalg.norm(similarity, axis=1)
    similarity = similarity.T.dot(mm.T)
135/232:
    ind = np.array([
        np.argpartition(s, -limit)[-limit:]
        for s in similarity
    ])
135/233: labels[ind]
135/234: fout
135/235: fin
135/236: ls -hal
135/237: fin = Path('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv').open()
135/238: fin.path
135/239: fin.fileno
135/240: fin.fileno()
135/241: fin.name
135/242: io.StringIO('hello\nworld\n').name
135/243: np.memmap?
135/244: Path(fin.name).is_file()
135/245: np.loadtxt?
135/246: fout = fin
135/247: fout = Path(fin.name)
135/248:
    if isinstance(fout, Path):
        if fout.is_file():
            fout.open('r+')
        else:
            fout.open('w+')
135/249: fout.mode
135/250: dir(fout)
135/251: fout.stat
135/252: type(mm)
135/253: isinstance(mm, np.memmap)
135/254: fout.tell()
135/255: fout.open().tell()
135/256: fout.open().seek(-1)
135/257: fout.open().seek(None)
135/258: fout.open().seek(0)
135/259: fout = fout.open()
135/260: mm.name
135/261: mm.filename
135/262: mm.offset
135/263: mm[-1].offset
135/264: mm[-1]
135/265: dir(mm)
135/266: mm.resize?
136/1: from search_engine import *
136/2: model.encode(['hi'])[0]
136/3: from search_engine import *
136/4: from search_engine import *
136/5: model.encode(['hi'])[0]
136/6: model.encode(['hi'])[0].shape
136/7: model.encode(['hi']).shape
136/8: len(model.encode(['hi'])[0])
137/1: from search_engine import *
137/2: fin = Path('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv').open()
137/3: mm = encode_to_memmap(fin)
137/4: fin = Path('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv').open()
138/1: from search_engine import *
138/2: fin = Path('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv').open()
138/3: mm = encode_to_memmap(fin)
139/1: fin = Path('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv').open()
139/2: from search_engine import *
139/3: fin = Path('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv').open()
139/4: mm = encode_to_memmap(fin)
139/5: mm.filename
139/6: fout
139/7: mm = np.memmap('batches.np.memmap', mode='r+', shape=(len(batches), len(batches[0])), offset=0, dtype=np.float32, order='C')  # C=C-style == row-major order, which is the default!
139/8: mm = np.memmap('batches.np.memmap', mode='r+', dtype=np.float32, order='C')  # C=C-style == row-major order, which is the default!
139/9: mm.shape
139/10: len(mm)
139/11: mm.size
139/12: mm.size?
140/1: from search_engine import *
140/2: fin = Path('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv').open()
140/3: mm = encode_to_memmap(fin)
140/4: ls /home/hobs/.nlpia2-data/wikipedia-titles-100k.*
140/5: ls /home/hobs/.nlpia2-data/*.memmap
140/6: fin.name
141/1: from search_engine import *
141/2: fin = Path('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv').open()
141/3: mm = encode_to_memmap(fin)
142/1: from search_engine import *
142/2: fin = Path('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv').open()
142/3: mm = encode_to_memmap(fin)
143/1: from search_engine import *
143/2: fin = Path('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv').open()
143/3: mm = encode_to_memmap(fin)
144/1: from search_engine import *
144/2: fin = Path('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv').open()
144/3: mm = encode_to_memmap(fin)
144/4: rm /home/hobs/.nlpia2-data/wikipedia-titles-100k.memmap
144/5: fin = Path('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv').open()
144/6: mm = encode_to_memmap(fin)
144/7: fin = Path('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv').open()
144/8: iterable_passage_batches = iter(generate_batches(generate_passages(fin)))
144/9: iterable_passage_batches
144/10: list(iterable_passage_batches)
144/11: iterable_passage_batches
144/12: next(iterable_passage_batches)
144/13: iterable_passage_batches = iter(generate_batches(generate_passages(fin)))
144/14:
for i in iterable_passage_batches:
    print(i)
144/15: fin = Path('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv').open()
144/16: iterable_passage_batches = iter(generate_batches(generate_passages(fin)))
144/17:
for i in iterable_passage_batches:
    print(i)
144/18: fin = Path('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv').open()
144/19: it = iter(generate_batches(fin))
144/20: next(it)
144/21: next(it)
145/1: from search_engine import *
145/2: fin = Path('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv').open()
145/3: rm /home/hobs/.nlpia2-data/wikipedia-titles-100k.memmap
145/4: it = iter(generate_batches(fin))
145/5: next(it)
145/6: next(it)
145/7: generate_batches.batch
145/8: more '/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv'
145/9: df = pd.read_csv('/home/hobs/.nlpia2-data/wikipedia-20220228-titles-all-in-ns0.csv.gz')
145/10: df.to_csv('/home/hobs/.nlpia2-data/wikipedia-20220228-titles-all-in-ns0.csv')
145/11: df
145/12: istitle = df['title'].str[0].isin('ABCDEFGHIJKLMNOPQRSTUVWXYZ')
145/13: istitle = df['title'].str[0].isin(list('ABCDEFGHIJKLMNOPQRSTUVWXYZ'))
145/14: istitle.sum()
145/15: istitle.sum() / len(istitle)
145/16: more /home/hobs/.nlpia2-data/wikipedia-20220228-titles-all-in-ns0.csv
145/17: df['title'][istitle].to_csv('/home/hobs/.nlpia2-data/wikipedia-20220228-titles-all-in-ns0.txt')
145/18: more '/home/hobs/.nlpia2-data/wikipedia-20220228-titles-all-in-ns0.txt'
145/19: istitle = istitle & df['title'].str[-1].isin(list('ABCDEFGHIJKLMNOPQRSTUVWXYZ'.lower()))
145/20: istitle.sum()
145/21: istitle.sum() / len(istitle)
145/22: df['title'][istitle].to_csv('/home/hobs/.nlpia2-data/wikipedia-20220228-titles-all-in-ns0.txt', index=False)
145/23: more '/home/hobs/.nlpia2-data/wikipedia-20220228-titles-all-in-ns0.txt'
145/24: fin = Path('/home/hobs/.nlpia2-data/wikipedia-20220228-titles-all-in-ns0.txt').open()
145/25: it = iter(generate_batches(fin))
145/26: next(it)
145/27: next(it)
145/28: next(it)
145/29: next(it)
145/30: next(it)
145/31: fin.seek(0)
145/32: next(it)
145/33: fin.seek(0)
145/34: it = iter(generate_encodings(generate_batches(fin)))
145/35: next(it)
145/36: next(it)
145/37: next(it)
145/38: fin.seek(0)
145/39: it = iter(generate_encodings(generate_batches(fin)))
145/40: type(fin)
145/41: isinstance(fin, io.TextIOWrapper)
146/1: from search_engine import *
146/2: fin = Path('/home/hobs/.nlpia2-data/wikipedia-20220228-titles-all-in-ns0.txt').open()
146/3: mm = encode_to_memmap(fin, batch_size=1)
147/1: from search_engine import *
147/2: fin = Path('/home/hobs/.nlpia2-data/wikipedia-20220228-titles-all-in-ns0.txt').open()
147/3: mm = encode_to_memmap(fin, batch_size=1)
147/4: rm '/home/hobs/.nlpia2-data/wikipedia-20220228-titles-all-in-ns0.memmap'
147/5: mm = encode_to_memmap(fin, batch_size=1)
147/6: mm
147/7: fout='/home/hobs/.nlpia2-data/wikipedia-20220228-titles-all-in-ns0.memmap'
147/8: ls -hal'/home/hobs/.nlpia2-data/wikipedia-20220228-titles-all-in-ns0.memmap'
147/9: ls -hal '/home/hobs/.nlpia2-data/wikipedia-20220228-titles-all-in-ns0.memmap'
147/10:
        mm = np.memmap(
            fout,
            mode='r+',
            offset=0,
            dtype=np.float32,
            order='C'
        )
147/11: mm.shape
147/12:
        mm = np.memmap(
            fout,
            mode='r+',
            offset=0,
            shape=(None, num_dim)
            dtype=np.float32,
            order='C'
        )
147/13:
        mm = np.memmap(
            fout,
            mode='r+',
            offset=0,
            shape=(None, 364),
            dtype=np.float32,
            order='C'
        )
147/14:
        mm = np.memmap(
            fout,
            mode='r+',
            offset=0,
            # shape=(None, 364),
            dtype=np.float32,
            order='C'
        )
147/15: mm.reshape(len(mm) // 364, 364)
147/16: 768 / 2
147/17: mm.reshape(len(mm) // 384, 384)
147/18: mm.flush()
147/19:
fin; fout=None; skip=0; batch_size=500;
        preprocessor=lambda x: x.strip().replace('_', ' ');
        encoder=model.encode
147/20: fin; fout=None; skip=0; batch_size=500; preprocessor=lambda x: x.strip().replace('_', ' '); encoder=model.encode
147/21: fin
147/22:
    encoder = getattr(encoder, 'encode', encoder)
    num_dim = len(encoder(['hi'])[0])
147/23:
    if isinstance(fin, str):
        if '\n' not in fin and '\r' not in fin:
            fin = Path(fin)
        else:
            fin = io.StringIO(fin)  # first line of string must be a filename!
    if isinstance(fin, Path):
        fin = fin.open()
147/24:

    fout = fout or getattr(fin, 'name', None)
    print(fout)
    fout = fout or DATA_DIR / fin.readline().strip()
    print(fout)
    fout = Path(fout).with_suffix('.memmap')
    print(fout)
    if isinstance(fout, str):
        fout = Path(fout)
147/25: fout
147/26:     iterable_passage_batches = iter(generate_batches(fin))  # generate_passages(fin)))
147/27:
    try:
        mm = np.memmap(
            fout,
            mode='r+',
            offset=0,
            dtype=np.float32,
            order='C'
        )
        mm.reshape(len(mm) / num_dim, num_dim)
        mm.flush()
        print(mm)
    except Exception as e:
        print(type(e), e)
        vectors = model.encode(next(iterable_passage_batches))
        print(vectors.shape)
        mm = np.memmap(
            fout,
            mode='w+',
            offset=0,
            shape=vectors.shape,
            dtype=vectors.dtype,
            order='C'
        )
        mm[:] = vectors
        mm.flush()
        print(mm)
147/28: mm.shape
147/29:
    for batch_num, batch in tqdm(enumerate(iterable_passage_batches)):
        print(batch)
        vectors = model.encode(batch)
        print(vectors.shape)
        mm.resize(mm.shape[0] + len(vectors), mm.shape[1])
        mm[-len(vectors):] = vectors
        mm.flush()
147/30: vectors.copy()
147/31:
    for batch_num, batch in tqdm(enumerate(iterable_passage_batches)):
        print(batch)
        vectors = model.encode(batch).copy()
        print(vectors.shape)
        mm.resize(mm.shape[0] + len(vectors), mm.shape[1])
        mm[-len(vectors):] = vectors
        mm.flush()
147/32: vectors
147/33: mm
147/34: mm[-1:]
147/35:
        vectors = model.encode(next(iterable_passage_batches))
        print(vectors.shape)
        mm = np.memmap(
            fout,
            mode='w+',
            offset=0,
            shape=vectors.shape,
            dtype=vectors.dtype,
            order='C'
        )
        mm[:] = vectors.copy()
        mm.flush()
147/36: mm
147/37: mm.resize(mm.resize(mm.shape[0] + len(vectors), mm.shape[1]))
147/38: num_lines = sum(1 for _ in fin)
147/39: num_lines
147/40: fin.seek(0)
147/41: num_lines = sum(1 for _ in fin)
147/42: num_lines
147/43: fin.seek(0)
147/44: mm = np.memmap(fout, mode='w+', shape=(num_lines, num_dim), dtype=np.float32, order='C')  # C=C-style == row-major order, which is the default!
148/1: fout = fin
148/2: from search_engine import *
149/1: from search_engine import *
149/2: fin = Path('/home/hobs/.nlpia2-data/wikipedia-20220228-titles-all-in-ns0.txt').open()
149/3: rm '/home/hobs/.nlpia2-data/wikipedia-20220228-titles-all-in-ns0.memmap'
149/4: mm = np.memmap('/home/hobs/.nlpia2-data/wikipedia-20220228-titles-all-in-ns0.memmap', dtype=np.float32, mode='w+', shape=(nrows, ncols))
149/5: nrows = sum(1 for _ in fin)
149/6: fin.seek(0)
149/7: ncols = len(model.encode(['hi'])[0])
149/8: ncols
149/9: mm = np.memmap('/home/hobs/.nlpia2-data/wikipedia-20220228-titles-all-in-ns0.memmap', dtype=np.float32, mode='w+', shape=(nrows, ncols))
149/10:     iterable_passage_batches = iter(generate_batches(fin))  # generate_passages(fin)))
149/11:
for i, batch in tqdm(enumerate(iterable_passage_batches)):
    mm[i, :] = model.encode(batch)[0]
149/12: mm = np.memmap('/home/hobs/.nlpia2-data/wikipedia-20220228-titles-all-in-ns0.memmap', dtype=np.float32, mode='w+', shape=(nrows, ncols))
149/13:
# for i, batch in tqdm(enumerate(iterable_passage_batches)):
#     mm[i*batch_size:(i+1)*batch_size, :] = model.encode(batch)
149/14: iterable_passage_batches = iter(generate_batches(generate_passages(fin), batch_size=1000))
149/15: next(iterable_passage_batches)
149/16: iterable_passage_batches = iter(generate_batches((x.strip().replace('_', ' ') for x in fin)), batch_size=1000))
149/17: iterable_passage_batches = iter(generate_batches((x.strip().replace('_', ' ') for x in fin), batch_size=1000))
149/18: it = iter(generate_batches((x.strip().replace('_', ' ') for x in fin), batch_size=1000))
149/19: next(it)
149/20: next(it)
149/21: fin.seek(0)
149/22: it = iter(generate_batches((x.strip().replace('_', ' ') for x in fin), batch_size=1000))
149/23: next(it)
149/24: fin.seek(0)
149/25: fin.readline()
149/26: it = iter(generate_batches((x.strip().replace('_', ' ') for x in fin), batch_size=1000))
149/27: numrows
149/28: nrows
149/29: nrows = sum(1 for _ in fin)
149/30: nrows
149/31: fin.seek(0)
149/32: fin.readline()
149/33: mm.shape
149/34: mm.resize(mm.shape[0]-1, mm.shape[1]))
149/35: mm.resize(mm.shape[0]-1, mm.shape[1])
149/36: np.resize(mm, shape=(nrows, ncols))
149/37: np.resize(mm, size=(nrows, ncols))
149/38: np.resize?
149/39: np.resize(mm, (nrows, ncols))
149/40: mm.shape
149/41:
for i, batch in tqdm(enumerate(it), total=nrows // batch_size):
    mm[i*batch_size:(i+1)*batch_size, :] = model.encode(batch)
149/42:
for i, batch in tqdm(enumerate(it), total=nrows // batch_size):
    mm[i*batch_size:(i+1)*batch_size, :] = model.encode(batch)
149/43: batch_size = 1000
149/44:
for i, batch in tqdm(enumerate(it), total=nrows // batch_size):
    mm[i*batch_size:(i+1)*batch_size, :] = model.encode(batch)
149/45: batch
149/46: len(batch)
149/47: it = iter(generate_batches((x.strip().replace('_', ' ') for x in fin), batch_size=1000))
149/48: fin.seek(0)
149/49: fin
149/50: it = iter(generate_batches((x.strip().replace('_', ' ') for x in fin), batch_size=1000))
149/51: len(next(it))
149/52: len(next(it))
149/53: len(next(it))
149/54: len(next(it))
149/55: fin.seek(0)
149/56: it = iter(generate_batches((x.strip().replace('_', ' ') for x in fin), batch_size=1000))
149/57: len(next(it))
149/58:
def generate_batches(iterable, batch_size=1):
    """ Break an iterable into batches (lists of len batch_size containing iterable's objects) """
    generate_batches.batch = []
    for i, obj in enumerate(iterable):
        generate_batches.batch.append(obj)
        if len(generate_batches.batch) >= batch_size:
            yield generate_batches.batch
149/59: fin.seek(0)
149/60: fin.readline()
149/61: it = iter(generate_batches((x.strip().replace('_', ' ') for x in fin), batch_size=1000))
149/62:
for i, batch in tqdm(enumerate(it), total=nrows // batch_size):
    mm[i*batch_size:(i+1)*batch_size, :] = model.encode(batch)
149/63: fin.seek(0)
149/64: it = iter(generate_batches((x.strip().replace('_', ' ') for x in fin), batch_size=1000))
149/65: next(it)
149/66: len(_)
149/67: next(it)
149/68: len(_)
149/69: next(it)
149/70: len(_)
149/71: next(it)
149/72: next(it)
149/73: next(it)
149/74: fin.seek(0)
149/75: it = iter(generate_batches(iter(x.strip().replace('_', ' ') for x in fin), batch_size=1000))
149/76:
def generate_batches(iterable, batch_size=1):
    """ Break an iterable into batches (lists of len batch_size containing iterable's objects) """
    generate_batches.batch = []
    for i, obj in enumerate(iterable):
        generate_batches.batch.append(obj)
        if len(generate_batches.batch) >= batch_size:
            yield generate_batches.batch
            generate_batches.batch = []
149/77: it = iter(generate_batches(iter(x.strip().replace('_', ' ') for x in fin), batch_size=1000))
149/78: fin.seek(0)
149/79: fin.readline()
149/80: it = iter(generate_batches(iter(x.strip().replace('_', ' ') for x in fin), batch_size=1000))
149/81: it = iter(generate_batches(iter(x.strip().replace('_', ' ') for x in fin), batch_size=1000))
149/82:
for i, batch in tqdm(enumerate(it), total=nrows // batch_size):
    mm[i*batch_size:(i+1)*batch_size, :] = model.encode(batch)
149/83: mm.flush()
149/84:
    similarity = encoder(queries)
    similarity = similarity.T / np.linalg.norm(similarity, axis=1)
    similarity = similarity.T.dot(mm.T)
149/85:
def find_similar(memmap, limit=1, queries=None, labels=None, encoder=model.encode):
    if isinstance(memmap, (str, Path)):
        memmap = np.memmap(memmap, mode='r')
    if isinstance(queries, str):
        queries = [queries]
    num_docs, num_dims = memmap.shape
    if labels is None:
        labels = np.arange(num_docs)
    similarity = encoder(queries)
    similarity /= np.linalg.norm(similarity, axis=1)
    similarity = similarity.dot(memmap.T)
    ind = np.array([
        np.argpartition(s, -limit)[-limit:]
        for s in similarity
    ])
    return labels[ind]
149/86: # find_similar(mm, limit=10, labels=list(fin))
149/87: fin.seek(0)
149/88: fin.readline()
149/89: labels=np.array(iter(x.strip().replace('_', ' ') for x in fin))
149/90: labels[0]
149/91: labels=np.array([x.strip().replace('_', ' ') for x in fin])
149/92: find_similar(mm, queries, limit=10, labels=labels)
149/93: queries=["Kyiv", "Kyev"]
149/94: find_similar(mm, queries=queries, limit=10, labels=labels)
149/95:
    similarity = encoder(queries)
    similarity = similarity.T / np.linalg.norm(similarity, axis=1)
    similarity = similarity.T.dot(mm.T)
149/96: encoder = model.encode
149/97:
    similarity = encoder(queries)
    similarity = similarity.T / np.linalg.norm(similarity, axis=1)
    similarity = similarity.T.dot(mm.T)
149/98:
    ind = np.array([
    ...:         np.argpartition(s, -limit)[-limit:]
    ...:         for s in similarity
    ...:     ])
149/99: limit=10
149/100:
    ind = np.array([
    ...:         np.argpartition(s, -limit)[-limit:]
    ...:         for s in similarity
    ...:     ])
149/101: ind
149/102: labels
149/103: labels[ind]
149/104: len(labels)
149/105: fout
149/106: mm.filename
149/107: fout = Path(_)
149/108: fout
149/109: path = fout.with_suffix('.labels.memmap')
149/110: mm2 = np.memmap('/home/hobs/.nlpia2-data/wikipedia-20220228-titles-all-in-ns0.labels.memmap', dtype=str, mode='w+', shape=(nrows, 1))
149/111: mm2 = np.memmap('/home/hobs/.nlpia2-data/wikipedia-20220228-titles-all-in-ns0.labels.memmap', dtype=str, mode='w')
149/112: mm2 = np.memmap('/home/hobs/.nlpia2-data/wikipedia-20220228-titles-all-in-ns0.labels.memmap', dtype=str, mode='w+')
149/113: mm2 = np.memmap('/home/hobs/.nlpia2-data/wikipedia-20220228-titles-all-in-ns0.labels.memmap', dtype=str, mode='w+', shape=(len(labels),))
149/114: np.dtype
149/115: np.dtype(str)
149/116: mm2 = np.memmap('/home/hobs/.nlpia2-data/wikipedia-20220228-titles-all-in-ns0.labels.memmap', dtype=np.dtype(str), mode='w+', shape=(len(labels),))
149/117: labels.tofile?
149/118: hist ~0/80-150
149/119: hist ~0/70-150
149/120: fin.filepath
149/121: ls /home/hobs/.nlpia2-data/wikipedia-20220228-titles-all-in-ns0*
149/122: more '/home/hobs/.nlpia2-data/wikipedia-20220228-titles-all-in-ns0.txt'
149/123: hist -o -p -f memmap.hist.ipy
149/124: hist -f memmap.hist.py
149/125: mm.filename
149/126: mm.shape
149/127: mm[0]
149/128: hist -o -p -f memmap.hist.ipy
150/1: wexit
151/1: from knowt import *
151/2: from knowt.memmap import *
151/3: from knowt.memmap import *
151/4: who
151/5: it = iter(range(3))
151/6: next(it)
151/7: next(it)
151/8: dir(it)
151/9: it.__length_hint__
151/10: it.__length_hint__()
152/1: import numpy as np
152/2: mm = np.memmap('/home/hobs/.nlpia2-data/wikipedia-20220228-titles-all-in-ns0.memmap', dtype=np.float32)
152/3: mm.shape
152/4: import pandas as pd
152/5: df = pd.read_csv('/home/hobs/.nlpia2-data/wikipedia-titles-100k.csv')
152/6: df = pd.read_csv('/home/hobs/.nlpia2-data/wikipedia-20220228-titles-all-in-ns0.csv')
152/7: df.columns
152/8: %run src/knowt/memmap.py
153/1: from knowt.memmap import *
154/1: import pandas as pd
155/1: import transformers
155/2:
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(checkpoint)
155/3:
from transformers import AutoProcessor

checkpoint = "microsoft/git-base"
processor = AutoProcessor.from_pretrained(checkpoint)
155/4:
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(checkpoint)
155/5: %run scripts/image_caption_dataset.py
156/1: %run scripts/image_caption_dataset.py
157/1: %run src/knowt/together.py
158/1: %run app.py generate --output-dir-path="./results" --input-image-file-path="./sample/images_with_text/sample1.png"
159/1: import easyocr
159/2: imoprt torch
159/3: import torch
159/4: torch.backends
159/5: dir(torch.backends)
159/6: dir(torch.backends.openmp)
159/7: torch.backends.openmp?
159/8: torch.backends.cpu
159/9: torch.cpu
159/10: torch.gpu
160/1: from knowt.llm import *
160/2: ask_search('StankDawg was a busy with Binary Revolution Radio and so the idea lay dormant for some time.')
160/3: rag = RAG()
160/4: rag.db
160/5: rag.db.embeddings
160/6: rag.db.encoder(['StankDawg was a busy with Binary Revolution Radio and so the idea lay dormant for some time.'])
160/7: rag.db.encoder(['StankDawg was a busy with Binary Revolution Radio and so the idea lay dormant for some time.'])[0]
160/8: np.linalg.norm(rag.db.encoder(['StankDawg was a busy with Binary Revolution Radio and so the idea lay dormant for some time.'])[0])
160/9: import numpy as np
160/10: np.linalg.norm(rag.db.encoder(['StankDawg was a busy with Binary Revolution Radio and so the idea lay dormant for some time.'])[0])
160/11: np.linalg.norm(rag.db.embeddings[0]))
160/12: np.linalg.norm(rag.db.embeddings[0])
160/13: ls data/corpus/
160/14: ls data/corpus/sentences.csv
160/15: df = pd.read_csv('data/corpus/sentences.csv')
160/16: import pandas as pd
160/17: df = pd.read_csv('data/corpus/sentences.csv')
160/18: df
160/19: df.iloc[0]
160/20: df.sentence.iloc[0]
160/21: rag.db.encoder(df.sentence.iloc[0])
160/22: rag.db.encoder(df.sentence.iloc[0]) - rag.db.embeddings[0]
160/23: stank = rag.db.encoder(['StankDawg was a busy with Binary Revolution Radio and so the idea lay dormant for some time.'])[0]
160/24: stank.dot(rag.db.embeddings)
160/25: stank = rag.db.encoder(['StankDawg was a busy with Binary Revolution Radio and so the idea lay dormant for some time.'])
160/26: stank.dot(rag.db.embeddings)
160/27: stank.dot(rag.db.embeddings.T)
160/28: dots = stank.dot(rag.db.embeddings.T)
160/29: dots.argmax()
160/30: df.iloc[dots.argmax()]
160/31: df.sentence.str.strip() == 'StankDawg was a busy with Binary Revolution Radio and so the idea lay dormant for some time.'
160/32: df.sentence[df.sentence.str.strip() == 'StankDawg was a busy with Binary Revolution Radio and so the idea lay dormant for some time.']
160/33: df.sentence[df.sentence.str.strip().str.split()[0] == 'StankDawg']
160/34: df.sentence[df.sentence.str.strip().apply(lambda x: x.strip().split()[0]) == 'StankDawg']
160/35: df.sentence[df.sentence.apply(lambda x: x.strip().split()[0]) == 'StankDawg']
160/36: df = df.fillna('')
160/37: df.sentence[df.sentence.str.strip().apply(lambda x: x.strip().split()[0]) == 'StankDawg']
160/38: df.sentence[df.sentence.str.strip().apply(lambda x: x.strip().split() if x else x) == 'StankDawg']
160/39: df.sentence[df.sentence.str.strip().apply(lambda x: 'stank' in x.lower())]
160/40: df.sentence[df.sentence.apply(lambda x: 'stank' in x.lower())]
160/41: df.sentence
160/42: df.head().T
160/43: df.head(2).T
160/44: df.head(3).T
160/45: df['filename']
160/46: df['filename'].apply(lambda x: '0001' in x)
160/47: df['filename'].apply(lambda x: '0001' in x).sum()
160/48: df['filename'][0]
160/49: db = VectorDB(refresh=True)
161/1: import pandas as pd
161/2: from knowt.llm import *
161/3: from knowt.search_engine import *
161/4: db = VectorDB(refresh=True)
162/1: import numpy as np
162/2: np.array(range(3))
162/3: np.array([range(3), range(2, 8, 2)])
162/4: np.array([range(3), range(2, 8, 2)]).T
162/5: np.array([np.arange(3), range(2, 8, 2)]).T
162/6: np.array([range(4), range(2, 9, 2)]).T
162/7: np.array([range(4).reshape(2,2), np.arange(2, 9, 2).reshape(2,2)]).T
162/8: np.array([np.arange(4).reshape(2,2), np.array(np.arange(2, 9, 2)).reshape(2,2)]).T
162/9: np.array([np.arange(4).reshape(2,2), np.arange(2, 9, 2).reshape(2,2)]).T
162/10: np.array([np.arange(4).reshape(2,2), np.arange(2, 9, 2).reshape(2,2)])
162/11: np.array(np.concatenate([np.arange(4).reshape(2,2), np.arange(2, 9, 2).reshape(2,2)]))
162/12: np.arange(4).reshape(2,2)
162/13: np.array(np.concatenate([np.arange(6).reshape(2,3), np.arange(2, 13, 2).reshape(2,3)]))
162/14: np.arange(6).reshape(2,3)
162/15: np.array(np.concatenate([np.arange(6).reshape(2,3), np.arange(2, 13, 2).reshape(2,3)], axis=1))
162/16: hist -o -p
162/17:
    >>> e1 = np.arange(6).reshape(2,3)
    >>> e1
162/18: e2 = np.arange(2, 14, 2).reshape(2,3)
162/19: e2
162/20:
qu = "wow, welcome week!"
ty = qu.index("we")
162/21: ty
163/1: from knowt.scrape_hpr import *
163/2: df = scrape_index()
163/3:

HPR_CSV_PATH = DATA_DIR / 'hpr_podcasts.csv'
163/4: dfold = pd.read_csv(HPR_CSV_PATH)
163/5: ls data
163/6: ls corpus_hpr
163/7: ls data/corpus_hpr
163/8: ls data/corpus_hpr/*.csv
163/9:

HPR_CSV_PATH = DATA_DIR / 'hpr_podcasts_raw.csv.gz'
163/10:

HPR_CSV_PATH = DATA_DIR / 'corpus_hpr' / 'hpr_podcasts_raw.csv.gz'
163/11: dfold = pd.read_csv(HPR_CSV_PATH)
163/12: dfold
163/13: dfold.columns
163/14: dfold = pd.read_csv(HPR_CSV_PATH, index_col=0)
163/15: dfold
163/16: dfold.index = dfold.index.astype(int)
163/17: dfold.index = dfold.index.values.astype(int)
163/18: dfold.index = dfold.index.fillna(0).values.astype(int)
163/19: dfold.index
163/20: len(dfold.index)
163/21: dfold.loc[0]
163/22: dfold.columns
163/23: dfold.iloc[0]
163/24:     row = 'url full_title_4digit subtitle series audio show_notes tags'.split()
163/25: row
163/26:     row = 'user url title full_title_4digit subtitle series audio show_notes tags'.split()
163/27: dfold.columns = row
163/28: dfold.iloc[0]
163/29: row
163/30: columns = 'user url title full_title_4digit subtitle series audio A B show_notes tags'.split()
163/31: dfold.columns = columns
163/32: dfold.iloc[:2].T
163/33: columns = 'title url user user_url full_title full_title_4digit subtitle series audio_urls show_notes tags'.split()
163/34: dfold.columns = columns
163/35: dfold.iloc[:2].T
163/36: dfold.to_csv(HPR_CSV_PATH, index=None)
163/37: HPR_CSV_PATH = DATA_DIR / 'corpus_hpr' / 'hpr_podcasts.csv.gz'
163/38: dfold.to_csv(HPR_CSV_PATH, index=None)
163/39: dfold = pd.read_csv(HPR_CSV_PATH, index_col=0)
163/40: dfold.iloc[:2].T
163/41: dfold
163/42: dfold = pd.read_csv(HPR_CSV_PATH)
163/43: HPR_CSV_PATH = DATA_DIR / 'corpus_hpr' / 'hpr_podcasts.csv.gz'
163/44: dfold = pd.read_csv(HPR_CSV_PATH)
164/1: from knowt.scrape_hpr import *
164/2: !ping google.com
164/3: !ping google.com
164/4: dfold = pd.read_csv(HPR_CSV_PATH)dflatest = scrape_index()
164/5: dfold = pd.read_csv(HPR_CSV_PATH)
164/6: dflatest = scrape_index()
164/7: dfold.columns
165/1: from knowt.scrape_hpr import *
165/2: dfold = pd.read_csv(HPR_CSV_PATH)
165/3: dfold.iloc[0].T
165/4: dfold.iloc[0]['show_notes']
165/5: dfold.iloc[0].to_dict
165/6: dfold.iloc[0].to_dict()
165/7: dfold
165/8:
    episodes = []
    scraped_urls = sorted(dfold['url'].str.strip().unique())
165/9:
def scrape_episode(url='./eps/hpr0030/', **kwargs):
    if url.lstrip('.').lstrip('/').startswith('eps/hpr'):
        url = '/'.join(['https://hackerpublicradio.org', url.lstrip('.').lstrip('/')])
    resp = requests.get(url)
    s = bs4.BeautifulSoup(resp.text)
    title, comments = s.find_all('h1')[1:3]
    subtitle, series = s.find_all('h3')[1:3]
    show_notes = list(series.next_siblings)[-1].next.next_sibling
    links = list(series.parent.find_all('a'))
    tags = [
        a.text for a in links
        if a.get('href', '').lstrip('.').lstrip('/').startswith('tags.html#')
    ]
    audio_urls = [a.get('href', '') for a in links if (a.text.lower().strip()[-3:] in 'ogg spx mp3')]
    row = dict(kwargs)
    row.update(dict(
        url=url,
        full_title_4digit=title.text,
        subtitle=subtitle.text,
        series=series.text,
        audio_urls=audio_urls,
        show_notes=show_notes.text,
        tags=tags))
    series.parent.find_all('a')
    return row
165/10: episodes
165/11:
def scrape_episode(url='./eps/hpr0030/', **kwargs):
    if url.lstrip('.').lstrip('/').startswith('eps/hpr'):
        url = '/'.join(['https://hackerpublicradio.org', url.lstrip('.').lstrip('/')])
    resp = requests.get(url)
    s = bs4.BeautifulSoup(resp.text)
    title, comments = s.find_all('h1')[1:3]
    subtitle, series = s.find_all('h3')[1:3]
    show_notes = list(series.next_siblings)[-1].next.next_sibling
    links = list(series.parent.find_all('a'))
    tags = [
        a.text for a in links
        if a.get('href', '').lstrip('.').lstrip('/').startswith('tags.html#')
    ]
    audio_urls = [a.get('href', '') for a in links if (a.text.lower().strip()[-3:] in 'ogg spx mp3')]
    row = dict(kwargs)
    row.update(dict(
        url=url,
        full_title_4digit=title.text,
        subtitle=subtitle.text,
        series=series.text,
        audio_urls=audio_urls,
        show_notes=show_notes.text,
        tags=tags))
    series.parent.find_all('a')
    return row
165/12: dflatest = dflatest.sort_values('url')
165/13: dflatest = df
165/14:
    dfold = pd.read_csv(HPR_CSV_PATH)
    dflatest = scrape_index()
    dflatest = dflatest.sort_values('url')
165/15:     scraped_urls = sorted(dfold['url'].str.strip().unique())
165/16: dfold.sort_values('url')
165/17: dfold = pd.read_csv(HPR_CSV_PATH).sort_values('url')
165/18:
    scraped_urls = sorted(dfold['url'].str.strip().unique())
    episodes = []
165/19:
    for i, row in tqdm(dflatest.iterrows()):
        row = row.to_dict()
        url = row['url']
        if url in scraped_urls:
            continue
        episodes += [scrape_episode(url, **row)]
        scraped_urls += episodes[-1]['url'].strip()
165/20:
    episodes = []
    for i, row in tqdm(dflatest.iterrows()):
        row = row.to_dict()
        url = row['url']
        if url in scraped_urls:
            continue
        episodes += [scrape_episode(**row)]
        scraped_urls += episodes[-1]['url'].strip()
165/21: episodes
165/22:
def scrape_episode(url='./eps/hpr0030/'):
    if url.lstrip('.').lstrip('/').startswith('eps/hpr'):
        url = '/'.join(['https://hackerpublicradio.org', url.lstrip('.').lstrip('/')])
    resp = requests.get(url)
    s = bs4.BeautifulSoup(resp.text)
    title, comments = s.find_all('h1')[1:3]
    subtitle, series = s.find_all('h3')[1:3]
    show_notes = list(series.next_siblings)[-1].next.next_sibling
    links = list(series.parent.find_all('a'))
    tags = [
        a.text for a in links
        if a.get('href', '').lstrip('.').lstrip('/').startswith('tags.html#')
    ]
    audio_urls = [a.get('href', '') for a in links if (a.text.lower().strip()[-3:] in 'ogg spx mp3')]
    row = dict(
        url=url,
        full_title_4digit=title.text,
        subtitle=subtitle.text,
        series=series.text,
        audio_urls=audio_urls,
        show_notes=show_notes.text,
        tags=tags)
    series.parent.find_all('a')
    return row
165/23:
    episodes = []
    for i, row in tqdm(dflatest.iterrows()):
        row = row.to_dict()
        url = row['url']
        if url in scraped_urls:
            continue
        row.update(scrape_episode(row['url']))
        episodes.append(row)
        scraped_urls += episodes[-1]['url'].strip()
165/24: dflatest.set_index('url', drop=False)
165/25: dflatest = dflatest.set_index('url', drop=False).to_dict()
165/26:
    if len(episodes) == len(dflatest):
        dflatest = dflatest.set_index('url', drop=False).to_dict()
        for i, episode in tqdm(episodes):
            dflatest[episode['url']].update(episode)
        dflatest = pd.DataFrame(dflatest)
    else:
        dflatest = pd.concat([dfold, pd.DataFrame(episodes)], axis=0)
165/27: dflatest
165/28: dflatest.colum s
165/29: dflatest.columns
165/30: dflatest.iloc[:2].T
165/31: dflatest['seq_num']
165/32: dflatest['seq_num'] = dflatest['url'].str.split('/').str[-2].str[3:].astype(int)
165/33: dflatest.iloc[:2].T
165/34: dflatest.iloc[-2:].T
166/1: %run src/knowt/scrape_hpr
166/2: %run src/knowt/scrape_hpr
166/3: %run src/knowt/scrape_hpr
166/4: ls data/corpus_hpr/*.csv
166/5: ls data/corpus_hpr/*.csv.gz
166/6: dflatest
166/7: dflatest.iloc[-2:].T
166/8: dflatest.iloc[:2].T
166/9: len(dflatest)
166/10: who
166/11: dfold
166/12: len(dfold)
166/13: dflatest.sort_values('url')
166/14: dflatesst = dflatest.sort_values('url')
166/15: dflatest = dflatest.sort_values('url')
166/16: dflatesst.reset_index()
166/17: dflatesst.reset_index(drop=True)
166/18: dflatest.reset_index(drop=True)
166/19: dflatest = dflatest.reset_index(drop=True)
166/20: hist
166/21: %run src/knowt/scrape_hpr
166/22: dfold
166/23: len(dfold)
166/24: len(dflatest)
166/25: ls -hal data/corpus_hpr/hpr_podcasts.csv.gz
166/26: ls -al data/corpus_hpr/hpr_podcasts.csv.gz
166/27: %run src/knowt/scrape_hpr
166/28: bs4.BeautifulSoup?
166/29: len(dfold)
166/30: len(dflatest)
166/31: dfold
166/32:     dflatest['url'].str.replace(BASE_URL, '.')
166/33: BASE_URL = 'https://hackerpublicradio.org'
166/34:     dflatest['url'].str.replace(BASE_URL, '.')
166/35: dflatest['url'] = dflatest['url'].str.replace(BASE_URL, '.')
166/36: dflatest.drop_duplicates()
166/37: dflatest.audio_urls
166/38: dflatest.audio_urls.iloc[0]
166/39:
newcols = []
for urls in dflatest['audio_urls'].astype(list):
    newcols.append({u[-3:].lower(): u for u in urls})
166/40:
newcols = []
for urls in dflatest['audio_urls'].apply(exec):
    newcols.append({u[-3:].lower(): u for u in urls})
166/41:
newcols = []
for urls in dflatest['audio_urls'].apply(exec):
    print(urls)
    newcols.append({u[-3:].lower(): u for u in urls})
166/42:
newcols = []
for urls in dflatest['audio_urls'].apply(exec):
    print(urls)
    # newcols.append({u[-3:].lower(): u for u in urls})
166/43:
newcols = []
for urls in dflatest['audio_urls']:
    print(urls)
    # newcols.append({u[-3:].lower(): u for u in urls})
166/44:
newcols = []
for urls in dflatest['audio_urls']:
    if isinstance(urls, str):
        urls = urls.lstrip('[').rstrip(']').split(',')
        urls = [u.strip().strip('"').strip("'").strip() for u in urls]
    newcols.append({u[-3:].lower(): u for u in urls})
166/45:
newcols = []
for urls in dflatest['audio_urls'].fillna([]):
    if isinstance(urls, str):
        urls = urls.lstrip('[').rstrip(']').split(',')
        urls = [u.strip().strip('"').strip("'").strip() for u in urls]
    newcols.append({u[-3:].lower(): u for u in urls})
166/46:
newcols = []
for urls in dflatest['audio_urls'].fillna(''):
    if isinstance(urls, str):
        urls = urls.strip().lstrip('[').rstrip(']').split(',')
        urls = [u.strip().strip('"').strip("'").strip() for u in urls]
    newcols.append({u[-3:].lower(): u for u in urls})
166/47: pd.concat([dflatest, pd.DataFrame(newcols)], axis=1)
166/48: dflatest.audio_urls.head()
166/49: dflatest.audio_urls.head().astype(list)
166/50: list(dflatest.audio_urls.head())
166/51: list(dflatest.audio_urls.tail())
166/52: hist
167/1: from knowt.scrape_hpr import *
167/2: who
167/3: dfold = pd.read_csv(HPR_CSV_PATH).sort_values('url')
167/4: dfold.url
167/5: dfold['url'] = dfold['url'].str.replace(BASE_URL, '.')
167/6: dfold.url
167/7: dfold.duplicated
167/8: dfold.url.duplicated
167/9: dfold.duplicated('url')
167/10: dfold.duplicated('url').sum()
167/11: dfold[dfold.duplicated('url')]
167/12: dfold.tags
167/13: dfold.tags.isna().sum()
167/14: dfold.iloc[0].to_dict()
167/15: dfold['tags'].apply(eval_str)
167/16: dfold['tags'].apply(eval_str_list)
167/17: dfold['tags'].apply(eval_list_str)
167/18: type((tuple, set, np.array))
167/19: [type(x) for x in (tuple, set, np.array)]
167/20: [type(x) for x in (tuple, set, np.ndarray)]
167/21:
def coerce_to_list(obj):
    if isinstance(obj, (tuple, set, np.ndarray)):
        return list(obj)
    if not isinstance(obj, (list, str)) and not obj:
        return []
    if isinstance(obj, str):
        obj = obj.strip().lstrip('[').rstrip(']').split(',')
        return [u.strip().strip('"').strip("'").strip() for u in obj]
167/22: dfold['tags'].apply(coerce_to_list)
167/23: dfold['tags2'] = dfold['tags'].apply(coerce_to_list)
167/24: import json
167/25: dfold['tags'].apply(json.loads)
167/26: dfold['tags'].str.apply(json.loads)
167/27: dfold['tags'].astype(str).apply(json.loads)
167/28: dfold['tags'].apply(str).apply(json.loads)
167/29: json.loads('')
167/30: np.nan
167/31:
def json_loads(obj):
    if obj is None:
        return np.nan
    if isinstance(obj, (tuple, set, np.ndarray)):
        return list(obj)
    if not isinstance(obj, (list, str)) and not obj:
        return []
    if isinstance(obj, str):
        try:
            return json.loads(obj)
        except Exception:
            pass
    return str(obj)
167/32: dfold['tags3'] = dfold['tags'].apply(json_loads)
167/33: dfold['tags3'] == dfold['tags2']
167/34: dfold['tags3']
167/35: dfold['tags2']
167/36: dfold['tags2']
167/37: import yaml
167/38: yaml.load('"hello"')
167/39: yaml.parse('"hello"')
167/40: list(yaml.parse('"hello"'))
167/41: list(yaml.parse("'hello'"))
167/42: list(yaml.parse("{'hello': 'hi'}"))
167/43: whois 114.119.135.171
167/44: !whois 114.119.135.171
167/45: !dig 114.119.135.171
167/46:
# wget \
     --recursive \
     --level 5 \
     --no-clobber \
     --page-requisites \
     --adjust-extension \
     --span-hosts \
     --convert-links \
     --restrict-file-names=windows \
     --domains $DOMAIN \
     --no-parent \
         $URL
167/47: DOMAIN=greenteapress.com
167/48: DOMAIN='greenteapress.com'
167/49: URL='https://www.greenteapress.com/thinkpython/thinkCSpy/html/'
167/50: pwd
167/51: cd data
167/52: mkdir corpus_python_textbooks
167/53: cd corpus_python_textbooks
167/54: URL='https://www.greenteapress.com/thinkpython/thinkCSpy/html/'
167/55:
wget \
     --recursive \
     --level 5 \
     --no-clobber \
     --page-requisites \
     --adjust-extension \
     --span-hosts \
     --convert-links \
     --restrict-file-names=windows \
     --domains $DOMAIN \
     --no-parent \
         $URL
167/56: hist
168/1: import pandas as pd
168/2: pd.read_csv('short_urls.txv')
168/3: pd.read_csv('short_urls.tsv')
168/4: pd.read_csv('short_urls.tsv', sep='\t')
168/5: df = pd.read_csv('short_urls.tsv', sep='\t')
168/6: df.head(2).T
168/7: df.head(1).T
168/8: more short_urls.tsv
168/9: df = pd.read_csv('short_urls.tsv', sep='\t', header=0, index=False)
168/10: df = pd.read_csv('short_urls.tsv', sep='\t', header=0, index=None)
168/11: df = pd.read_csv('short_urls.tsv', sep='\t', header=0, index_col=None)
168/12: df.iloc[0]
168/13: df = pd.read_csv('short_urls.tsv', sep='\t', header=0, index_col=None)
168/14: df.iloc[0]
168/15: df = pd.read_csv('short_urls.tsv', sep='\t', header=0, index_col=None)
168/16: df
168/17: df.head(1).T
168/18: df.head(2).T
168/19: df.iloc(1:3).T
168/20: df.iloc[1:3].T
168/21: df.iloc[0].T
168/22:
table = []
for i, row in df.iterrows():
    table.append(row.to_dict())
168/23: table
168/24:
for row in table:
    if np.isnan(row['last_referrer']):
        print(row)
168/25: import numpy as np
168/26:
for row in table:
    if np.isnan(row['last_referrer']):
        print(row)
168/27:
for row in table:
    if not isinstance(row['last_referrer'], str):
        print(row)
168/28: df
168/29: df.iloc[22]
168/30: df.iloc[22]['last_visited']
168/31:
for row in table:
    if not isinstance(row['last_visited'], str):
        print(row)
168/32:
for i, row in enumerate(table):
    if not isinstance(row['last_visited'], str):
        print(i)
        table[i]['last_visited'] = table[i]['last_referrer']
168/33: df2 = pd.DataFrame(table)
168/34: df2
170/1: ls
170/2: cd code
170/3: cd hobs
170/4: cd proai.org
170/5: cd ..
170/6: ls -hal
170/7: cd ..
170/8: ls -hal
170/9: cd ..
170/10: cd hobs
170/11: ls
170/12: cd proai.org/
170/13: ls -hal
170/14: df = pd.read_csv('short_urls.tsv', sep='\t', header=0, index_col=None)
170/15: import pandas as pd
170/16: df = pd.read_csv('short_urls.tsv', sep='\t', header=0, index_col=None)
170/17: df = pd.read_csv('short_urls.tsv', sep='\t', header=0, index_col=None)
170/18: df
170/19: df
170/20: df[df['last_visited'].str.lower().str.startswith('desktop')]
170/21: df['last_visited'].str.lower().str.startswith('desktop')
170/22: df = df.fillna(' ')
170/23: df['last_visited'].str.lower().str.startswith('desktop')
170/24: df[df['last_visited'].str.lower().str.startswith('desktop')]
170/25: i = df['last_visited'].str.lower().str.startswith('desktop')]
170/26: i = df['last_visited'].str.lower().str.startswith('desktop')
170/27: df['browser'][i] = df['last_visited'][i].copy()
170/28:
table = []
for idx, row in df.iterrows():
    table.append(row.to_dict())
    row = table[-1]
    if row['last_visited'].lower().strip().startswith('desk') or row['last_visited'].lower().strip().startswith('mob'):
        row['browser'] = row['last_visited']
        row['last_visited'] = row['last_referrer']
170/29: pd.DataFrame(table)
170/30: df2 = pd.DataFrame(table)
170/31: df2[df2.columns[-2]]
170/32: df2[df2.columns[-3]]
170/33:
table = []
# for idx, row in df.iterrows():
    table.append(row.to_dict())
    row = table[-1]
    c0 = df.columns[-4]
    c1 = df.columns[-3]
    c2 = df.columns[-2]
    
    if not row[c2].lower().strip() and row[c1].lower().strip().startswith('feb'):
        row[c2] = row[c1]
        row[c1] = row[c0]
170/34:
table = []
for idx, row in df.iterrows():
    table.append(row.to_dict())
    row = table[-1]
    c0 = df.columns[-4]
    c1 = df.columns[-3]
    c2 = df.columns[-2]
    
    if not row[c2].lower().strip() and row[c1].lower().strip().startswith('feb'):
        row[c2] = row[c1]
        row[c1] = row[c0]
170/35: pd.DataFrame(table)
170/36:
table = []
for idx, row in df.iterrows():
    table.append(row.to_dict())
    row = table[-1]
    c0 = df.columns[-4]
    c1 = df.columns[-3]
    c2 = df.columns[-2]
    
    if row[c2].lower().strip() == row[c1].lower():
        row[c1] = ''
170/37: pd.DataFrame(table)
170/38:
table = []
for idx, row in df.iterrows():
    table.append(row.to_dict())
    row = table[-1]
    c0 = df.columns[-4]
    c1 = df.columns[-3]
    c2 = df.columns[-2]
    
    if not row[c2].lower().strip() and row[c1].lower().strip().startswith('feb'):
        row[c2] = row[c1]
        row[c1] = row[c0]
170/39: df3 = pd.DataFrame(table)
170/40:
table = []
for idx, row in df.iterrows():
    table.append(row.to_dict())
    row = table[-1]
    c0 = df.columns[-4]
    c1 = df.columns[-3]
    c2 = df.columns[-2]
    
    if row[c2].lower().strip() == row[c1].lower():
        row[c1] = ''
170/41: df4 = pd.DataFrame(table)
170/42: df4
170/43:
table = []
for idx, row in df.iterrows():
    table.append(row.to_dict())
    row = table[-1]
    c0 = df.columns[-3]
    c1 = df.columns[-2]
    c2 = df.columns[-1]
    
    if row[c1].strip() and row[c2].lower().strip() == row[c1].lower():
        row[c1] = ''
170/44:
table = []
for idx, row in df.iterrows():
    table.append(row.to_dict())
    row = table[-1]
    c0 = df.columns[-4]
    c1 = df.columns[-3]
    c2 = df.columns[-2]
    
    if row[c1] and row[c2].lower().strip() == row[c1].lower().strip():
        row[c1] = ''
170/45: hist
170/46: hist -o -p -f clean_short_urls_tsv.hist.ipy
171/1: np.argsort??
171/2: import numpy as np
171/3: np.argsort??
172/1: from knowt.ask import *
172/2:
    rag = RAG(prompt_template=RAG_PROMPT_TEMPLATE, llm_model=model, client=CLIENT)
    context = rag.db.search_pretty(question)
    print(context)
    # print("FIXME: context above doesn't seem to be used in the rag.ask() or doesnt have enough relevant info!")
    answers = rag.ask(question, context=context)
172/3: who
172/4:
question='What is Python?'
prompt_template=RAG_PROMPT_TEMPLATE
client=CLIENT
model='mistralai/mistral-7b-instruct'
172/5:
    rag = RAG(prompt_template=prompt_template, llm_model=model, client=client)
    context = rag.db.search_pretty(question)
    print(context)
    # print("FIXME: context above doesn't seem to be used in the rag.ask() or doesnt have enough relevant info!")
    answers = rag.ask(question, context=context)
172/6: question = 'What is the Python Global Interpretter Lock (GIL)?'
172/7:
    rag = RAG(prompt_template=prompt_template, llm_model=model, client=client)
    context = rag.db.search_pretty(question)
    print(context)
    # print("FIXME: context above doesn't seem to be used in the rag.ask() or doesnt have enough relevant info!")
    answers = rag.ask(question, context=context)
172/8: model
172/9: RAG_PROMPT
172/10: RAG_PROMPT_TEMPLATE
172/11: context
172/12: answers
172/13: question = 'Which HPR episode was about the Global Interpretter Lock?'
172/14:
    rag = RAG(prompt_template=prompt_template, llm_model=model, client=client)
    context = rag.db.search_pretty(question)
    print(context)
    # print("FIXME: context above doesn't seem to be used in the rag.ask() or doesnt have enough relevant info!")
    answers = rag.ask(question, context=context)
172/15: answers
172/16: question = 'Which HPR episode was about the Python Global Interpretter Lock?'
172/17:
    rag = RAG(prompt_template=prompt_template, llm_model=model, client=client)
    context = rag.db.search_pretty(question)
    print(context)
    # print("FIXME: context above doesn't seem to be used in the rag.ask() or doesnt have enough relevant info!")
    answers = rag.ask(question, context=context)
172/18: answers
172/19: rag
172/20: rag.hist
172/21: rag.hist[-1].keys()
172/22: rag.hist[-1]['question']
172/23: len(rag.hist[-1]['search_results'])
172/24: rag.db.search('Python GIL')
172/25: rag.db.pretty_search('Python GIL')
172/26: rag.db.search_pretty('Python GIL')
172/27: print(rag.db.search_pretty('Python GIL'))
172/28: print(rag.db.search_pretty('What is the Python GIL'))
172/29: print(rag.db.search_pretty('What is the GIL?'))
172/30: print(rag.db.search_pretty('GIL'))
172/31: print(rag.ask('GIL'))
172/32: rag.hist[-1]['prompt']
172/33: print(rag.hist[-1]['prompt'])
172/34: print(rag.hist[-1]['context'])
174/1: from knowt.ask import *
174/2: ask_rag('What is the GIL?')
174/3: ask_rag??
174/4: ask_rag??
174/5: import mmap
175/1: from llm import *
175/2: from knowt.llm import *
175/3: rag = RAG?
175/4: rag = RAG??
176/1: from knowt.llm import *
176/2: rag = RAG(db=VectorDB?
176/3: rag = RAG(db=VectorDB(df='./data/corpus_nutrition/'))
176/4: ls data/corpus_nutrition
176/5: more data/corpus_nutrition/nutrition_sentences.csv
176/6: rm -rf data/corpus_nutrition/nutrition_sentences.csv
176/7: ls data/corpus_nutrition/AdvancedAccumulation/
176/8: more data/corpus_nutrition/AdvancedAccumulation/filter.html.md
176/9: rm -rf data/corpus_nutrition/*
176/10: ls data/corpus_fopp/
176/11: ls data/corpus_fopp/Files/
176/12: more data/corpus_fopp/Files/AlternativeFileReadingMethods.html.md
176/13: ls -hal data/corpus*
176/14: ls -hal data/
176/15: ls -hal data/corpus/
176/16: ls -hal data/corpus_hpr/
176/17: ls -hal data/corpus/
176/18: rm -rf data/corpus_nutrition/
177/1: from stackapi import StackAPI
177/2:
SITE = StackAPI('stackoverflow')
comments = SITE.fetch('comments')
177/3: questions = SITE.fetch('questions', min=40, tagged='python', sort='votes')
177/4: answers += SITE.fetch('answers', min=2, sort='votes')
177/5: answers = []
177/6: questions
177/7:
for q in tqdm(questions):
    answers += [SITE.fetch('answers', question_id=q['question_id'], min=1, sort='votes')]
177/8: from tqdm import tqdm
177/9:
for q in tqdm(questions['items']):
    answers += [SITE.fetch('answers', question_id=q['question_id'], min=1, sort='votes')]
    print(answers[-1]['quota_remaining'], answers[-1]['backoff'])
177/10: answers.keys()
177/11: answers[0]
177/12: answers[0].keys()
177/13: answers[-1]['quota_max']
177/14: answers[-1]['quota_remaining']
177/15: answers[-1]['backoff']
177/16: answers[-2]['backoff']
177/17: answers[0]['items']
177/18: questions['items'][0]
177/19: 10_000 / 3600 / 24
177/20: 3600 * 24
177/21: len(questions)
177/22: len(questions['items'])
177/23: len(answers)
177/24: len(answers['items'])
177/25: sum(len(a['items']) for a in answers)
177/26: sum(len(a['items']) for a in questions)
177/27: questions[0]
177/28: questions.keys()
177/29: len(answers)
177/30: import jsonlines
177/31: mkdir data/corpus_stackoverflow
177/32: fout = jsonlines.open('data/corpus_stackoverflow/questions.jsonlines', 'a')
177/33: fout.write_all(questions)
177/34: fout = jsonlines.open('data/corpus_stackoverflow/answers.jsonlines', 'a')
177/35: fout.write_all(answers)
177/36: ls -hal data/corpus_stackoverflow/
177/37: fout = jsonlines.open('data/corpus_stackoverflow/questions.jsonlines', 'a')
177/38: fout.write_all([questions])
177/39: fout = jsonlines.open('data/corpus_stackoverflow/questions.jsonlines', 'w')
177/40: fout.write_all([questions])
177/41: fout.close()
177/42: ls -hal data/corpus_stackoverflow/
177/43: fout = jsonlines.open('data/corpus_stackoverflow/answers.jsonlines', 'a')
177/44: fout.write_all(answers)
177/45: ls -hal data/corpus_stackoverflow/
177/46: fout = jsonlines.open('data/corpus_stackoverflow/answers.jsonlines', 'w')
177/47: fout.write_all(answers)
177/48: fout.close()
177/49: hist -o -p
177/50: answers[0]
177/51: answers[0]['items']
177/52: answers[0]['items'][0]
177/53: len(answers[0]['items'])
177/54: answers[0]['items'][-1]
177/55: hist -o -p
177/56: questions['items'][0]
177/57: q
177/58: q['question_id']
177/59: answers += [SITE.fetch('answers', question_id=q['question_id'])]
177/60: answers[-1]
177/61: q['question_id']
177/62: answers += [SITE.fetch?
177/63: qans = [SITE.fetch(f'questions/{q["question_id"]}/answers', filter='withbody')]
177/64: qans
177/65: len(qans['items'])
177/66: len(qans)
177/67: len(qans[0])
177/68: len(qans[0]['items'])
177/69: q['question_id']
177/70: hist
177/71: questionswithbodies = SITE.fetch('questions', min=40, tagged='python', sort='votes', filter='withbody')
177/72: questionswithbodies['items'][0]
177/73: questions['items'][0]
177/74:
>>> with jsonlines.open('data/corpus_stackoverflow/questions.jsonlines', 'w') as qfile:
>>>     qfile.write_all([questions])
177/75:
from tqdm import tqdm
answers = []

for q in tqdm(questions['items']):
    qans = client.fetch(f'questions/{q["question_id"]}/answers', filter='withbody')
    answers.append(qans)
    print(qans['quota_remaining'], qans[-1]['backoff'])
177/76: client = SITE
177/77:
from tqdm import tqdm
answers = []

for q in tqdm(questions['items']):
    qans = client.fetch(f'questions/{q["question_id"]}/answers', filter='withbody')
    answers.append(qans)
    print(qans['quota_remaining'], qans[-1]['backoff'])
177/78:
from tqdm import tqdm
answers = []

for q in tqdm(questions['items']):
    qans = client.fetch(f'questions/{q["question_id"]}/answers', filter='withbody')
    answers.append(qans)
    print(qans['quota_remaining'], qans['backoff'])
177/79:
>>> with jsonlines.open('data/corpus_stackoverflow/answers.jsonlines', 'w') as afile:
>>>     afile.write_all([answers])
178/1: from llm import *
178/2: from knowt.llm import *
178/3: who
178/4: RAG?
178/5: RAG(db='data/corpus_hpr')
178/6: HPR_CSV_PATH = DATA_DIR / 'corpus_hpr' / 'hpr_podcasts.csv.gz'
178/7: from knowt.constants import *
178/8: HPR_CSV_PATH = DATA_DIR / 'corpus_hpr' / 'hpr_podcasts.csv.gz'
178/9: !git grep 'nutrition_sentences.csv'
178/10: !git grep 'nutrition_sentences'
178/11: git log --stat data/corpus/nutrition_sentences.csv
178/12: !git log --stat data/corpus/nutrition_sentences.csv
178/13: !git log --stat | grep data/corpus/nutrition_sentences.csv -B10
178/14: !git log --stat | egrep data/corpus.nutrition -B10
178/15: !git log --stat | egrep data/corpus_nutrition -B10
178/16: !git log --stat
178/17: 1R
178/18: !git log --pretty
178/19: q
178/20: !git log --help
178/21: !gitlog
178/22: !glog
178/23: !git log
179/1: from knowt.llm import *
179/2:
>>> db = VectorDB(DATA_DIR / 'corpus_nutrition', search_limit=10, min_relevance=.5)
>>> q = 'What is the healthiest fruit?'
>>> topdocs = db.search(q)
>>> len(topdocs)
179/3: from search_engine import *
179/4: from knowt.search_engine import *
179/5: from search import *
179/6: from knowt.search import *
180/1: import bz2
180/2: import knowt
180/3: import knowt.search
181/1: from knowt.search import *
181/2:
>>> db = VectorDB(DATA_DIR / 'corpus_nutrition', search_limit=10, min_relevance=.5)
>>> q = 'What is the healthiest fruit?'
>>> topdocs = db.search(q)
>>> len(topdocs)
182/1: from knowt.search import *
182/2: >>> db = VectorDB(DATA_DIR / 'corpus_nutrition', search_limit=10, min_relevance=.5)
183/1: from knowt.search import *
183/2: >>> db = VectorDB(DATA_DIR / 'corpus_nutrition', search_limit=10, min_relevance=.5)
184/1: from knowt.search import *
184/2: >>> db = VectorDB(DATA_DIR / 'corpus_nutrition', search_limit=10, min_relevance=.5)
185/1: from knowt.search import *
185/2: >>> db = VectorDB(DATA_DIR / 'corpus_nutrition', search_limit=10, min_relevance=.5)
185/3: ls data/corpus_nutrition/
185/4: ls data/corpus_nutrition/*.csv.bz2
185/5: ls data/corpus_nutrition/*.csv
186/1: from knowt.search import *
186/2: >>> db = VectorDB(DATA_DIR / 'corpus_nutrition', search_limit=10, min_relevance=.5)
186/3:
>>> db = VectorDB(DATA_DIR / 'corpus_nutrition', search_limit=10, min_relevance=.5)
>>> q = 'What is the healthiest fruit?'
>>> topdocs = db.search(q)
>>> len(topdocs)
186/4: topdocs
186/5: topdocs['sentence']
186/6: print('\n'.join(topdocs['sentence']))
186/7: pretty_print(topdocs)
186/8: print('\n'.join(topdocs['sentence'][:2]))
186/9:
>>> topdocs = db.search(q, min_relevance=.7)
>>> len(topdocs)
186/10:
>>> topdocs = db.search(q, min_relevance=.69)
>>> len(topdocs)
186/11: topdocs[TEXT_LABEL][0][:10]
186/12: topdocs[TEXT_LABEL].iloc[0][:10]
186/13: topdocs[TEXT_LABEL].iloc[0][:20]
186/14: topdocs[TEXT_LABEL].iloc[0][:21]
186/15: topdocs[TEXT_LABEL].iloc[0][:22]
186/16: topdocs[TEXT_LABEL].iloc[0][:32]
186/17: topdocs[TEXT_LABEL].iloc[0][:30]
186/18: hist -o -p
187/1: from pathlib import Path
187/2: Path('what.csv.bz2.joblib')
187/3: p = _
187/4: p.with_suffix('.x')
187/5: p.suffixes
187/6: p.name
187/7: p.with_suffix('')
187/8: p.with_suffix('')
187/9: p.with_suffix('').with_suffix('')
187/10: p.with_suffix('').with_suffix('').with_suffix('')
187/11: p.with_suffix('').with_suffix('').with_suffix('').with_suffix('')
187/12: p.with_suffix('').with_suffix('').with_suffix('').with_suffix('').suffix
187/13: p.with_suffix?
188/1: import doctest
188/2: import knowt.search
189/1: import doctest
189/2: from knowt import search
189/3: doctest.testmod(search)
189/4: doctest.testmod(search)
190/1: import doctest
190/2: from knowt import search
190/3: import doctest
190/4: doctest.testmod(search)
191/1: import doctest
191/2: from knowt import search
191/3: doctest.testmod(search)
192/1: from knowt.search import *
192/2:
>>> db = VectorDB(DATA_DIR / 'corpus_nutrition', search_limit=10, min_relevance=.5)
>>> q = 'What is the healthiest fruit?'
>>> topdocs = db.search(q)
>>> len(topdocs)
192/3: rm data/corpus_nutrition/sentences*
192/4: rm data/corpus_nutrition/sentence*
192/5: ls -hal data/corpus_nutrition/*csv*
192/6: rm data/corpus_nutrition/*.csv
192/7:
>>> db = VectorDB(DATA_DIR / 'corpus_nutrition', search_limit=10, min_relevance=.5)
>>> q = 'What is the healthiest fruit?'
>>> topdocs = db.search(q)
>>> len(topdocs)
192/8: ls data/corpus/*csv*
192/9: ls data/corpus_nutrition/*csv*
193/1:
>>> rag = RAG(db=DATA_DIR / 'corpus_nutrition')
>>> q = 'What is the healthiest fruit?'
>>> rag.ask(q)
193/2: from knowt.llm import *
193/3:
>>> rag = RAG(db=DATA_DIR / 'corpus_nutrition')
>>> q = 'What is the healthiest fruit?'
>>> rag.ask(q)
193/4: from knowt.constants import *
193/5:
>>> rag = RAG(db=DATA_DIR / 'corpus_nutrition')
>>> q = 'What is the healthiest fruit?'
>>> rag.ask(q)
193/6: from knowt.constants import *
194/1: from knowt.constants import *
194/2: from knowt.llm import *
194/3:
>>> rag = RAG(db=DATA_DIR / 'corpus_nutrition')
>>> q = 'What is the healthiest fruit?'
>>> rag.ask(q)
195/1: from knowt.llm import *
195/2: from knowt.constants import *
195/3:
>>> rag = RAG(db=DATA_DIR / 'corpus_nutrition')
>>> q = 'What is the healthiest fruit?'
>>> rag.ask(q)
196/1: from knowt.llm import *
196/2: from knowt.constants import *
196/3:
>>> rag = RAG(db=DATA_DIR / 'corpus_nutrition')
>>> q = 'What is the healthiest fruit?'
>>> rag.ask(q)
197/1: from knowt.llm import *
197/2: from knowt.constants import *
197/3:
>>> rag = RAG(db=DATA_DIR / 'corpus_nutrition')
>>> q = 'What is the healthiest fruit?'
>>> rag.ask(q)
197/4:
        rag.completion = rag.client.chat.completions.create(
            extra_headers={
                "HTTP-Referer": "https://qary.ai",  # Optional, for including your app on openrouter.ai rankings.
                "X-Title": "https://qary.ai",  # Optional. Shows in rankings on openrouter.ai.
            },
            model=rag.llm_model_name,
            messages=[{"role": "user", "content": rag.prompt}, ],)
197/5: rag.prompt_template.format(**rag.hist[-1])
197/6: prompt = rag.prompt_template.format(**rag.hist[-1])
197/7:
        rag.completion = rag.client.chat.completions.create(
            extra_headers={
                "HTTP-Referer": "https://qary.ai",  # Optional, for including your app on openrouter.ai rankings.
                "X-Title": "https://qary.ai",  # Optional. Shows in rankings on openrouter.ai.
            },
            model=rag.llm_model_name,
            messages=[{"role": "user", "content": prompt}, ],)
197/8: rag.completion.choices
197/9: rag.completion.choices[0]
197/10: rag.completion
198/1:
>>> rag = RAG(db=DATA_DIR / 'corpus_nutrition')
>>> q = 'What is the healthiest fruit?'
>>> rag.ask(q)
198/2: from knowt.constants import *
198/3: from knowt.llm import *
198/4:
>>> rag = RAG(db=DATA_DIR / 'corpus_nutrition')
>>> q = 'What is the healthiest fruit?'
>>> rag.ask(q)
199/1: from knowt.constants import *
199/2: from knowt.llm import *
199/3:
>>> rag = RAG(db=DATA_DIR / 'corpus_nutrition')
>>> q = 'What is the healthiest fruit?'
>>> rag.ask(q)
200/1: from knowt.constants import *
200/2: from knowt.llm import *
200/3: from knowt.constants import *
200/4:
>>> rag = RAG(db=DATA_DIR / 'corpus_nutrition')
>>> q = 'What is the healthiest fruit?'
>>> rag.ask(q)
200/5: rag.hist[-1]
200/6: rag.hist[-1]['answer']
200/7: rag.hist[-1]['answer'][:30]
200/8: rag.hist[-1]['answer'][:60]
200/9: rag.hist[-1]['answer'][:59]
200/10: rag.hist[-1]['answer'][:69]
200/11: hist -o -p
200/12:
>>> q = 'How much exercise is healthiest?'
>>> rag.ask(q)
200/13:
>>> q = 'How much exercise is healthiest?'
>>> rag.ask(q)[:69]
200/14: hist -o -p
201/1: Path('what.csv.bz2.joblib')
201/2: from pathlib import Path
201/3: Path('what.csv.bz2.joblib')
201/4: p = _
201/5: p.parts
201/6: dir(p)
201/7: p.stem
201/8: p.with_name('')
201/9: p.with_name('x')
201/10: basename what/ever.txt
201/11: !basename what/ever.txt
201/12:
%%bash
ls
203/1:
from transformers import AutoProcessor, Blip2ForConditionalGeneration
import torch

processor = AutoProcessor.from_pretrained("Salesforce/blip2-opt-2.7b")
model = Blip2ForConditionalGeneration.from_pretrained("Salesforce/blip2-opt-2.7b", torch_dtype=torch.float16)
203/2: hist
204/1: from api.llm import *
204/2: who
204/3: generate_caption('/home/hobs/code/hobs/nlpia-manuscript/manuscript/images/ch02/piano_roll.jpg')
204/4: image = '/home/hobs/code/hobs/nlpia-manuscript/manuscript/images/ch02/piano_roll.jpg'
204/5:
        image = requests.get(image, stream=True).raw
        image = Image.open(image).convert('RGB')
204/6:
        image = requests.get(image, stream=True).raw
        image = Image.open(image).convert('RGB')
205/1: from api.llm import *
205/2:
        image = requests.get(image, stream=True).raw
        image = Image.open(image).convert('RGB')
205/3: image = '/home/hobs/code/hobs/nlpia-manuscript/manuscript/images/ch02/piano_roll.jpg'
205/4:
        image = Path(image).open('rb')
        image = Image.open(image).convert('RGB')
205/5: display(image.resize((596, 437)))
205/6: i = _
205/7: i.show()
205/8: display?
205/9:
    generated_ids = model.generate(**inputs, max_new_tokens=40)
    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
205/10:     inputs = processor(image, return_tensors="pt").to(device, torch.float16)
205/11: inputs
205/12:     generated_ids = model.generate(**inputs, max_new_tokens=40)
205/13:
import requests
from PIL import Image

url = 'https://media.newyorker.com/cartoons/63dc6847be24a6a76d90eb99/master/w_1160,c_limit/230213_a26611_838.jpg'
image = Image.open(requests.get(url, stream=True).raw).convert('RGB')  
display(image.resize((596, 437)))
205/14:
from transformers import AutoProcessor, Blip2ForConditionalGeneration
import torch

processor = AutoProcessor.from_pretrained("Salesforce/blip2-opt-2.7b")
model = Blip2ForConditionalGeneration.from_pretrained("Salesforce/blip2-opt-2.7b", torch_dtype=torch.float16)
205/15:
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
205/16:
inputs = processor(image, return_tensors="pt").to(device, torch.float16)

generated_ids = model.generate(**inputs, max_new_tokens=20)
205/17:
from PIL import Image
import requests
from transformers import Blip2Processor, Blip2ForConditionalGeneration
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"

processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
model = Blip2ForConditionalGeneration.from_pretrained(
    "Salesforce/blip2-opt-2.7b", load_in_8bit=True, device_map={"": 0}, torch_dtype=torch.float16
)  # doctest: +IGNORE_RESULT

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
205/18: !pip install accelerate
206/1:
import torch
from PIL import Image
import requests
from transformers import Blip2Processor, Blip2Model

processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
model = Blip2Model.from_pretrained("Salesforce/blip2-opt-2.7b")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(images=image, return_tensors="pt")
qformer_outputs = model.get_qformer_features(**inputs)
206/2:
from PIL import Image
import requests
from transformers import Blip2Processor, Blip2ForConditionalGeneration
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"

processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
model = Blip2ForConditionalGeneration.from_pretrained(
    "Salesforce/blip2-opt-2.7b", load_in_8bit=True, device_map={"": 0}, torch_dtype=torch.float16
)  # doctest: +IGNORE_RESULT

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
207/1:
from PIL import Image
import requests
from transformers import Blip2Processor, Blip2ForConditionalGeneration
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"

processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
model = Blip2ForConditionalGeneration.from_pretrained(
    "Salesforce/blip2-opt-2.7b", load_in_8bit=True, device_map={"": 0}, torch_dtype=torch.float16
)  # doctest: +IGNORE_RESULT

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
208/1: import accelerate
208/2:
from PIL import Image
import requests
from transformers import Blip2Processor, Blip2ForConditionalGeneration
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"

processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
model = Blip2ForConditionalGeneration.from_pretrained(
    "Salesforce/blip2-opt-2.7b", load_in_8bit=True, device_map={"": 0}, torch_dtype=torch.float16
)  # doctest: +IGNORE_RESULT

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
208/3: url
208/4: device
208/5:
from PIL import Image
import requests
from transformers import Blip2Processor, Blip2ForConditionalGeneration
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"

processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
model = Blip2ForConditionalGeneration.from_pretrained(
    "Salesforce/blip2-opt-2.7b", torch_dtype=torch.float16
)  # doctest: +IGNORE_RESULT

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
208/6:
inputs = processor(images=image, return_tensors="pt").to(device, torch.float16)

generated_ids = model.generate(**inputs)

generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()

print(generated_text)
208/7:
# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("image-to-text", model="Salesforce/blip2-opt-2.7b")
208/8: pipe??
208/9: url = "http://images.cocodataset.org/val2017/000000039769.jpg"
208/10: pipe(url)
208/11: pipe(url, max_new_tokens=20)
208/12: pipe(url, max_new_tokens=40)
208/13: hist
208/14: url = '/home/hobs/code/hobs/nlpia-manuscript/manuscript/images/ch02/piano_roll.jpg'
208/15: pipe(url, max_new_tokens=40)
208/16: hist
208/17:
# Use a pipeline as a high-level helper
from transformers import pipeline

pipe2 = pipeline("image-to-text", model="Salesforce/blip-image-captioning-large")
208/18: ls
208/19: from api.llm import *
208/20: who
208/21: generate_caption('/home/hobs/code/hobs/nlpia-manuscript/manuscript/images/ch02/piano_roll.jpg')
209/1: import yaml
209/2: yaml.load(open('image_log.yaml'))
209/3: yaml.safe_load(open('image_log.yaml'))
209/4: yaml.safe_load(open('image_log.yaml'))
209/5: yaml.safe_load(open('image_log.yaml'))
209/6: yaml.safe_load(open('image_log.yaml'))
209/7: yaml.safe_load(open('image_log.yaml'))
209/8: images=_
209/9: hist -f image_log.py
209/10: ipython
210/1: from knowt.constants import *
210/2: from knowt.llm import *
210/3: who
210/4: rag = RAG('data/corpus_nutrition')
210/5: rag = RAG(df='data/corpus_nutrition')
210/6: rag = RAG(db='data/corpus_nutrition')
211/1: from knowt.llm import *
211/2: rag = RAG(db='data/corpus_nutrition')
212/1: from knowt.llm import *
212/2: rag = RAG(db='data/corpus_nutrition')
212/3: rag = RAG(db=VectorDB('./data/corpus_nutrition'))
212/4:
>>> rag = RAG(db=DATA_DIR / 'corpus_nutrition')
>>> q = 'What is the healthiest fruit?'
>>> rag.ask(q)
212/5: from knowt.constants import *
212/6:
>>> rag = RAG(db=DATA_DIR / 'corpus_nutrition')
>>> q = 'What is the healthiest fruit?'
>>> rag.ask(q)
213/1: from knowt.constants import *
213/2: from knowt.llm import *
213/3:
>>> rag = RAG(db=DATA_DIR / 'corpus_nutrition')
>>> q = 'What is the healthiest fruit?'
>>> rag.ask(q)
213/4: >>> rag = RAG()
214/1: from knowt.llm import *
214/2: >>> rag = RAG()
215/1: from knowt.llm import *
215/2: >>> rag = RAG()
215/3: rag.ask('What is phone phreaking?')
215/4: rag.db.search('What is phone phreaking?')
215/5: rag.db.search('What is phone phreaking?', search_limit=10)
215/6: rag.db.search('What is phone phreaking?', limit=10)
215/7: rag.db.search('What is phone phreaking?', limit=10)['sentence']
215/8: rag.db.search_pretty('What is phone phreaking?')
215/9: rag.db.search('Explain phone phreaking to me.', limit=10)['sentence']
215/10: rag.ask('Explain phone phreaking to me.', limit=10)['sentence']
215/11: rag.ask('Explain phone phreaking to me.')
215/12: rag.ask('What is phone phreakin?')
215/13: rag.db.search('Explain phone phreaking to me.', limit=10)
215/14: rag.db.search('phone phreaking')
215/15: rag.db.search('phone phreaking', min_relevance=.5)
215/16: rag.db.search('Explain phone phreaking', min_relevance=.5)
215/17: rag.db.search('Explain phone phreaking to me', min_relevance=.5)
215/18: rag.ask('Explain phone phreaking to me', min_relevance=.5)
215/19: hist -o -p
216/1:
# Load model directly
from transformers import AutoModel
model = AutoModel.from_pretrained("anktiwerize/mistral-7b-fine-tuned-lrre")
216/2:
# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("text-generation", model="mistralai/Mistral-7B-Instruct-v0.2")
216/3: who
216/4: from knowt.llm import *
216/5: who
216/6: rag = RAG(db=DATA_DIR / 'corpus_hpr', self_hosted=True, llm_model="mistralai/Mistral-7B-Instruct-v0.2")
216/7: from knowt.search import *
216/8: rag = RAG(db=DATA_DIR / 'corpus_hpr', self_hosted=True, llm_model="mistralai/Mistral-7B-Instruct-v0.2")
217/1: from knowt.search import *
219/1: from knowt.constants import *
219/2: from knowt.search import *
219/3: from knowt.llm import *
219/4: rag = RAG(db=DATA_DIR / 'corpus_hpr', self_hosted=True, llm_model="mistralai/Mistral-7B-Instruct-v0.2")
219/5: rag.ask('Explain phone phreaking to me.', min_relevance=.5)
219/6: rag.temperature = temperature = min(temperature or 0, RAG_TEMPERATURE)
219/7: temperature = 0.05
219/8: rag.temperature = temperature = min(temperature or 0, RAG_TEMPERATURE)
219/9: from knowt.constants import *
219/10:
TODAY = datetime.date.today()
RAG_SEARCH_LIMIT = 8
RAG_MIN_RELEVANCE = .5
RAG_TEMPERATURE = 0.05
RAG_SELF_HOSTED = True
219/11: rag.temperature = temperature = min(temperature or 0, RAG_TEMPERATURE)
219/12: rag.ask('Explain phone phreaking to me.', min_relevance=.5)
219/13: rag.pipe
219/14: rag.prompt
219/15: rag.prompt_template.format(**rag.hist[-1])
219/16: prompt = rag.prompt_template.format(**rag.hist[-1])
219/17: ans = rag.pipe(prompt)
219/18: ans = rag.pipe(prompt, max_new_tokens=256)
219/19: ans
219/20: len(ans)
219/21: len(ans[0])
219/22: pipeline?
219/23: pipe?
219/24: rag.pipe?
219/25: print(ans[0]['generated_text'])
219/26: print(ans[0]['generated_text'][len(prompt):])
220/1: import pandas as pd
220/2: pd.read_csv?
220/3: import numpy as np
220/4: x = np.array([])
220/5: df = pd.DataFrame(x)
220/6: x = np.array([(0,)])
220/7: x
220/8: x = np.array(["{0: 5}"])
220/9: x = np.array(["{'k': 'v'}"])
221/1:
import turtle

wn = turtle.Screen()
mickey = turtle.Turtle()

mickey.circle(50)
mickey.left(110)
mickey.forward(110)
mickey.circle(30)
mickey.backward(110)
mickey.right(70)
mickey.forward(110)
mickey.circle(30)
222/1:
import turtle

wn = turtle.Screen()
mickey = turtle.Turtle()

mickey.circle(50)
mickey.left(110)
mickey.forward(110)
mickey.circle(30)
mickey.backward(110)
mickey.right(70)
mickey.forward(110)
mickey.circle(30)
223/1:
import turtle

wn = turtle.Screen()
mickey = turtle.Turtle()

mickey.circle(50)
mickey.left(110)
mickey.forward(110)
mickey.circle(30)
mickey.backward(110)
mickey.right(70)
mickey.forward(110)
mickey.circle(30)
224/1:
import turtle

wn = turtle.Screen()
mickey = turtle.Turtle()

mickey.circle(50)
mickey.left(110)
mickey.forward(110)
mickey.circle(30)
mickey.backward(110)
mickey.right(70)
mickey.forward(110)
mickey.circle(30)
224/2:
import turtle

wn = turtle.Screen()
mickey = turtle.Turtle()

mickey.circle(50)
mickey.left(110)
mickey.forward(110)
mickey.circle(30)
mickey.backward(110)
mickey.right(70)
mickey.forward(110)
mickey.circle(30)
225/1:
import turtle

wn = turtle.Screen()
mickey = turtle.Turtle()

mickey.circle(50)
mickey.left(110)
mickey.forward(110)
mickey.circle(30)
mickey.backward(110)
mickey.right(70)
mickey.forward(110)
mickey.circle(30)
225/2: del mickey
225/3: turtle.circle?
225/4: turtle.fillcolor?
225/5: turtle.fillcolor('red')
225/6:
import turtle

wn = turtle.Screen()
mickey = turtle.Turtle()

mickey.circle(50)
mickey.left(110)
mickey.forward(110)
mickey.circle(30)
mickey.backward(110)
mickey.right(70)
mickey.forward(110)
mickey.circle(30)
226/1:
import turtle

wn = turtle.Screen()
mickey = turtle.Turtle()

mickey.circle(50)
mickey.left(110)
mickey.penup()
mickey.forward(110)
mickey.pendown()
mickey.fillcolor(255, 0, 0)
mickey.circle(30)
mickey.backward(110)
mickey.right(70)
mickey.forward(110)
mickey.circle(30)
226/2: mickey.fillcolor((255,0,0))
226/3: mickey.fillcolor?
226/4: mickey.colormode
226/5: turtle.colormode()
226/6: mickey.fillcolor((1,0,0))
226/7: wn.clear()
226/8:
import turtle

wn = turtle.Screen()
mickey = turtle.Turtle()
colormode = turtle.colormode()

mickey.circle(50)
mickey.left(110)
mickey.penup()
mickey.forward(110)
mickey.pendown()
mickey.fillcolor((colormode, 0, 0))
mickey.circle(30)
mickey.backward(110)
mickey.right(70)
mickey.penup()
mickey.forward(110)
mickey.circle(30)
226/9: mickey.clear()
226/10: wn.clear()
226/11:
import turtle

wn = turtle.Screen()
mickey = turtle.Turtle()
colormode = turtle.colormode()

mickey.circle(50)
mickey.left(110)
mickey.penup()
mickey.forward(110)
mickey.pendown()
mickey.fillcolor((colormode, 0, 0))
mickey.begin_fill()
mickey.circle(30)
mickey.end_fill()
mickey.backward(110)
mickey.right(70)
mickey.penup()
mickey.forward(110)
mickey.pendown()
mickey.begin_fill()
mickey.circle(30)
226/12:
import turtle

wn = turtle.Screen()
mickey = turtle.Turtle()
colormode = turtle.colormode()

mickey.circle(50)
mickey.left(110)
mickey.penup()
mickey.forward(110)
mickey.pendown()
mickey.fillcolor((colormode, 0, 0))
mickey.begin_fill()
mickey.circle(30)
mickey.end_fill()
mickey.penup()
mickey.backward(110)
mickey.right(70)
mickey.forward(110)
mickey.pendown()
mickey.begin_fill()
mickey.circle(30)
226/13: mickey.end_fill()
226/14: hist -o -p -f turtle_red_mouse_ears.hist.ipy
226/15: mickey.clear()
226/16: mickey.clear()
226/17: wn.clear()
226/18:
import turtle

wn = turtle.Screen()
mickey = turtle.Turtle()
colormode = turtle.colormode()

mickey.circle(50)
mickey.left(110)
226/19:
mickey.left(110)
mickey.penup()
mickey.forward(110)
mickey.pendown()
mickey.fillcolor((colormode, 0, 0))
mickey.begin_fill()
mickey.circle(30)
mickey.end_fill()
226/20: mickey.clear()
226/21: wn.clear()
226/22:
mickey.penup()
mickey.forward(110)
mickey.pendown()
mickey.fillcolor((colormode, 0, 0))
mickey.begin_fill()
mickey.circle(30)
mickey.end_fill()
226/23: wn.clear()
226/24: wn.clear()
226/25:
import turtle

wn = turtle.Screen()
mickey = turtle.Turtle()
colormode = turtle.colormode()
226/26: mickey.circle(50)
226/27: mickey.left(110)
226/28:
mickey.penup()
mickey.forward(110)
mickey.pendown()
mickey.fillcolor((colormode, 0, 0))
mickey.begin_fill()
mickey.circle(30)
mickey.end_fill()
226/29: mickey.backward(110)
226/30: wn.clear()
226/31: wn.clear()
226/32: mickey.clear()
226/33: mickey.circle(50)
226/34: mickey.left(110)
226/35:
import turtle

wn = turtle.Screen()
mickey = turtle.Turtle()
colormode = turtle.colormode()
226/36: wn.clear()
226/37: mickey.clear()
226/38:
import turtle

wn = turtle.Screen()
mickey = turtle.Turtle()
colormode = turtle.colormode()
226/39: mickey.circle(50)
226/40: mickey.left(110)
226/41:
mickey.penup()
mickey.forward(110)
mickey.pendown()
226/42:
mickey.fillcolor((colormode, 0, 0))
mickey.begin_fill()
mickey.circle(30)
mickey.end_fill()
226/43:
mickey.penup()
mickey.backward(110)
mickey.pendown()
226/44:
mickey.right(70)
mickey.forward(110)
226/45: hist ~0/30-44
226/46: hist ~0/35-44
226/47: hist ~0/40-44
226/48: hist ~0/38-44
226/49: hist ~0/38-44 -f turtle_mouse_ears.hist.py
226/50: hist
226/51: hist ~0/38-44 -o -p -f turtle_mouse_ears.hist.ipy
226/52: turtle
226/53: wn
226/54: mickey
226/55: help(mickey.fillcolor)
226/56: mickey.fillcolor('red')
226/57: hist
226/58: turtle.colormode()
227/1:
    import requests
    data = {"data": {"ndarray": [message_text]}}
    r = requests.post(url=url, json=data)
    embeddings = r.json()["data"]["ndarray"]
227/2: message_text = 'Some new message text.'
227/3:
    import requests
    data = {"data": {"ndarray": [message_text]}}
    r = requests.post(url=url, json=data)
    embeddings = r.json()["data"]["ndarray"]
227/4: url = 'https://ml.delvin.to'
227/5:
    import requests
    data = {"data": {"ndarray": [message_text]}}
    r = requests.post(url=url, json=data)
    embeddings = r.json()["data"]["ndarray"]
227/6:
    import requests
    data = {"data": {"ndarray": [message_text]}}
    resp = requests.post(url=url, json=data)
    resp
227/7: resp.json()
227/8: url
227/9: url += '/'
227/10: url
227/11:
    import requests
    data = {"data": {"ndarray": [message_text]}}
    resp = requests.post(url=url, json=data)
    resp
227/12: resp.json()
228/1: import xarray
228/2: help(xarray)
228/3: from xarray import DataFrame
228/4: xarray.backends
228/5: xarray.backends()
228/6: list(xarray.backends)
228/7: xarray.load_data?
228/8: help(xarray.load_data)
228/9:
xr = xarray
data = xr.DataArray(np.random.randn(2, 3), dims=("x", "y"), coords={"x": [10, 20]})
228/10: import numpy as np
228/11:
xr = xarray
data = xr.DataArray(np.random.randn(2, 3), dims=("x", "y"), coords={"x": [10, 20]})
228/12: ls data/corpus_hpr/*embed*
228/13: import joblib
228/14: embed = joblib.load('data/corpus_hpr/sentences.embeddings.joblib')
228/15: embed.shape
228/16:

x = xr.DataArray(embed, dims=("sentence", "dim")), coords={"dim": list(range(384)) })
228/17:

x = xr.DataArray(embed, dims=("sentence", "dim"), coords={"dim": list(range(384)) })
228/18: x
228/19: x.backent
228/20: x.backend
228/21: x.to_zarr()
228/22: v = np.random.randn(3, 384)
228/23: v.dot(x)
228/24: v.dot(x.T)
228/25: %timeit v.dot(x.T)
228/26: %timeit v.dot(embed.T)
228/27: x.to_netcdf('data/corpus_hpr/sentences.embeddings.netcdf')
228/28: from numpy import memmap
228/29: memmap?
228/30: help(memmap)
228/31: embed0 = np.memmap('data/corpus_hpr/sentences.embeddings.memmap', shape=embed.shape)
228/32: embed0 = np.memmap('data/corpus_hpr/sentences.embeddings.memmap', shape=embed.shape, mode='w+')
228/33: embed0[:] = embed
228/34: embed0.flush()
229/1: embed = np.memmap('data/corpus_hpr/sentences.embeddings.memmap', shape=embed.shape, mode='r+')
229/2: import numpy as np
229/3: import xarray as xr
229/4: !free
229/5: !free -h
229/6: v = np.random.randn(3, 384)
229/7:

x = xr.open_dataset('data/corpus_hpr/sentences.embeddings.netcdf')
229/8: !free
229/9: %timeit v.dot(embed.T)
229/10: embed = np.memmap('data/corpus_hpr/sentences.embeddings.memmap', shape=embed.shape, mode='r+')
229/11: who
229/12: shape = (41531,384)
229/13: x.shape
229/14: x.dims
229/15: x.dims.shape
229/16: list(x.dims)
229/17: shape == (x.dims['sentence'], x.dims['dim'])
229/18: x.sizes
229/19: list(x.sizes)
229/20: shape == (x.sizes['sentence'], x.sizes['dim'])
229/21: shape = (x.sizes['sentence'], x.sizes['dim'])
229/22: embed = np.memmap('data/corpus_hpr/sentences.embeddings.memmap', shape=shape, mode='r+')
229/23: !free -h
229/24: !free
229/25: %timeit v.dot(embed.T)
229/26: %timeit v.dot(embed.T)
229/27: %timeit v.dot(x.T)
229/28: %timeit v.dot(x.transpose())
229/29: x = xr.open_dataset('data/corpus_hpr/sentences.embeddings.netcdf', shape=(shape[1], shape[0]))
229/30: x = xr.open_dataset?
229/31: x = x.transpose()
229/32: %timeit v.dot(x)
229/33: v = xarrayDataArray(v)
229/34: v = xarray.DataArray(v)
229/35: v = xr.DataArray(v)
229/36: %timeit v.dot(x)
229/37: xr.dot(v, x)
229/38: v = np.random.randn(3, 384)
229/39: xr.dot(v, x)
229/40: x.dims
229/41:
da_a = xr.DataArray(np.arange(3 * 2).reshape(3, 2), dims=["a", "b"])
da_b = xr.DataArray(np.arange(3 * 2 * 2).reshape(3, 2, 2), dims=["a", "b", "c"])
da_c = xr.DataArray(np.arange(2 * 3).reshape(2, 3), dims=["c", "d"])
229/42: da_a.dot(da_c)
229/43: da_a
229/44: type(da_a)
229/45: type(x)
229/46: x0 = x.to_dataarray()
229/47: !free
229/48: 8217772 - 8507388
229/49: !free -h
229/50: ls -hal data/corpus_hpr/sentences.embeddings.netcdf
229/51: ls -hal data/corpus_hpr/sentences.embeddings.joblib
229/52: ls -hal data/corpus_hpr/sentences.embeddings.memmap
229/53: 8302724 - 8507388
229/54: %timeit v.dot(x0)
229/55: %timeit v.dot(x0)
229/56: hist -o -p -f scripts/speed_test_xarray_vs_memmap.hist.ipy
229/57: memstart = 8268476
229/58: memopendataset = 8302724
229/59: memnpmemmap = 8217772
229/60: memxtodataarray = 8507388
229/61: !free
229/62: memend = 8432168
229/63: hist -o -p -f scripts/speed_test_xarray_vs_memmap.hist.ipy
229/64: hist -f scripts/speed_test_xarray_vs_memmap.hist.py
230/1: filenames = open('corpus_files.txt').readlines()
230/2: from pathlib import Path
230/3:
from knowt.constants import DATA_DIR, CORPUS_DIR
base_dir = (CORPUS_DIR / 'corpus_runestone_rst')
base_dir.mkdir()
230/4:
DATA_DIR = Path('/home/hobs/code/tangibleai/community/knowt/data')
base_dir = (DATA_DIR / 'corpus_runestone_rst')
base_dir.mkdir()
230/5:
filenames = open('corpus_files.txt').readlines()
for f in filenames:
    p = Path(f)
    if not p.parent.is_dir():
        p.parent.mkdir(parents=True, exist_ok=True)
    if p.suffix.lower() == '.rst':
        print(p)
        with p.open() as fin:
            with (base_dir / p).open('w') as fout:
                fout.writelines(fin.readlines())
230/6: filenames
230/7:
filenames = open('corpus_files.txt').readlines()
for f in filenames:
    p = Path(f.strip().lstrip('.').lstrip('/'))
    if not p.parent.is_dir():
        p.parent.mkdir(parents=True, exist_ok=True)
    if p.suffix.lower() == '.rst':
        print(p)
        with p.open() as fin:
            with (base_dir / p).open('w') as fout:
                fout.writelines(fin.readlines())
230/8:
filenames = open('corpus_files.txt').readlines()
for f in filenames:
    p = Path(f.strip().lstrip('.').lstrip('/'))
    dest = base_dir / p
    if not dest.parent.is_dir():
        print(f'mkdir {dest}')
        p.parent.mkdir(parents=True, exist_ok=True)
    if p.suffix.lower() == '.rst':
        print(p)
        with p.open() as fin:
            with (base_dir / p).open('w') as fout:
                fout.writelines(fin.readlines())
230/9:
filenames = open('corpus_files.txt').readlines()
for f in filenames:
    p = Path(f.strip().lstrip('.').lstrip('/'))
    dest = base_dir / p
    if not dest.parent.is_dir():
        print(f'mkdir {dest.parent}')
        p.parent.mkdir(parents=True, exist_ok=True)
    if p.suffix.lower() == '.rst':
        print(p, dest)
        with p.open() as fin:
            with dest.open('w') as fout:
                fout.writelines(fin.readlines())
230/10: ls -hal /home/hobs/code/tangibleai/community/knowt/data/corpus_runestone_rst/InstructorGuide/_sources/Video/
230/11: ls -hal /home/hobs/code/tangibleai/community/knowt/data/corpus_runestone_rst/InstructorGuide/_sources/
230/12: ls -hal /home/hobs/code/tangibleai/community/knowt/data/corpus_runestone_rst/
230/13: p
230/14:
filenames = open('corpus_files.txt').readlines()
for f in filenames:
    p = Path(f.strip().lstrip('.').lstrip('/'))
    dest = base_dir / p
    if not dest.parent.is_dir():
        print(f'mkdir {dest.parent}')
        dest.mkdir(parents=True, exist_ok=True)
    if p.suffix.lower() == '.rst':
        print(p, dest)
        with p.open() as fin:
            with dest.open('w') as fout:
                fout.writelines(fin.readlines())
230/15: rm /home/hobs/code/tangibleai/community/knowt/data/corpus_runestone_rst/InstructorGuide/_sources/Video/youtube.rst
230/16: rm -r /home/hobs/code/tangibleai/community/knowt/data/corpus_runestone_rst/InstructorGuide/_sources/Video/youtube.rst
230/17:
filenames = open('corpus_files.txt').readlines()
for f in filenames:
    p = Path(f.strip().lstrip('.').lstrip('/'))
    dest = base_dir / p
    if not dest.parent.is_dir():
        print(f'mkdir {dest.parent}')
        dest.mkdir(parents=True, exist_ok=True)
    if p.suffix.lower() == '.rst':
        print(p, dest)
        with p.open() as fin:
            with dest.open('w') as fout:
                fout.writelines(fin.readlines())
230/18:
filenames = open('corpus_files.txt').readlines()
for f in filenames:
    p = Path(f.strip().lstrip('.').lstrip('/'))
    dest = base_dir / p
    if dest.is_dir():
        dest.unlink()
    if not dest.parent.is_dir():
        print(f'mkdir {dest.parent}')
        dest.mkdir(parents=True, exist_ok=True)
    if p.suffix.lower() == '.rst':
        print(p, dest)
        with p.open() as fin:
            with dest.open('w') as fout:
                fout.writelines(fin.readlines())
230/19: dest
230/20: dest.is_dir()
230/21: dest.unlink?
230/22: dest.rmdir()
230/23:
filenames = open('corpus_files.txt').readlines()
for f in filenames:
    p = Path(f.strip().lstrip('.').lstrip('/'))
    dest = base_dir / p
    if dest.is_dir():
        dest.rmdir()
    if not dest.parent.is_dir():
        print(f'mkdir {dest.parent}')
        dest.mkdir(parents=True, exist_ok=True)
    if p.suffix.lower() == '.rst':
        print(p, dest)
        with p.open() as fin:
            with dest.open('w') as fout:
                fout.writelines(fin.readlines())
230/24: find /home/hobs/code/tangibleai/community/knowt/data/corpus_runestone_rst/ -type d -name '*.rst'
230/25: find /home/hobs/code/tangibleai/community/knowt/data/corpus_runestone_rst/ -type=d -name='*.rst'
230/26: !find /home/hobs/code/tangibleai/community/knowt/data/corpus_runestone_rst/ -type d -name '*.rst'
230/27: rm -r /home/hobs/code/tangibleai/community/knowt/data/corpus_runestone_rst/InstructorGuide/_sources/Assessments/shortanswer.rst
230/28:
filenames = open('corpus_files.txt').readlines()
for f in filenames:
    p = Path(f.strip().lstrip('.').lstrip('/'))
    dest = base_dir / p
    if dest.is_dir():
        dest.rmdir()
    if not dest.parent.is_dir():
        print(f'mkdir {dest.parent}')
        dest.mkdir(parents=True, exist_ok=True)
    if p.suffix.lower() == '.rst':
        print(p, dest)
        with p.open() as fin:
            with dest.open('w') as fout:
                fout.writelines(fin.readlines())
230/29: !find /home/hobs/code/tangibleai/community/knowt/data/corpus_runestone_rst/ -type d -name '*.rst'
230/30:
filenames = open('corpus_files.txt').readlines()
for f in filenames:
    p = Path(f.strip().lstrip('.').lstrip('/'))
    dest = base_dir / p
    if dest.is_dir():
        dest.rmdir()
    if not dest.parent.is_dir():
        print(f'mkdir {dest.parent}')
        dest.parent.mkdir(parents=True, exist_ok=True)
    if p.suffix.lower() == '.rst':
        print(p, dest)
        with p.open() as fin:
            with dest.open('w') as fout:
                fout.writelines(fin.readlines())
230/31: pwd
230/32: hist -f ../../community/knowt/scripts/clone_runestone_books_and_copy_rst.py
230/33: hist -o -p -f ../../community/knowt/scripts/clone_runestone_books_and_copy_rst.hist.ipy
232/1: syl = '/home/hobs/code/hobs/mesa/2024/mesa_python.gitlab.io/src/content/blog/syllabus.yaml'
232/2: BLOG_DIR = Path('../mesa_python.gitlab.io/src/content/blog/')
232/3: from mesa_python.constants import *
232/4: who
232/5: BLOG_DIR = Path('../mesa_python.gitlab.io/src/content/blog/')
232/6: yaml
232/7: import yaml
232/8: import pyyaml
233/1: import yaml
233/2: from mesa_python.constants import *
233/3: who
233/4: ASTRO_BLOG_DIR / 'syllabus.yaml'
233/5: (ASTRO_BLOG_DIR / 'syllabus.yaml').open()
233/6: ASTRO_BLOG_DIR.is_dir()
233/7: ASTRO_BLOG_DIR.parent.is_dir()
233/8: ASTRO_BLOG_DIR.parent.parent.is_dir()
233/9: BASE_DIR.is_dir()
233/10:
ASTRO_BASE_DIR = BASE_DIR.parent / 'mesa_python.gitlab.io' / 'src' / 'content'
ASTRO_BLOG_DIR = ASTRO_BASE_DIR / 'blog'
233/11: ASTRO_BLOG_DIR.is_dir()
233/12: (ASTRO_BLOG_DIR / 'syllabus.yaml').open()
233/13: yaml.safe_load((ASTRO_BLOG_DIR / 'syllabus.yaml').open())
233/14: yaml.full_load((ASTRO_BLOG_DIR / 'syllabus.yaml').open())
233/15: yaml.full_load((ASTRO_BLOG_DIR / 'syllabus.yaml').open())
233/16: yaml.full_load((ASTRO_BLOG_DIR / 'syllabus.yaml').open())
233/17: yaml.full_load((ASTRO_BLOG_DIR / 'syllabus.yaml').open())
233/18: yaml.full_load((ASTRO_BLOG_DIR / 'syllabus.yaml').open())
233/19: yaml.full_load((ASTRO_BLOG_DIR / 'syllabus.yaml').open())
233/20: yaml.full_load((ASTRO_BLOG_DIR / 'syllabus.yaml').open())
233/21: syl = yaml.full_load((ASTRO_BLOG_DIR / 'syllabus.yaml').open())
233/22: syl['schedule']
233/23:
for title, wk in syl['schedule'].items():
    print(title, wk)
233/24:
for title, wk in syl['schedule'].items():
    print(title, wk['summary'])
233/25:
for title, wk in syl['schedule'].items():
    print(title.split()[-1] + '.', title, wk['summary'])
233/26:
for title, wk in syl['schedule'].items():
    desc = wk['summary']
    reading = re.findall(r'fopp[^ ]*', desc.lower())
    print(title.split()[-1] + '.', reading, wk['summary'])
233/27: import re
233/28:
for title, wk in syl['schedule'].items():
    desc = wk['summary']
    reading = re.findall(r'fopp[^ ]*', desc.lower())
    print(title.split()[-1] + '.', reading, wk['summary'])
233/29: yaml.full_load((ASTRO_BLOG_DIR / 'syllabus.yaml').open())
233/30: syl = yaml.full_load((ASTRO_BLOG_DIR / 'syllabus.yaml').open())
233/31:
for title, wk in syl['schedule'].items():
    desc = wk['summary']
    reading = re.findall(r'fopp[^ ]*', desc.lower())
    print(title.split()[-1] + '.', reading, wk['summary'])
233/32:
for title, wk in syl['schedule'].items():
    desc = wk['summary']
    reading = re.findall(r'FOPP-\d+', desc.lower()) or [""]
    print(title.split()[-1] + '.', reading[0], wk['summary'])
233/33:
for title, wk in syl['schedule'].items():
    desc = wk['summary']
    skills = wk['skill']
    reading = [(re.findall(r'FOPP-\d+', skill) or [""])[0] for skill in skills]
    print(title.split()[-1] + '.', reading, desc)
233/34:
for title, wk in syl['schedule'].items():
    desc = wk['summary']
    skills = wk['skills']
    reading = [(re.findall(r'FOPP-\d+', skill) or [""])[0] for skill in skills]
    print(title.split()[-1] + '.', reading, desc)
233/35:
for title, wk in syl['schedule'].items():
    desc = wk['summary']
    skills = wk['skills']
    reading = [(re.findall(r'FOPP-\d+', skill) or [""]) for skill in skills]
    print(title.split()[-1] + '.', [r for r in reading if r], desc)
233/36:
for title, wk in syl['schedule'].items():
    desc = wk['summary']
    skills = wk['skills']
    reading = [(re.findall(r'FOPP-\d+', skill) or [""])[0] for skill in skills]
    print(title.split()[-1] + '.', [r for r in reading if r], desc)
233/37:
for title, wk in syl['schedule'].items():
    desc = wk['summary']
    skills = wk['skills']
    readings = [(re.findall(r'FOPP-\d+', skill) or [""])[0] for skill in skills]
    readings = [r.split('-')[-1] for r in readings if r]
    print(title.split()[-1] + '.', readings, desc)
233/38:
for title, wk in syl['schedule'].items():
    desc = wk['summary']
    skills = wk['skills']
    readings = [(re.findall(r'FOPP-\d+', skill) or [""])[0] for skill in skills]
    readings = [r.split('-')[-1] for r in readings if r]
    chapters = 'Ch ' + '-'.join(readings)
    print(title.split()[-1] + '.', chapters, desc)
233/39:
for title, wk in syl['schedule'].items():
    desc = wk['summary']
    skills = wk['skills']
    readings = [(re.findall(r'FOPP-\d+', skill) or [""])[0] for skill in skills]
    readings = [r.split('-')[-1] for r in readings if r]
    readings = 'Runestone Ch ' + '-'.join(readings)
    print(title.split()[-1] + '.', readings, desc)
233/40: hist -o -p -f scripts/syllabus_from_yaml.hist.ipy
233/41: hist -f scripts/syllabus_from_yaml.hist.py
233/42: syl = yaml.full_load((ASTRO_BLOG_DIR / 'syllabus.yaml').open())
233/43:
for title, wk in syl['schedule'].items():
    desc = wk['summary']
    skills = wk['skills']
    readings = [(re.findall(r'FOPP-\d+', skill) or [""])[0] for skill in skills]
    readings = [r.split('-')[-1] for r in readings if r]
    readings = 'Runestone Ch ' + '-'.join(readings)
    print(title.split()[-1] + '.', readings, desc)
233/44: hist -f scripts/syllabus_from_yaml.hist.py
233/45: hist ~0/38-44 -f 'scripts/syllabus_from_yaml.hist.py'
233/46: hist ~0/38-44 -f scripts/syllabus_from_yaml.hist.py
233/47: ls scripts
233/48: more scripts/syllabus_from_yaml.hist.py
233/49:
for title, wk in syl['schedule'].items():
    desc = wk['summary']
    skills = wk['skills']
    readings = [(re.findall(r'FOPP-\d+', skill) or [""])[0] for skill in skills]
    readings = [r.split('-')[-1] for r in readings if r]
    readings = 'Runestone Ch ' + '-'.join(readings)
    print(readings, desc)
233/50: syl['start_date']
233/51: datetime.datetime(syl['start_date'])
233/52: datetime.datetime.fromisoformat(syl['start_date'])
233/53: hist
233/54: start = datetime.datetime.fromisoformat(syl['start_date'])
233/55: start.date
233/56: start.date()
233/57: start.date().isoformat()
233/58: d = start.date()
233/59: d += 7
233/60: d += datetime.timedelta(7)
233/61: d
233/62: d0 = start.date()
233/63: publish_dates = [start.date() + datetime.timedelta(i*7 - 2) for i in range(16)]
233/64: len(publish_dates)
233/65: due_dates = [start.date() + datetime.timedelta(i*7 - 1 + 7) for i in range(16)]
233/66: hist
233/67:
import re
import yaml
import datetime

import pandas as pd

from mesa_python.constants import *

syl = yaml.full_load((ASTRO_BLOG_DIR / 'syllabus.yaml').open())
start = datetime.datetime.fromisoformat(syl['start_date'])
assignments = []

for i, (title, wk) in enumerate(syl['schedule'].items()):
    desc = wk['summary']
    skills = wk['skills']
    readings = [(re.findall(r'FOPP-\d+', skill) or [""])[0] for skill in skills]
    readings = [r.split('-')[-1] for r in readings if r]
    readings_md = 'Runestone Ch ' + '-'.join(readings)
    assignments.append(dict(
        publish=start.date() + datetime.timedelta(i * 7 - 2),
        due=start.date() + datetime.timedelta(i * 7 - 1 + 7),
        desc=desc,
        skills=skills,
        readings=readings,
        readings_md=readings_md,
        md=f"1. **Week {title.split()[-1]}**: {readings_md} - {desc}",
    ))
print(pd.DataFrame(assignments))
234/1:
import re
import yaml
import datetime

import pandas as pd

from mesa_python.constants import *

syl = yaml.full_load((ASTRO_BLOG_DIR / 'syllabus.yaml').open())
start = datetime.datetime.fromisoformat(syl['start_date'])
assignments = []

for i, (title, wk) in enumerate(syl['schedule'].items()):
    desc = wk['summary']
    skills = wk['skills']
    readings = [(re.findall(r'FOPP-\d+', skill) or [""])[0] for skill in skills]
    readings = [r.split('-')[-1] for r in readings if r]
    readings_md = 'Runestone Ch ' + '-'.join(readings)
    assignments.append(dict(
        publish=start.date() + datetime.timedelta(i * 7 - 2),
        due=start.date() + datetime.timedelta(i * 7 - 1 + 7),
        desc=desc,
        skills=skills,
        readings=readings,
        readings_md=readings_md,
        md=f"1. **Week {title.split()[-1]}**: {readings_md} - {desc}",
    ))
print(pd.DataFrame(assignments))
234/2:
import re
import yaml
import datetime

import pandas as pd

from mesa_python.constants import *

syl = yaml.full_load((ASTRO_BLOG_DIR / 'syllabus.yaml').open())
start = datetime.datetime.fromisoformat(syl['start_date'])
assignments = []

for i, (title, wk) in enumerate(syl['schedule'].items()):
    desc = wk['summary']
    skills = wk['skills']
    readings = [(re.findall(r'FOPP-\d+', skill) or [""])[0] for skill in skills]
    readings = [r.split('-')[-1] for r in readings if r]
    readings_md = 'Runestone Ch ' + '-'.join(readings)
    assignments.append(dict(
        publish=start.date() + datetime.timedelta(i * 7 - 2),
        due=start.date() + datetime.timedelta(i * 7 - 1 + 7),
        desc=desc,
        skills=skills,
        readings=readings,
        readings_md=readings_md,
        md=f"1. **Week {title.split()[-1]}**: {readings_md} - {desc}",
    ))
print(pd.DataFrame(assignments))
234/3:
import re
import yaml
import datetime

import pandas as pd

from mesa_python.constants import *

syl = yaml.full_load((ASTRO_BLOG_DIR / 'syllabus.yaml').open())
start = datetime.datetime.fromisoformat(syl['start_date'])
assignments = []

for i, (title, wk) in enumerate(syl['schedule'].items()):
    desc = wk['summary']
    skills = wk['skills']
    readings = [(re.findall(r'FOPP-\d+', skill) or [""])[0] for skill in skills]
    readings = [r.split('-')[-1] for r in readings if r]
    readings_md = 'Runestone Ch ' + '-'.join(readings)
    assignments.append(dict(
        publish=start.date() + datetime.timedelta(i * 7 - 2),
        due=start.date() + datetime.timedelta(i * 7 - 1 + 7),
        desc=desc,
        skills=skills,
        readings=readings,
        readings_md=readings_md,
        md=f"1. **Week {title.split()[-1]}**: {readings_md} - {desc}",
    ))
df = pd.DataFrame(assignments)
234/4: df.to_html()
234/5: df
234/6:
for i, (title, wk) in enumerate(syl['schedule'].items()):
    desc = wk['summary']
    skills = wk['skills']
    readings = [(re.findall(r'FOPP-\d+', skill) or [""])[0] for skill in skills]
    readings = [r.split('-')[-1] for r in readings if r]
    readings_md = 'Runestone Ch ' + '-'.join(readings)
    assignments.append(dict(
        publish=start.date() + datetime.timedelta(i * 7 - 2),
        due=start.date() + datetime.timedelta(i * 7 - 1 + 7),
        desc=desc,
        skills=skills,
        runestone=readings,
        readings_md='Ch' + '-'.join(readings),
        md=f"1. **Week {title.split()[-1]}**: {readings_md} - {desc}",
    ))
df = pd.DataFrame(assignments)
234/7: df
234/8:
assignments = []
for i, (title, wk) in enumerate(syl['schedule'].items()):
    desc = wk['summary']
    skills = wk['skills']
    readings = [(re.findall(r'FOPP-\d+', skill) or [""])[0] for skill in skills]
    readings = [r.split('-')[-1] for r in readings if r]
    readings_md = 'Runestone Ch ' + '-'.join(readings)
    assignments.append(dict(
        publish=start.date() + datetime.timedelta(i * 7 - 2),
        due=start.date() + datetime.timedelta(i * 7 - 1 + 7),
        desc=desc,
        skills=skills,
        runestone=readings,
        readings_md='Ch' + '-'.join(readings),
        md=f"1. **Week {title.split()[-1]}**: {readings_md} - {desc}",
    ))
df = pd.DataFrame(assignments)
234/9: df
234/10: df['due', 'runestone', 'readings_md']
234/11: df[['due', 'runestone', 'readings_md']]
234/12:
assignments = []
for i, (title, wk) in enumerate(syl['schedule'].items()):
    desc = wk['summary']
    skills = wk['skills']
    readings = [(re.findall(r'FOPP-\d+', skill) or [""])[0] for skill in skills]
    readings = [r.split('-')[-1] for r in readings if r]
    readings_md = 'Runestone Ch ' + '-'.join(readings)
    assignments.append(dict(
        publish=start.date() + datetime.timedelta(i * 7 - 2),
        due=start.date() + datetime.timedelta(i * 7 - 1 + 7),
        desc=desc,
        skills=skills,
        runestone=readings,
        readings_md='Ch ' + '-'.join(readings),
        md=f"1. **Week {title.split()[-1]}**: {readings_md} - {desc}",
    ))
df = pd.DataFrame(assignments)
print(df)
print(df[['due', 'runestone', 'readings_md']])
234/13:
assignments = []
for i, (title, wk) in enumerate(syl['schedule'].items()):
    desc = wk['summary']
    skills = wk['skills']
    readings = [(re.findall(r'FOPP-\d+', skill) or [""])[0] for skill in skills]
    readings = [r.split('-')[-1] for r in readings if r]
    readings_md = 'Runestone Ch ' + '-'.join(readings)
    assignments.append(dict(
        publish=start.date() + datetime.timedelta(i * 7 - 2),
        due=start.date() + datetime.timedelta(i * 7 - 1 + 7),
        desc=desc,
        skills=skills,
        runestone=readings,
        readings_md='Ch ' + '-'.join(readings),
        md=f"1. **Week {title.split()[-1]}**: {readings_md} - {desc}",
    ))
df = pd.DataFrame(assignments)
print(df)
due = df[['due', 'runestone', 'readings_md']]
print(due.to_markdown())
print(due.to_html())
235/1: %run scripts/syllabus_from_yaml.hist.py
235/2: %run scripts/syllabus_from_yaml.hist.py
235/3: %run scripts/syllabus_from_yaml.hist.py
235/4: %run scripts/syllabus_from_yaml.hist.py
235/5: %run scripts/syllabus_from_yaml.hist.py
235/6: %run scripts/syllabus_from_yaml.hist.py
235/7: %run scripts/syllabus_from_yaml.hist.py
235/8: %run scripts/syllabus_from_yaml.hist.py
235/9: due.columns
235/10: %run scripts/syllabus_from_yaml.py
235/11: %run scripts/syllabus_from_yaml.py
235/12: %run scripts/syllabus_from_yaml.hist.py
235/13: %run scripts/syllabus_from_yaml.py
235/14: %run scripts/syllabus_from_yaml.py
235/15: cp scripts/syllabus_from_yaml.py scripts/syllabus_from_yaml.hist.py
235/16: %run scripts/syllabus_from_yaml.py
235/17: %run scripts/syllabus_from_yaml.py
235/18: %run scripts/syllabus_from_yaml.py
235/19: %run scripts/syllabus_from_yaml.py
235/20: cp scripts/syllabus_from_yaml.py scripts/syllabus_from_yaml.hist.py
236/1: ls src/mesa_python/
236/2: from mesa_python.df_join_canvas_rune import *
237/1: from mesa.df_join_canvas_rune import *
238/1: from mesa.df_join_canvas_rune import *
239/1: import doctest
240/1: import ipdb
240/2: import ipydb
240/3: import ipdbg
241/1: %run src/nlpia2/text_processing/extractors.py
241/2: OFFICIAL_MANUSCRIPT_DIR
241/3: OFFICIAL_ADOC_DIR
242/1: %run nlpia2.text_processing.converters.adocs2notebooks()
242/2: import nlpia2.text_processing.converters.adocs2notebooks
242/3: from nlpia2.text_processing.converters import adocs2notebooks
242/4: adocs2notebooks?
242/5: adocs2notebooks('manuscript/adoc/')
242/6: who
242/7: results = __
242/8: results
242/9: results = __
242/10: results
242/11: !git status
241/4: %run scripts/syllabus_from_yaml.py
243/1: %run nlpia2.text_processing.converters.adocs2notebooks()
243/2: %run scripts/syllabus_from_yaml.py
243/3: %run mesa.syllabus_from_yaml
243/4: %run src/mesa/syllabus_from_yaml
243/5: %run src/mesa/syllabus_from_yaml
243/6: %run src/mesa/syllabus_from_yaml
243/7: mv src/mesa/syllabus_from_yaml.py src/mesa/schedule_from_yaml.py
243/8: %run src/mesa/schedule_from_yaml
243/9: %run src/mesa/schedule_from_yaml
243/10: %run src/mesa/schedule_from_yaml
243/11: %run src/mesa/schedule_from_yaml
245/1: from api.llm import *
245/2: cd api
245/3: import llm
245/4: ls ..
245/5: llm.generate_caption('../ogimage.png')
245/6: more llm.py
246/1: import pandas as pd
247/1: from nlpia2 import alttext
248/1: from nlpia2 import alttext
248/2: from nlpia2.constants import *
248/3: OFFICIAL_MANUSCRIPT_DIR
248/4: OFFICIAL_RELATIVE_MANUSCRIPT_DIR
248/5: ls ../
248/6: ls ../..
248/7: ls ../../hobs/nlpia-manuscript/image_log/
248/8: import pandas as pd
248/9: p = Path('../../hobs/nlpia-manuscript/image_log/')
248/10: p.glob('**/*.jpg')
248/11: p = Path('../../hobs/nlpia-manuscript/image_log/image_log.csv')
248/12: pd.read_csv(p)
248/13: df = _
248/14: df.link
248/15: df.link.apply(lambda x: Path('../../hobs/nlpia-manuscript/') / Path(x))
248/16: df.link.apply(lambda x: Path('../../hobs/nlpia-manuscript/') / str(x))
248/17: df.path = df.link.apply(lambda x: Path('../../hobs/nlpia-manuscript/') / str(x))
248/18: df['path'] = df.link.apply(lambda x: Path('../../hobs/nlpia-manuscript/') / str(x))
248/19: df.path
248/20: df.path.values
248/21:
alts = []
for p in df.path.values:
    print(p)
    alts.append(alttext.generate_caption(p)[0].values()[0])
    print(alts[-1])
248/22: df['path'][0]
248/23: df['path'][0].is_file()
248/24: df['path'][0].parent.parent.parent.is_dir()
248/25: df['path'][0].parent.parent.is_dir()
248/26: p
248/27: df['path'] = df.link.apply(lambda x: Path('../../hobs/nlpia-manuscript/manuscript') / str(x).lstrip('.').lstrip('/'))
248/28: df.path.values
248/29:
alts = []
for p in df.path.values:
    print(p)
    alts.append(alttext.generate_caption(p)[0].values()[0])
    print(alts[-1])
248/30: df['path'][0].is_file()
248/31: alttext.generate_caption(df['path'][0])
248/32: df['path'][0]
248/33: alttext.generate_caption('../../hobs/nlpia-manuscript/manuscript/images/ch03/cosine_distance_justification.png')
248/34: alttext.generate_caption(str(df['path'][0]))
248/35:
alts = []
for p in df.path.values:
    print(p)
    alts.append(alttext.generate_caption(str(p))[0].values()[0])
    print(alts[-1])
248/36:
alts = []
for p in df.path.values:
    print(p)
    alts.append(alttext.generate_caption(str(p))[0])
    print(alts[-1])
248/37:
for i, p in enumerate(df.path.values):
    if i < len(alts):
        continue
    print(p)
    try:
        alts.append(alttext.generate_caption(str(p))[0])
    except Exception as e:
        alts.append(e)
    print(alts[-1])
248/38:
for i, p in enumerate(df.path.values):
    if isinstance(alts[i], dict):
        continue
    import base64

    binary_fc = open(p, 'rb').read()  # fc aka file_content
    base64_utf8_str = base64.b64encode(binary_fc).decode('utf-8')
    outpath = p.with_suffix('.base64' + p.suffix)
    outpath.open('wb').write(base64_utf8_str)
    dataurl = f'data:image/{p.suffix.strip('.')};base64,{base64_utf8_str}'

    print(p)
    try:
        alts.append(alttext.generate_caption(dataurl)[0])
    except Exception as e:
        alts.append(e)
    print(alts[-1])
248/39:
for i, p in enumerate(df.path.values):
    if isinstance(alts[i], dict):
        continue
    import base64

    binary_fc = open(p, 'rb').read()  # fc aka file_content
    base64_utf8_str = base64.b64encode(binary_fc).decode('utf-8')
    outpath = p.with_suffix('.base64' + p.suffix)
    outpath.open('wb').write(base64_utf8_str)
    dataurl = f'data:image/{p.suffix.strip(".")};base64,{base64_utf8_str}'

    print(p)
    try:
        alts.append(alttext.generate_caption(dataurl)[0])
    except Exception as e:
        alts.append(e)
    print(alts[-1])
248/40:
# import base64
for i, p in enumerate(df.path.values):
    if isinstance(alts[i], dict):
        continue
    

    # binary_fc = open(p, 'rb').read()  # fc aka file_content
    # base64_utf8_str = base64.b64encode(binary_fc).decode('utf-8')
    # outpath = p.with_suffix('.base64' + p.suffix)
    # outpath.open('wb').write(base64_utf8_str)
    # dataurl = f'data:image/{p.suffix.strip(".")};base64,{base64_utf8_str}'
    for let in 'abcdefg':
        if p.parent.name.startswith('app') and p.parent.name.endswith(f'_{let}'):
            p = p.parent.parent / f'appendix-{let}' / p.name
    print(p)
    try:
        alts.append(alttext.generate_caption(dataurl)[0])
    except Exception as e:
        alts.append(e)
    print(alts[-1])
248/41:
# import base64
for i, p in enumerate(df.path.values):
    if isinstance(alts[i], dict):
        continue
    

    # binary_fc = open(p, 'rb').read()  # fc aka file_content
    # base64_utf8_str = base64.b64encode(binary_fc).decode('utf-8')
    # outpath = p.with_suffix('.base64' + p.suffix)
    # outpath.open('wb').write(base64_utf8_str)
    # dataurl = f'data:image/{p.suffix.strip(".")};base64,{base64_utf8_str}'
    for let in 'abcdefg':
        if p.parent.name.startswith('app') and p.parent.name.endswith(f'_{let}'):
            p = p.parent.parent / f'appendix-{let}' / p.name
    print(p)
    try:
        alts.append(alttext.generate_caption(p)[0])
    except Exception as e:
        alts.append(e)
    print(alts[-1])
248/42: ls ../../hobs/nlpia-manuscript/manuscript/images/appendix-d/overfit.png
248/43: ls ../../hobs/nlpia-manuscript/manuscript/images/appendix-d/
248/44: ls ../../hobs/nlpia-manuscript/manuscript/images/appendix-d/overfit.png
248/45:
# import base64
for i, p in enumerate(df.path.values):
    if isinstance(alts[i], dict):
        continue
    

    # binary_fc = open(p, 'rb').read()  # fc aka file_content
    # base64_utf8_str = base64.b64encode(binary_fc).decode('utf-8')
    # outpath = p.with_suffix('.base64' + p.suffix)
    # outpath.open('wb').write(base64_utf8_str)
    # dataurl = f'data:image/{p.suffix.strip(".")};base64,{base64_utf8_str}'
    for let in 'abcdefg':
        if p.parent.name.startswith('app') and p.parent.name.endswith(f'_{let}'):
            p = p.parent.parent / f'appendix-{let}' / p.name
    print(p)
    try:
        alts.append(alttext.generate_caption(str(p))[0])
    except Exception as e:
        alts.append(e)
    print(alts[-1])
248/46:
# import base64
for i, p in enumerate(df.path.values):
    if isinstance(alts[i], dict):
        continue
    

    # binary_fc = open(p, 'rb').read()  # fc aka file_content
    # base64_utf8_str = base64.b64encode(binary_fc).decode('utf-8')
    # outpath = p.with_suffix('.base64' + p.suffix)
    # outpath.open('wb').write(base64_utf8_str)
    # dataurl = f'data:image/{p.suffix.strip(".")};base64,{base64_utf8_str}'
    for let in 'abcdefg':
        if p.parent.name.startswith('app') and p.parent.name.endswith(f'_{let}'):
            p = p.parent.parent / f'appendix-{let}' / p.name
    print(p)
    try:
        alts[i] = alttext.generate_caption(str(p))[0]
    except Exception as e:
        alts[i] = e
    print(alts[-1])
248/47:
# import base64
for i, p in enumerate(df.path.values):
    if isinstance(alts[i], dict):
        continue
    

    # binary_fc = open(p, 'rb').read()  # fc aka file_content
    # base64_utf8_str = base64.b64encode(binary_fc).decode('utf-8')
    # outpath = p.with_suffix('.base64' + p.suffix)
    # outpath.open('wb').write(base64_utf8_str)
    # dataurl = f'data:image/{p.suffix.strip(".")};base64,{base64_utf8_str}'
    for let in 'abcdefg':
        if p.parent.name.startswith('app') and p.parent.name.endswith(f'_{let}'):
            p = p.parent.parent / f'appendix-{let}' / p.name
    print(i, p)
    try:
        alts[i] = alttext.generate_caption(str(p))[0]
    except Exception as e:
        alts[i] = e
    print(alts[i])
248/48: len(alts)
248/49: len(df)
248/50: alts[121:]
248/51: pd.DataFrame(alts)
248/52: pd.DataFrame(alts[:121])
248/53: pd.DataFrame([(str(a) if isinstance(a, Exception) else list(a.values())[0]) for a in alts]).to_csv('alts.csv')
248/54: dfalt = pd.DataFrame([(str(a) if isinstance(a, Exception) else list(a.values())[0]) for a in alts])
248/55: dfalt
248/56: il = Path('../../hobs/nlpia-manuscript/image_log/')
248/57: il.is_dir()
248/58: il.list()
248/59: il.lstat
248/60: il.lstat()
248/61: dfalt.to_csv(il / 'image_log_alttext.csv')
249/1: hist ~1/0-
249/2: hist ~1/
249/3: pwd
249/4: hist ~1/ -f scripts/generate_alttext.hist.py
250/1:
from nlpia2 import alttext
from nlpia2.constants import *
250/2:
IMAGE_DIR = Path('/home/hobs/code/hobs/nlpia-manuscript/manuscript/images')

alts = []
for glob in ['**/*.png', '**/*.jpg']:
    for i, p in enumerate(IMAGE_DIR.glob(glob)):
        if p.is_file():
            try:
                alts.append(list(alttext.generate_caption(str(p))[0].values())[0])
                print('SUCCESS!!!!', alts[-1])
            except Exception as e:
                print(p)
                alts.append(str(p))
            print(alts[-1])
250/3:
IMAGE_DIR = Path('/home/hobs/code/hobs/nlpia-manuscript/manuscript/images')

alts = []
for glob in ['**/*.png', '**/*.jpg']:
    for i, p in enumerate(IMAGE_DIR.glob(glob)):
        if p.is_file():
            try:
                alts.append(list(alttext.generate_caption(str(p))[0].values())[0])
                print('SUCCESS!!!!', alts[-1])
            except Exception as e:
                print(p)
                alts.append(str(p))
            print(alts[-1])
250/4:
IMAGE_DIR = Path('/home/hobs/code/hobs/nlpia-manuscript/manuscript/images')

alts = []
for glob in ['**/*.png', '**/*.jpg']:
    for i, p in enumerate(IMAGE_DIR.glob(glob)):
        if p.is_file():
            print(p)
            try:
                alts.append(list(alttext.generate_caption(str(p))[0].values())[0])
                print('SUCCESS!!!!', alts[-1])
            except Exception as e:
                print(e)
                alts.append(str(p))
        else:
            print('NOT A FILE!!!!!!!!!!!!!', p)
251/1:
from sys import argv
import requests
import json

def run_rust_code(code: str) -> None:
    URL = "https://play.rust-lang.org/execute"
    payload = {
        "channel": "stable",
        "mode": "debug",
        "edition": "2021",
        "crateType": "bin",
        "tests": False,
        "code": code,
        "backtrace": False
    }
    data = json.dumps(payload)
    headers = {"Content-type": "application/json"}
    response = requests.post(url=URL, data=data, headers=headers)
    print(response.json()["stdout"])

def main():
    help_msg = """
Rust Playground Locally allows you to play around rust code without the need to create a rust file and compile it.
At its core, this program submits your Rust code to the Rust playground API (https://play.rust-lang.org/) which executes your
Rust code and prints out the stdout or stderr (if any).

To run it, just enter this command:
`python3 -m rust_playground`

(Note: Running a python file may differ depending on the OS you use, so run it which ever's the right way for your OS)
"""
    if len(argv) > 1 and (argv[1] == "-h" or argv[1] == "--help"):
        print(help_msg)
        return

    welcome_prompt = '''Welcome to Rust Playground Locally!
Your Rust Code will be sent to the Rust Playground (https://play.rust-lang.org/) API so please make sure you have an internet connection.'''
    print(welcome_prompt)

    lines = []
    while True:
        raw_input = input('>>> ')
        if not raw_input:
            break
        lines.append(raw_input + '\n')
    raw_code_input = ''.join(lines).strip()

    print('\nRunning your code, please wait...\n')

    run_rust_code(raw_code_input)

if __name__ == '__main__':
    main()
251/2:
from sys import argv
import requests
import json

def run_rust_code(code: str) -> None:
    URL = "https://play.rust-lang.org/execute"
    payload = {
        "channel": "stable",
        "mode": "debug",
        "edition": "2021",
        "crateType": "bin",
        "tests": False,
        "code": code,
        "backtrace": False
    }
    data = json.dumps(payload)
    headers = {"Content-type": "application/json"}
    response = requests.post(url=URL, data=data, headers=headers)
    print(response.json()["stdout"])

def main():
    help_msg = """
Rust Playground Locally allows you to play around rust code without the need to create a rust file and compile it.
At its core, this program submits your Rust code to the Rust playground API (https://play.rust-lang.org/) which executes your
Rust code and prints out the stdout or stderr (if any).

To run it, just enter this command:
`python3 -m rust_playground`

(Note: Running a python file may differ depending on the OS you use, so run it which ever's the right way for your OS)
"""
    if len(argv) > 1 and (argv[1] == "-h" or argv[1] == "--help"):
        print(help_msg)
        return

    welcome_prompt = '''Welcome to Rust Playground Locally!
Your Rust Code will be sent to the Rust Playground (https://play.rust-lang.org/) API so please make sure you have an internet connection.'''
    print(welcome_prompt)

    lines = []
    while True:
        raw_input = input('>>> ')
        if not raw_input:
            break
        lines.append(raw_input + '\n')
    raw_code_input = ''.join(lines).strip()

    print('\nRunning your code, please wait...\n')

    raw_output = run_rust_code(raw_code_input)
    print(raw_output)
    return raw_output
251/3: main()
251/4: main()
251/5:
from sys import argv
import requests
import json

def run_rust_code(code: str) -> None:
    URL = "https://play.rust-lang.org/execute"
    payload = {
        "channel": "stable",
        "mode": "debug",
        "edition": "2021",
        "crateType": "bin",
        "tests": False,
        "code": code,
        "backtrace": False
    }
    data = json.dumps(payload)
    headers = {"Content-type": "application/json"}
    response = requests.post(url=URL, data=data, headers=headers)
    print(response)
    print(response.json()["stdout"])
    return(respones.json())

def main():
    help_msg = """
Rust Playground Locally allows you to play around rust code without the need to create a rust file and compile it.
At its core, this program submits your Rust code to the Rust playground API (https://play.rust-lang.org/) which executes your
Rust code and prints out the stdout or stderr (if any).

To run it, just enter this command:
`python3 -m rust_playground`

(Note: Running a python file may differ depending on the OS you use, so run it which ever's the right way for your OS)
"""
    if len(argv) > 1 and (argv[1] == "-h" or argv[1] == "--help"):
        print(help_msg)
        return

    welcome_prompt = '''Welcome to Rust Playground Locally!
Your Rust Code will be sent to the Rust Playground (https://play.rust-lang.org/) API so please make sure you have an internet connection.'''
    print(welcome_prompt)

    lines = []
    while True:
        raw_input = input('>>> ')
        if not raw_input:
            break
        lines.append(raw_input + '\n')
    raw_code_input = ''.join(lines).strip()

    print('\nRunning your code, please wait...\n')

    raw_output = run_rust_code(raw_code_input)
    print(raw_output)
    return raw_output
251/6:
    URL = "https://play.rust-lang.org/execute"
    payload = {
        "channel": "stable",
        "mode": "debug",
        "edition": "2021",
        "crateType": "bin",
        "tests": False,
        "code": code,
        "backtrace": False
    }
    data = json.dumps(payload)
    headers = {"Content-type": "application/json"}
    response = requests.post(url=URL, data=data, headers=headers)
251/7: code = 'println!("Hello Vlad and Cetin");'
251/8:
    URL = "https://play.rust-lang.org/execute"
    payload = {
        "channel": "stable",
        "mode": "debug",
        "edition": "2021",
        "crateType": "bin",
        "tests": False,
        "code": code,
        "backtrace": False
    }
    data = json.dumps(payload)
    headers = {"Content-type": "application/json"}
    response = requests.post(url=URL, data=data, headers=headers)
251/9: response
251/10: response.text
251/11: print(response.text)
251/12: main()
251/13:
from sys import argv
import requests
import json

def run_rust_code(code: str) -> None:
    URL = "https://play.rust-lang.org/execute"
    payload = {
        "channel": "stable",
        "mode": "debug",
        "edition": "2021",
        "crateType": "bin",
        "tests": False,
        "code": code,
        "backtrace": False
    }
    data = json.dumps(payload)
    headers = {"Content-type": "application/json"}
    response = requests.post(url=URL, data=data, headers=headers)
    print(response)
    print(response.json()["stdout"])
    return(respone.json())

def main():
    help_msg = """
Rust Playground Locally allows you to play around rust code without the need to create a rust file and compile it.
At its core, this program submits your Rust code to the Rust playground API (https://play.rust-lang.org/) which executes your
Rust code and prints out the stdout or stderr (if any).

To run it, just enter this command:
`python3 -m rust_playground`

(Note: Running a python file may differ depending on the OS you use, so run it which ever's the right way for your OS)
"""
    if len(argv) > 1 and (argv[1] == "-h" or argv[1] == "--help"):
        print(help_msg)
        return

    welcome_prompt = '''Welcome to Rust Playground Locally!
Your Rust Code will be sent to the Rust Playground (https://play.rust-lang.org/) API so please make sure you have an internet connection.'''
    print(welcome_prompt)

    lines = []
    while True:
        raw_input = input('>>> ')
        if not raw_input:
            break
        lines.append(raw_input + '\n')
    raw_code_input = ''.join(lines).strip()

    print('\nRunning your code, please wait...\n')

    raw_output = run_rust_code(raw_code_input)
    print(raw_output)
    return raw_output
251/14: main()
250/5:
IMAGE_DIR = Path('/home/hobs/code/hobs/nlpia-manuscript/manuscript/images')

altsf = []
for glob in ['**/*.png', '**/*.jpg']:
    for i, p in enumerate(IMAGE_DIR.glob(glob)):
        if p.is_file():
            print(p)
            altsf.append(str(p))
        else:
            print('NOT A FILE!!!!!!!!!!!!!', p)
250/6: list(zip(alts, altsf))
250/7: df = pd.DataFrame(list(zip(alts, altsf)), columns='caption path'.split())
250/8:
len(alts
)
250/9: len(altsf)
250/10: df
250/11: df.to_csv(IMAGE_DIR / 'image_log_blip_captions.csv')
250/12: list(IMAGE_DIR.glob('**/.jpeg'))
250/13: list(IMAGE_DIR.glob('**/.PNG'))
250/14: list(IMAGE_DIR.glob('**/.bmp'))
250/15: list(IMAGE_DIR.glob('**/.BMP'))
250/16: list(IMAGE_DIR.glob('**/.csv'))
250/17: list(IMAGE_DIR.glob('**/*.csv'))
250/18: list(IMAGE_DIR.glob('**/*.jpeg'))
250/19: allfiles = list(IMAGE_DIR.glob('**/*'))
250/20: df
250/21:
IMAGE_DIR = Path('/home/hobs/code/hobs/nlpia-manuscript/manuscript/images')

more = []
for glob in ['**/*.jpeg', '**/*.JPG', '**/*.JPEG', '*.PNG', '*.BMP', '*.bmp', '*.svg']:
    for i, p in enumerate(IMAGE_DIR.glob(glob)):
        print(p)
        d = dict(caption='', path=str(p))
        try:
            d['caption'] = list(alttext.generate_caption(str(p))[0].values())[0]
            print('SUCCESS!!!!', d)
        except Exception as e:
            print(e)
        more.append(d)
250/22: df.to_records()[:5]
250/23: df.to_records?
250/24: df.to_records(index=False)[:5]
250/25: type(df.to_records(index=False)[0])
250/26: numpy.record?
250/27: dir(df.to_records(index=False)[0])
250/28: r = (df.to_records(index=False)[0])
250/29: r.all()
250/30: r.item()
250/31: r = df.to_records(index=False)[0]
250/32: type(r)
250/33: r.items()
250/34: df.to_dict?
250/35: df.to_dict(orient='records')[:5]
250/36: old = df.to_dict(orient='records')[:5]
250/37: dfnew = pd.DataFrame(old+more)
250/38: dfnew
250/39: len(dfnew)
250/40: old = df.to_dict(orient='records')
250/41: dfnew = pd.DataFrame(old+more)
250/42: len(dfnew)
250/43: len(df)
250/44:
imagefiles = []
for a in allfiles:
    if Path(a).suffix.lower().strip()[1:] in 'png jpg jpeg svg gif'.split():
        imagefiles.append(str(a))
250/45: len(imagefiles)
250/46: done = set(df['path'])
250/47:
imagefiles, tbd = [], []
for a in allfiles:
    if Path(a).suffix.lower().strip()[1:] in 'png jpg jpeg svg gif'.split():
        imagefiles.append(str(a))
    if str(a) not in done:
        tbd.append(str(a))
250/48: tbd
250/49: tbd
250/50: set([Path(x).suffix[1:] for x in tbd])
250/51: set([Path(x).suffix[1:] for x in allfiles])
250/52: p.suffix?
250/53: pwd
250/54:
import yaml
image_links = yaml.safe_load(open('image_log.yaml'))
250/55: ls
250/56: OFFICIAL_RELATIVE_MANUSCRIPT_DIR
250/57: BASE_DIR
250/58:
OFFICIAL_MANUSCRIPT_DIR = BASE_DIR
OFFICIAL_RELATIVE_MANUSCRIPT_DIR = 'nlpia-manuscript/manuscript'
for i in range(4):
    OFFICIAL_MANUSCRIPT_DIR = OFFICIAL_MANUSCRIPT_DIR.parent
    if (OFFICIAL_MANUSCRIPT_DIR / OFFICIAL_RELATIVE_MANUSCRIPT_DIR / 'adoc').is_dir():
        break
OFFICIAL_MANUSCRIPT_DIR = (OFFICIAL_MANUSCRIPT_DIR / OFFICIAL_RELATIVE_MANUSCRIPT_DIR).resolve()
OFFICIAL_ADOC_DIR = (OFFICIAL_MANUSCRIPT_DIR / 'adoc').resolve()
250/59: OFFICIAL_MANUSCRIPT_DIR
250/60: hist
250/61: df
250/62: new
250/63: more
250/64: old
250/65: pd.DataFrame(old+more)
250/66: df2 = _
250/67: df2.to_csv(IMAGE_DIR / 'image_log_blip_captions_1more.csv')
253/1: hist
253/2: hist ~1
253/3: hist ~1/
253/4: hist ~2/
253/5: hist ~3/
253/6: p = Path()
253/7: from pathlib import Path
253/8: p = Path.home()
253/9: p
253/10: Path('x.1.2.3').suffixes
253/11: '.'.join(Path('x.1.2.3').suffixes)
253/12: ''.join(Path('x.1.2.3').suffixes)
253/13: p
253/14: pwd
253/15: ls -hal
253/16: !find . -iname '*.jpg'
253/17: !find . -iname '*.jpeg'
253/18: !find . -iname '*.png'
253/19: !find . -ipath 'ch10/*.png'
253/20: !find . -ipath '**/ch10/*.png'
253/21: !find . -ipath '**/ch09/*.png'
253/22: !find . -ipath '**/ch10/*.jpg'
253/23: !find . -ipath '**/ch10/*.jpeg'
253/24: !find . -ipath '**/ch10/*.svg'
253/25: !find . -ipath '**/ch10/*'
253/26: !find . -ipath '**/images/ch10/*'
253/27: !find . -ipath '**/images/ch-10/*'
253/28: OFFICIAL_MANUSCRIPT_DIR
253/29: IMAGE_DIR = Path('/home/hobs/code/hobs/nlpia-manuscript/manuscript/images')
253/30: !find $IMAGE_DIR -ipath '**/images/*10/*'
253/31: image = '/home/hobs/code/hobs/nlpia-manuscript/manuscript/images/ch10/llm_survey.png'
253/32: image = Path(image)
253/33: image.stat()
253/34: image.stat()['st_size']
253/35: image.stat().st_size
253/36: ls $IMAGE_DIR.parent.parent
253/37: ls $IMAGE_DIR.parent
253/38: cp -r $IMAGE_DIR.parent /home/hobs/.nlpia2-data/
253/39: cp -r ${IMAGE_DIR.parent}/adoc/* /home/hobs/code/tangibleai/nlpia2/src/nlpia2/data/manuscript/adoc/
253/40: ls ${IMAGE_DIR.parent}/adoc/
253/41: ls $(IMAGE_DIR.parent)/adoc/
253/42: ls $(IMAGE_DIR.parent / 'adoc')
253/43: ls $(IMAGE_DIR.parent / 'adoc')/
253/44: ls $(IMAGE_DIR.parent / 'adoc' / '*')
253/45: ls $(IMAGE_DIR.parent / 'adoc')
253/46: ls ${IMAGE_DIR.parent / 'adoc'}
254/1: from nlpia2.constants import *
254/2: OFFICIAL_MANUSCRIPT_DIR
256/1: from mesa.process_runestone_autogrades import *
256/2: who
257/1: from mesa.process_runestone_autogrades import *
257/2: who
257/3: dfs = process_all_autograde_files()
257/4: %run src/mesa/process_runestone_autogrades
257/5: dfs = process_all_autograde_files()
257/6: %run src/mesa/process_runestone_autogrades
257/7: dfs = process_all_autograde_files()
257/8: pd.read_csv?
257/9: %run src/mesa/process_runestone_autogrades
257/10: %run src/mesa/process_runestone_autogrades
257/11: dfs = process_all_autograde_files()
257/12: dfs
257/13: len(dfs)
257/14: %run src/mesa/process_runestone_autogrades
257/15: %run src/mesa/schedule_from_yaml
257/16: who
257/17: schedule_html
257/18: print(schedule_html)
258/1: import datetime
258/2: datetime.datetime.fromisoformat?
259/1:
import spacy
spacy.cli.download("en_core_web_sm")
nlp = spacy.load("en_core_web_sm")
sentence = ('It has also arisen in criminal justice, healthcare, and '
    'hiring, compounding existing racial, economic, and gender biases.')
doc = nlp(sentence)
tokens = [token.text for token in doc]
tokens


# #### 

# In[2]:


from collections import Counter
bag_of_words = Counter(tokens)
bag_of_words


# #### 

# In[3]:


bag_of_words.most_common(3)  # <1>


# #### 

# In[4]:


import pandas as pd
most_common = dict(bag_of_words.most_common())  # <1>
counts = pd.Series(most_common)  # <2>
counts


# #### 

# In[5]:


len(counts)  # <1>


# #### 

# In[6]:


counts.sum()


# #### 

# In[7]:


len(tokens)  # <2>


# #### 

# In[8]:


counts / counts.sum()  # <3>


# #### 

# In[9]:


counts['justice']


# #### 

# In[10]:


counts['justice'] / counts.sum()


# #### 

# In[11]:


sentence = "Algorithmic bias has been cited in cases ranging from " \
    "election outcomes to the spread of online hate speech."
tokens = [tok.text for tok in nlp(sentence)]
counts = Counter(tokens)
dict(counts)


# #### 

# In[12]:


import requests
url = ('https://gitlab.com/tangibleai/nlpia2/'
       '-/raw/main/src/nlpia2/ch03/bias_intro.txt')
response = requests.get(url)
response


# #### 

# In[13]:


bias_intro_bytes = response.content  # <1>
bias_intro = response.text  # <2>
assert bias_intro_bytes.decode() == bias_intro    # <3>
bias_intro[:70]


# #### 

# In[14]:


tokens = [tok.text for tok in nlp(bias_intro)]
counts = Counter(tokens)
counts


# #### 

# In[15]:


counts.most_common(5)


# #### 

# In[16]:


counts.most_common()[-4:]


# #### 

# In[17]:


docs = [nlp(s) for s in bias_intro.split('\n')
        if s.strip()]  # <1>
counts = []
for doc in docs:
    counts.append(Counter([
        t.text.lower() for t in doc]))  # <2>
df = pd.DataFrame(counts)
df = df.fillna(0).astype(int)  # <3>
len(df)


# #### 

# In[18]:


df.head()


# #### 

# In[19]:


df.iloc[10]  # <1>


# #### 

# In[20]:


docs_tokens = []
for doc in docs:
    docs_tokens.append([
        tok.text.lower() for tok in nlp(doc.text)])  # <1>
len(docs_tokens[0])


# #### 

# In[21]:


all_doc_tokens = []
for tokens in docs_tokens:
    all_doc_tokens.extend(tokens)
len(all_doc_tokens)


# #### 

# In[22]:


vocab = sorted(  # <1>
    set(all_doc_tokens))  # <2>
len(vocab)


# #### 

# In[23]:


len(all_doc_tokens) / len(vocab)  # <3>


# #### 

# In[24]:


vocab  # <1>


# #### 

# In[27]:


count_vectors = []
for tokens in docs_tokens:
    count_vectors.append(Counter(tokens))
tf = pd.DataFrame(count_vectors)  # <1>
tf.T.sort
tf.fillna('')


# #### 

# In[ ]:


from sklearn.feature_extraction.text import CountVectorizer
corpus = [doc.text for doc in docs]
vectorizer = CountVectorizer()
count_vectors = vectorizer.fit_transform(corpus)  # <1>
print(count_vectors.toarray()) # <2>


# #### 

# In[ ]:


import math
def cosine_sim(vec1, vec2):
    vec1 = [val for val in vec1.values()] # <1>
    vec2 = [val for val in vec2.values()]

    dot_prod = 0
    for i, v in enumerate(vec1):
        dot_prod += v * vec2[i]

    mag_1 = math.sqrt(sum([x**2 for x in vec1]))
    mag_2 = math.sqrt(sum([x**2 for x in vec2]))

    return dot_prod / (mag_1 * mag_2)
259/2:

# #### .Cosine similarity

# In[ ]:


from sklearn.metrics.pairwise import cosine_similarity
vec1 = count_vectors[1,:]
vec2 = count_vectors[2,:]
cosine_similarity(vec1, vec2)


# #### .Cosine similarity

# In[ ]:


new_sentence = "What is algorithmic bias?"
ngram_docs = copy.copy(docs)
ngram_docs.append(new_sentence)


# #### .Cosine similarity

# In[ ]:


new_sentence_vector = vectorizer.transform([new_sentence])
print(new_sentence_vector.toarray())


# #### .Cosine similarity

# In[ ]:


cosine_similarity(count_vectors[1,:], new_sentence)


# #### .Cosine similarity

# In[ ]:


ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))
ngram_vectors = ngram_vectorizer.fit_transform(corpus)
print(ngram_vectors.toarray())


# #### .Cosine similarity

# In[ ]:


cosine_similarity(ngram_vectors[1,:], ngram_vectors[2,:])


# #### 

# In[ ]:


from this import s
print (s)


# #### 

# In[ ]:


char_vectorizer = CountVectorizer(
    ngram_range=(1,1), analyzer='char')  # <1>
s_char_frequencies = char_vectorizer.fit_transform(s)
259/3: tf
259/4: tf.T
259/5: tf.T.sort_index()
259/6:
count_vectors = []
for tokens in docs_tokens:
    count_vectors.append(Counter(tokens))
tf = pd.DataFrame(count_vectors)  # <1>
tf.T.sort_index()
tf.fillna('')
259/7:
>>> import math
>>> def cosine_sim(vec1, vec2):
...     vec1 = [val for val in vec1.values()]  # <1>
...     vec2 = [val for val in vec2.values()]
...
...     dot_prod = 0
...     for i, v in enumerate(vec1):
...         dot_prod += v * vec2[i]
...
...     mag_1 = math.sqrt(sum([x**2 for x in vec1]))
...     mag_2 = math.sqrt(sum([x**2 for x in vec2]))
...
...     return dot_prod / (mag_1 * mag_2)
259/8:
>>> from sklearn.metrics.pairwise import cosine_similarity
>>> vec1 = tf.values[:1,:]  # <1>
>>> vec2 = tf.values[1:2,:]
>>> cosine_similarity(vec1, vec2)
259/9: >>> cosine_sim(vec1, vec2)
259/10:
>>> import math
>>> def cosine_sim(vec1, vec2):
...     dot_prod = 0
...     for x1, x2 in zip(vec1, vec2):
...         dot_prod += x1 * x2
...
...     mag_1 = math.sqrt(sum([x1**2 for x1 in vec1]))
...     mag_2 = math.sqrt(sum([x2**2 for x2 in vec2]))
...
...     return dot_prod / (mag_1 * mag_2)
259/11: >>> cosine_sim(vec1, vec2)
259/12: vec1
259/13:
>>> from sklearn.metrics.pairwise import cosine_similarity
>>> vec1 = tf.values[:1,:]  # <1>
>>> vec2 = tf.values[1:2,:]
>>> cosine_sim(vec1, vec2)
array([[0.117...]])
259/14:
>>> from sklearn.metrics.pairwise import cosine_similarity
>>> vec1 = tf.values[:1,:]  # <1>
>>> vec2 = tf.values[1:2,:]
>>> cosine_sim(vec1, vec2)
259/15:  cosine_sim(vec1[0], vec2[0])
259/16: cosine_simimarity(vec1, vec2)
259/17:
>>> from sklearn.metrics.pairwise import cosine_similarity
>>> vec1 = tf.values[:1,:]  # <1>
>>> vec2 = tf.values[1:2,:]
>>> cosine_simimarity(vec1, vec2)  # <2>
259/18:
>>> from sklearn.metrics.pairwise import cosine_similarity
>>> vec1 = tf.values[:1,:]  # <1>
>>> vec2 = tf.values[1:2,:]
>>> cosine_similarity(vec1, vec2)  # <2>
259/19:
>>> from sklearn.metrics.pairwise import cosine_similarity
>>> tf = tf.fillna(0)  # <1>
>>> vec1 = tf.values[:1,:]  # <2>
>>> vec2 = tf.values[1:2,:]
>>> cosine_similarity(vec1, vec2)  # <3>
259/20: cosine_sim(vec1[0], vec2[0])
259/21:
>>> from seaborn import hist
>>> char_vectorizer = CountVectorizer(
...     ngram_range=(1, 1), analyzer='char')  # <1>
>>> s_char_frequencies = char_vectorizer.fit_transform(s)
>>> hist(s_char_frequencies, s_char_vectorizer)  # <2>
259/22:
>>> from seaborn import histplot
>>> char_vectorizer = CountVectorizer(
...     ngram_range=(1, 1), analyzer='char')  # <1>
>>> s_char_frequencies = char_vectorizer.fit_transform(s)
>>> histplot(
...     s_char_frequencies,
...     s_char_vectorizer)  # <2>
259/23:
>>> from sklearn.feature_extraction.text import CountVectorizer
>>> corpus = [doc.text for doc in docs]
>>> vectorizer = CountVectorizer()
>>> count_vectors = vectorizer.fit_transform(corpus)  # <1>
>>> print(count_vectors.toarray()) # <2>
259/24: count_vectors
259/25: count_vectors.T
259/26: count_vectors.toarray().T
259/27: corpus
259/28: count_vectors.todense().T
259/29: pd.options.display.max_columns = 16
259/30: count_vectors.todense().T
259/31: pd.DataFrame(count_vectors)
259/32: np.set_printoptions(edgeitems=8)
259/33: import numpy as np
259/34: np.set_printoptions(edgeitems=8)
259/35: count_vectors.todense().T
259/36: np.set_printoptions(edgeitems=5)
259/37: count_vectors.todense().T
259/38: np.set_printoptions(edgeitems=7)
259/39: count_vectors.todense().T
259/40: np.set_printoptions(edgeitems=8)
259/41: count_vectors.todense().T
259/42: count_vectors.toarray().T
259/43: hist -o -p
259/44: count_vectors.T.toarray()
259/45: hist -o -p
259/46:
>>> from sklearn.feature_extraction.text import CountVectorizer
>>> corpus = [doc.text for doc in docs]
>>> vectorizer = CountVectorizer()
>>> vectorizer = vectorizer.fit(corpus)
>>> count_vectors = vectorizer.transform(corpus)  # <1>
>>> np.set_printoptions(edgeitems=7)
>>> count_vectors.todense().T
>>> np.set_printoptions(edgeitems=8)
>>> count_vectors.toarray().T  # <2>
259/47:
>>> from sklearn.feature_extraction.text import CountVectorizer
>>> corpus = [doc.text for doc in docs]
>>> vectorizer = CountVectorizer(stopwords=False)
>>> vectorizer = vectorizer.fit(corpus)
>>> count_vectors = vectorizer.transform(corpus)  # <1>
>>> np.set_printoptions(edgeitems=7)
>>> count_vectors.todense().T
>>> np.set_printoptions(edgeitems=8)
>>> count_vectors.toarray().T  # <2>
259/48: corpus[0]
259/49:
>>> from sklearn.feature_extraction.text import CountVectorizer
>>> corpus = [doc.text for doc in docs]
>>> vectorizer = CountVectorizer(stop_words=False)
>>> vectorizer = vectorizer.fit(corpus)
>>> count_vectors = vectorizer.transform(corpus)  # <1>
>>> np.set_printoptions(edgeitems=7)
>>> count_vectors.todense().T
>>> np.set_printoptions(edgeitems=8)
>>> count_vectors.toarray().T  # <2>
259/50:
>>> from sklearn.feature_extraction.text import CountVectorizer
>>> corpus = [doc.text for doc in docs]
>>> vectorizer = CountVectorizer(stop_words=None)
>>> vectorizer = vectorizer.fit(corpus)
>>> count_vectors = vectorizer.transform(corpus)  # <1>
>>> np.set_printoptions(edgeitems=7)
>>> count_vectors.todense().T
>>> np.set_printoptions(edgeitems=8)
>>> count_vectors.toarray().T  # <2>
259/51:
>>> from sklearn.feature_extraction.text import CountVectorizer
>>> corpus = [doc.text for doc in docs]
>>> vectorizer = CountVectorizer(stop_words=None)
>>> vectorizer = vectorizer.fit(corpus)
>>> count_vectors = vectorizer.transform(corpus)  # <1>
>>> np.set_printoptions(edgeitems=7)
>>> count_vectors.todense().T
>>> np.set_printoptions(edgeitems=8)
>>> count_vectors.toarray().T  # <2>
259/52: vectorizer.vocabulary
259/53: vectorizer.vocabulary_
259/54: CountVectorizer?
259/55: len(vectorizer.vocabulary_)
259/56: count_vectors.shape
259/57: pd.DataFrame(count_vectors)
259/58: pd.DataFrame(count_vectors.toarray())
259/59: pd.DataFrame(count_vectors.toarray(), columns=vectorizer.vocabulary_)
259/60: pd.DataFrame(count_vectors.toarray(), columns=vectorizer.vocabulary_).T
259/61: pd.DataFrame?
259/62: hist -o -p
259/63:
>>> np.set_printoptions(edgeitems=8)  # <1>

>>> from sklearn.feature_extraction.text import CountVectorizer
>>> corpus = [doc.text for doc in docs]

>>> vectorizer = CountVectorizer()  # <2>
>>> vectorizer = vectorizer.fit(corpus)
>>> count_vectors = vectorizer.transform(corpus)  # <3>
>>> count_vectors
259/64: count_vectors.toarray()
259/65: !git commit -am 'countvectorizer sparse'
260/1: from knowt.search import *
   1: import numpy as np
   2: vecs = np.memmap('data/corpus_wikipedia/articles.embeddings.memmap', shape=shape, mode='r+')
   3: shape = (x.sizes['sentence'], x.sizes['dim'])
   4: hist -h
   5: help(hist)
   6: hist?
   7: %history?
   8: %history -g -n 1
   9: %history -g
  10: %history -g
  11: %history -g -f scripts/all.hist.ipy
